\tikzstyle{my-box}=[
    rectangle,
    draw=hidden-draw,
    rounded corners,
    text opacity=1,
    minimum height=1.5em,
    minimum width=5em,
    inner sep=2pt,
    align=center,
    fill opacity=.5,
    line width=0.8pt,
]
\tikzstyle{leaf}=[my-box, minimum height=1.5em,
    fill=hidden-pink!80, text=black, align=left,font=\normalsize,
    inner xsep=2pt,
    inner ysep=4pt,
    line width=0.8pt,
]
\begin{figure*}[t]
	\centering
    \resizebox{\textwidth}{!}{
	\begin{forest}
        forked edges,
		for tree={
                grow=east,
                reversed=true,
                anchor=base west,
                parent anchor=east,
                child anchor=west,
                base=center,
                font=\large,
                rectangle,
                draw=hidden-draw,
                rounded corners,
                align=left,
                text centered,
                minimum width=4em,
                edge+={darkgray, line width=1pt},
                s sep=3pt,
                inner xsep=2pt,
                inner ysep=3pt,
                line width=0.8pt,
                ver/.style={rotate=90, child anchor=north, parent anchor=south, anchor=center},
            },
            where level=1{text width=6em,font=\normalsize, }{},
            where level=2{text width=10em,font=\normalsize}{},
            where level=3{text width=12em,font=\normalsize,}{},
            where level=4{text width=20em,font=\normalsize,}{},
            % where level=5{text width=6em,font=\normalsize,}{},
	    [Architecture, ver
			[Position \\Embeddings
            [
                Position \\Embedding types
                [Absolute
    				    [
                            Sinusoidal\cite{vaswani2017attention}{, }Learned\cite{gehring2017convolutional}{, }Complex\cite{wang2019encoding} {, }\\NoPE\cite{chi2023latent,kazemnejad2024impact}
                           , text width=24.5em
                            ]
    			     ]
    			  [Relative
    					[
        					RoPE\cite{su2024roformer}{, }Alibi\cite{press2021train}{, }Sandwich\cite{chi2022dissecting}{, }XPOS\cite{sun2022length}{, }\\T5\cite{raffel2020exploring}{, }HoPE\cite{chen2024hope}
                           , text width=24.5em
    					]
    			  ]
                    [Content-Aware
                        [
                            CoPE\cite{golovneva2024contextual}{, }DAPE\cite{zheng2025dape}
                           , text width=24.5em
                        ]
                    ]
            ]
            [
                Extrapolation\\ Methods
                [
                    Position Reorganization
                    [
                    DCA\cite{an2024training}{, }ReRoPE\cite{kexuefm-9708} {, }String\cite{an2024does}{, }SelfExtend\cite{jin2024llm}
                   , text width=24.5em
                    ]
                ]
                [
                    Position Interpolation
                    [
                    ABF\cite{xiong2023effective}{, }NTK\cite{peng2023yarn}{, }YaRN\cite{peng2023yarn}{, }LongRoPE\cite{ding2024longrope}
                   , text width=24.5em
                    ]
                ]
                [
                    Hierarchical Position \\Embedding
                    [
                    BiPE\cite{he2024two}{, }HiRoPE\citep{zhang2024hirope}
                  , text width=24.5em
                    ]
                ]
                [
                    Position Simulation
                    [
                    RandPos\cite{ruoss2023randomized}{, }PoSE\cite{zhu2023pose}{, }Cream\cite{wuefficient}{, }\\SkipAlign\cite{wu2024long} {, }LongRecipe\cite{hu2024longrecipe}
                   , text width=24.5em
                    ]
                ]
            ]
			]
			[Attention, 
				    [Transformer-Based \\Architecture
    				[
                    Sparse Attention
                            [
                            Longformer\cite{beltagy2020longformerlongdocumenttransformer}{, }MoA\cite{fu2024moamixturesparseattention}{, }NSA\cite{yuan2025nativesparseattentionhardwarealigned}{, }\\Weighted GQA\cite{chinnakonduru2024weightedgroupedqueryattention}{, }Duo Attention\cite{xiao2024duoattentionefficientlongcontextllm}{, }CORM\cite{dai2024cormcacheoptimizationrecent}{, }\\LongHeADS\cite{lu2024longheadsmultiheadattentionsecretly4}{, }SnapKV\cite{li2024snapkvllmknowslooking}
                           , text width=24.5em
                            ]
                    ]
                    [Hierarchical Attention
                            [
                            HAN\cite{han}{, }Hi-Transformer\cite{hi-transformer}{, }ERNIE-SPARSE\cite{ERNIE-SPARSE}
                           , text width=24.5em
                            ]
                    ]
                    [Recurrent Transformer
                            [
                            Transformer-XL~\cite{TransformerXL}{, }Memformer~\cite{Memformer}{, }Block-Recurrent\\Transformer\cite{BlockRecurrentTransformers}{, }RMT\cite{rmt}{, }Infinite Attention~\cite{infinitransformer}
                           , text width=24.5em
                            ]
                    ]
			]
                    [Linear-Complexity \\Architecture
                        [
                        SSM
                        [
                            Mamba\cite{gu2023mamba}{, }ReMamba\cite{yuan2025remambaequipmambaeffective}{, }TAIPAN\cite{vannguyen2024taipanefficientexpressivestate}{, }\\DeciMamba\cite{benkish2024decimambaexploringlengthextrapolation}
                           , text width=24.5em
                            ]
                        ]
                        [
                        Linear Attention
                        [
                            Linear Transformer\cite{katharopoulos2020transformersrnnsfastautoregressive}{, }lightning attention\cite{qin2024lightningattention2freelunch}{, }\\RetNet\cite{sun2023retentivenetworksuccessortransformer}
                           , text width=24.5em
                            ]
                        ]
                    ]
				[Hybrid Architecture
					[
                        Layer-wise
                        [
                            Jamba\cite{lieber2024jamba}{, }RecurrentGemma\cite{recurrentgemma}{, }Minimax-01\cite{minimax2025minimax01scalingfoundationmodels}{, }\\Command R\cite{c4ai-command-r7b-12-2024}
                           , text width=24.5em
                            ]
					]
                    [
                    Prefill-Decode
                    [
                            RWKV\cite{peng2023rwkvreinventingrnnstransformer}{, }YOCO\cite{sun2025you}{, }GoldFinch\cite{goldstein2024goldfinch}
                           , text width=24.5em
                            ]
                    ]
                    [
                    Head-wise
                    [
                            Hymba\cite{dong2024hymba}{, }Samba\cite{ren2024samba}
                           , text width=24.5em
                            ]
                    ]
				]	
                ]
                ]
	\end{forest}}
	\caption{Taxonomy of Long Context Model Architectures.}
    \label{fig:taxonomy-model}
\end{figure*}
