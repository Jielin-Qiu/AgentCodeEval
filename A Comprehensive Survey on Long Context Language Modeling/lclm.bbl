\begin{thebibliography}{693}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Ntk(2023)]{NtkAlibi2023}
Ntk-alibi: Long text extrapolation of alibi position encoding through interpolation, August 2023.
\newblock URL \url{https://github.com/keezen/ntk_alibi}.

\bibitem[lon(2023)]{longdata}
Long-data-collections.
\newblock \url{https://huggingface.co/datasets/togethercomputer/Long-Data-Collections}, 2023.

\bibitem[Abbas et~al.(2023)Abbas, Tirumala, Simig, Ganguli, and Morcos]{SemDeDup}
Amro Abbas, Kushal Tirumala, Daniel Simig, Surya Ganguli, and Ari~S. Morcos.
\newblock Semdedup: Data-efficient learning at web-scale through semantic deduplication.
\newblock \emph{CoRR}, abs/2303.09540, 2023.
\newblock \doi{10.48550/ARXIV.2303.09540}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2303.09540}.

\bibitem[Adams et~al.(2024)Adams, Busch, Han, Excoffier, Ortala, L{\"o}ser, Aerts, Kather, Truhn, and Bressem]{adams2024longhealth}
Lisa Adams, Felix Busch, Tianyu Han, Jean-Baptiste Excoffier, Matthieu Ortala, Alexander L{\"o}ser, Hugo~JWL Aerts, Jakob~Nikolas Kather, Daniel Truhn, and Keno Bressem.
\newblock Longhealth: A question answering benchmark with long clinical documents.
\newblock \emph{arXiv preprint arXiv:2401.14490}, 2024.

\bibitem[Agrawal et~al.(2024)Agrawal, Dang, Nezhad, Pokharel, and Scheinberg]{agrawal2024evaluating}
Ameeta Agrawal, Andy Dang, Sina~Bagheri Nezhad, Rhitabrat Pokharel, and Russell Scheinberg.
\newblock Evaluating multilingual long-context models for retrieval and reasoning.
\newblock In \emph{Proceedings of the Fourth Workshop on Multilingual Representation Learning (MRL 2024)}, pages 216--231, 2024.

\bibitem[Ai(2024)]{talkie}
Talkie Ai.
\newblock Talkie | ai-native character community, 2024.
\newblock URL \url{https://www.talkie-ai.com/}.

\bibitem[Ainslie et~al.(2023)Ainslie, Lee{-}Thorp, de~Jong, Zemlyanskiy, Lebr{\'{o}}n, and Sanghai]{ainslie2023gqatraininggeneralizedmultiquery}
Joshua Ainslie, James Lee{-}Thorp, Michiel de~Jong, Yury Zemlyanskiy, Federico Lebr{\'{o}}n, and Sumit Sanghai.
\newblock {GQA:} training generalized multi-query transformer models from multi-head checkpoints.
\newblock In \emph{{EMNLP}}, pages 4895--4901. Association for Computational Linguistics, 2023.

\bibitem[Al-Khateeb et~al.(2023)Al-Khateeb, Dey, Soboleva, and Hestness]{al2023position}
Faisal Al-Khateeb, Nolan Dey, Daria Soboleva, and Joel Hestness.
\newblock Position interpolation improves alibi extrapolation.
\newblock \emph{arXiv preprint arXiv:2310.13017}, 2023.

\bibitem[Alayrac et~al.(2022)Alayrac, Donahue, Luc, Miech, Barr, Hasson, Lenc, Mensch, Millican, Reynolds, Ring, Rutherford, Cabi, Han, Gong, Samangooei, Monteiro, Menick, Borgeaud, Brock, Nematzadeh, Sharifzadeh, Binkowski, Barreira, Vinyals, Zisserman, and Simonyan]{Alayrac-2022-nips-flamingo}
Jean{-}Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob~L. Menick, Sebastian Borgeaud, Andy Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, and Kar{\'{e}}n Simonyan.
\newblock Flamingo: a visual language model for few-shot learning.
\newblock In Sanmi Koyejo, S.~Mohamed, A.~Agarwal, Danielle Belgrave, K.~Cho, and A.~Oh, editors, \emph{Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022}, 2022.
\newblock URL \url{http://papers.nips.cc/paper\_files/paper/2022/hash/960a172bc7fbf0177ccccbb411a7d800-Abstract-Conference.html}.

\bibitem[An et~al.(2023)An, Gong, Zhong, Zhao, Li, Zhang, Kong, and Qiu]{an2023eval}
Chenxin An, Shansan Gong, Ming Zhong, Xingjian Zhao, Mukai Li, Jun Zhang, Lingpeng Kong, and Xipeng Qiu.
\newblock L-eval: Instituting standardized evaluation for long context language models.
\newblock \emph{arXiv preprint arXiv:2307.11088}, 2023.

\bibitem[An et~al.(2024{\natexlab{a}})An, Huang, Zhang, Gong, Qiu, Zhou, and Kong]{an2024training}
Chenxin An, Fei Huang, Jun Zhang, Shansan Gong, Xipeng Qiu, Chang Zhou, and Lingpeng Kong.
\newblock Training-free long-context scaling of large language models.
\newblock \emph{arXiv preprint arXiv:2402.17463}, 2024{\natexlab{a}}.

\bibitem[An et~al.(2024{\natexlab{b}})An, Zhang, Zhong, Li, Gong, Luo, Xu, and Kong]{an2024does}
Chenxin An, Jun Zhang, Ming Zhong, Lei Li, Shansan Gong, Yao Luo, Jingjing Xu, and Lingpeng Kong.
\newblock Why does the effective context length of llms fall short?
\newblock \emph{arXiv preprint arXiv:2410.18745}, 2024{\natexlab{b}}.

\bibitem[An et~al.(2024{\natexlab{c}})An, Ma, Lin, Zheng, Lou, and Chen]{an2024make}
Shengnan An, Zexiong Ma, Zeqi Lin, Nanning Zheng, Jian-Guang Lou, and Weizhu Chen.
\newblock Make your {LLM} fully utilize the context.
\newblock In \emph{The Thirty-eighth Annual Conference on Neural Information Processing Systems}, 2024{\natexlab{c}}.
\newblock URL \url{https://openreview.net/forum?id=YGTVEmBXtV}.

\bibitem[Anthropic()]{contextual-retrieval}
Anthropic.
\newblock Introducing contextual retrieval.
\newblock \url{https://www.anthropic.com/news/contextual-retrieval}.

\bibitem[Anysphere(2025)]{cursor_ai_2025}
Anysphere.
\newblock Cursor - the ai code editor.
\newblock \url{https://www.cursor.com/en}, 2025.
\newblock URL \url{https://www.cursor.com}.

\bibitem[Arora et~al.(2024)Arora, Eyuboglu, Zhang, Timalsina, Alberti, Zinsley, Zou, Rudra, and Ré]{arora2024simplelinearattentionlanguage}
Simran Arora, Sabri Eyuboglu, Michael Zhang, Aman Timalsina, Silas Alberti, Dylan Zinsley, James Zou, Atri Rudra, and Christopher Ré.
\newblock Simple linear attention language models balance the recall-throughput tradeoff, 2024.
\newblock URL \url{https://arxiv.org/abs/2402.18668}.

\bibitem[Bai et~al.(2023)Bai, Lv, Zhang, Lyu, Tang, Huang, Du, Liu, Zeng, Hou, et~al.]{bai2023longbench}
Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, et~al.
\newblock Longbench: A bilingual, multitask benchmark for long context understanding.
\newblock \emph{arXiv preprint arXiv:2308.14508}, 2023.

\bibitem[Bai et~al.(2024{\natexlab{a}})Bai, Lv, Zhang, He, Qi, Hou, Tang, Dong, and Li]{bai2024longalign}
Yushi Bai, Xin Lv, Jiajie Zhang, Yuze He, Ji~Qi, Lei Hou, Jie Tang, Yuxiao Dong, and Juanzi Li.
\newblock Longalign: A recipe for long context alignment of large language models.
\newblock \emph{arXiv preprint arXiv:2401.18058}, 2024{\natexlab{a}}.

\bibitem[Bai et~al.(2024{\natexlab{b}})Bai, Lv, Zhang, He, Qi, Hou, Tang, Dong, and Li]{longalign}
Yushi Bai, Xin Lv, Jiajie Zhang, Yuze He, Ji~Qi, Lei Hou, Jie Tang, Yuxiao Dong, and Juanzi Li.
\newblock {L}ong{A}lign: A recipe for long context alignment of large language models.
\newblock In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen, editors, \emph{Findings of the Association for Computational Linguistics: EMNLP 2024}, pages 1376--1395, Miami, Florida, USA, November 2024{\natexlab{b}}. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2024.findings-emnlp.74}.
\newblock URL \url{https://aclanthology.org/2024.findings-emnlp.74}.

\bibitem[Bai et~al.(2024{\natexlab{c}})Bai, Tu, Zhang, Peng, Wang, Lv, Cao, Xu, Hou, Dong, Tang, and Li]{bai2024longbench2}
Yushi Bai, Shangqing Tu, Jiajie Zhang, Hao Peng, Xiaozhi Wang, Xin Lv, Shulin Cao, Jiazheng Xu, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li.
\newblock Longbench v2: Towards deeper understanding and reasoning on realistic long-context multitasks.
\newblock \emph{arXiv preprint arXiv:2412.15204}, 2024{\natexlab{c}}.

\bibitem[Bai et~al.(2024{\natexlab{d}})Bai, Zhang, Lv, Zheng, Zhu, Hou, Dong, Tang, and Li]{bai2024longwriter}
Yushi Bai, Jiajie Zhang, Xin Lv, Linzhi Zheng, Siqi Zhu, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li.
\newblock Longwriter: Unleashing 10,000+ word generation from long context llms.
\newblock \emph{arXiv preprint arXiv:2408.07055}, 2024{\natexlab{d}}.

\bibitem[Bai et~al.(2020)Bai, Zhang, Zhu, and Jin]{bai2020pipeswitch}
Zhihao Bai, Zhen Zhang, Yibo Zhu, and Xin Jin.
\newblock $\{$PipeSwitch$\}$: Fast pipelined context switching for deep learning applications.
\newblock In \emph{14th USENIX Symposium on Operating Systems Design and Implementation (OSDI 20)}, pages 499--514, 2020.

\bibitem[Banerjee and Lavie(2005)]{banerjee2005meteor}
Satanjeev Banerjee and Alon Lavie.
\newblock Meteor: An automatic metric for mt evaluation with improved correlation with human judgments.
\newblock In \emph{Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization}, pages 65--72, 2005.

\bibitem[Bao et~al.(2021)Bao, Zhang, Teng, Chen, and Luo]{bao2021g}
Guangsheng Bao, Yue Zhang, Zhiyang Teng, Boxing Chen, and Weihua Luo.
\newblock G-transformer for document-level machine translation.
\newblock In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli, editors, \emph{Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, {ACL/IJCNLP} 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021}, pages 3442--3455. Association for Computational Linguistics, 2021.
\newblock \doi{10.18653/V1/2021.ACL-LONG.267}.
\newblock URL \url{https://doi.org/10.18653/v1/2021.acl-long.267}.

\bibitem[Beltagy et~al.(2020{\natexlab{a}})Beltagy, Peters, and Cohan]{DBLP:journals/corr/abs-2004-05150}
Iz~Beltagy, Matthew~E. Peters, and Arman Cohan.
\newblock Longformer: The long-document transformer.
\newblock \emph{CoRR}, abs/2004.05150, 2020{\natexlab{a}}.
\newblock URL \url{https://arxiv.org/abs/2004.05150}.

\bibitem[Beltagy et~al.(2020{\natexlab{b}})Beltagy, Peters, and Cohan]{beltagy2020longformerlongdocumenttransformer}
Iz~Beltagy, Matthew~E. Peters, and Arman Cohan.
\newblock Longformer: The long-document transformer, 2020{\natexlab{b}}.
\newblock URL \url{https://arxiv.org/abs/2004.05150}.

\bibitem[Ben-Kish et~al.(2024)Ben-Kish, Zimerman, Abu-Hussein, Cohen, Globerson, Wolf, and Giryes]{benkish2024decimambaexploringlengthextrapolation}
Assaf Ben-Kish, Itamar Zimerman, Shady Abu-Hussein, Nadav Cohen, Amir Globerson, Lior Wolf, and Raja Giryes.
\newblock Decimamba: Exploring the length extrapolation potential of mamba, 2024.
\newblock URL \url{https://arxiv.org/abs/2406.14528}.

\bibitem[Bertsch et~al.(2024)Bertsch, Ivgi, Alon, Berant, Gormley, and Neubig]{bertsch2024context}
Amanda Bertsch, Maor Ivgi, Uri Alon, Jonathan Berant, Matthew~R Gormley, and Graham Neubig.
\newblock In-context learning with long-context models: An in-depth exploration.
\newblock \emph{arXiv preprint arXiv:2405.00200}, 2024.

\bibitem[bloc97(2023)]{ntk_aware_reddit}
bloc97.
\newblock {NTK-Aware Scaled RoPE allows LLaMA models to have extended (8k+) context size without any fine-tuning and minimal perplexity degradation.}, 2023.
\newblock URL \url{https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_scaled_rope_allows_llama_models_to_have/}.

\bibitem[Bogomolov et~al.(2024)Bogomolov, Eliseeva, Galimzyanov, Glukhov, Shapkin, Tigina, Golubev, Kovrigin, van Deursen, Izadi, et~al.]{bogomolov2024long}
Egor Bogomolov, Aleksandra Eliseeva, Timur Galimzyanov, Evgeniy Glukhov, Anton Shapkin, Maria Tigina, Yaroslav Golubev, Alexander Kovrigin, Arie van Deursen, Maliheh Izadi, et~al.
\newblock Long code arena: a set of benchmarks for long-context code models.
\newblock \emph{arXiv preprint arXiv:2406.11612}, 2024.

\bibitem[Borgeaud et~al.(2021)Borgeaud, Mensch, Hoffmann, Cai, Rutherford, Millican, van~den Driessche, Lespiau, Damoc, Clark, de~Las~Casas, Guy, Menick, Ring, Hennigan, Huang, Maggiore, Jones, Cassirer, Brock, Paganini, Irving, Vinyals, Osindero, Simonyan, Rae, Elsen, and Sifre]{retro}
Sebastian Borgeaud, A.~Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George van~den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, Diego de~Las~Casas, Aurelia Guy, Jacob Menick, Roman Ring, T.~Hennigan, Saffron Huang, Lorenzo Maggiore, Chris Jones, Albin Cassirer, Andy Brock, Michela Paganini, G.~Irving, O.~Vinyals, Simon Osindero, K.~Simonyan, Jack~W. Rae, Erich Elsen, and L.~Sifre.
\newblock Improving language models by retrieving from trillions of tokens.
\newblock \emph{International Conference on Machine Learning}, 2021.

\bibitem[Bosselut et~al.(2018)Bosselut, Celikyilmaz, He, Gao, Huang, and Choi]{bosselut2018discourse}
Antoine Bosselut, Asli Celikyilmaz, Xiaodong He, Jianfeng Gao, Po-Sen Huang, and Yejin Choi.
\newblock Discourse-aware neural rewards for coherent text generation.
\newblock \emph{arXiv preprint arXiv:1805.03766}, 2018.

\bibitem[Botev et~al.(2024)Botev, De, Smith, Fernando, Muraru, Haroun, Berrada, Pascanu, Sessa, Dadashi, Hussenot, Ferret, Girgin, Bachem, Andreev, Kenealy, Mesnard, Hardin, Bhupatiraju, Pathak, Sifre, Rivi{\`{e}}re, Kale, Love, Tafti, Joulin, Fiedel, Senter, Chen, Srinivasan, Desjardins, Budden, Doucet, Vikram, Paszke, Gale, Borgeaud, Chen, Brock, Paterson, Brennan, Risdal, Gundluru, Devanathan, Mooney, Chauhan, Culliton, Martins, Bandy, Huntsperger, Cameron, Zucker, Warkentin, Peran, Giang, Ghahramani, Farabet, Kavukcuoglu, Hassabis, Hadsell, Teh, and de~Frietas]{recurrentgemma}
Aleksandar Botev, Soham De, Samuel~L. Smith, Anushan Fernando, George{-}Cristian Muraru, Ruba Haroun, Leonard Berrada, Razvan Pascanu, Pier~Giuseppe Sessa, Robert Dadashi, L{\'{e}}onard Hussenot, Johan Ferret, Sertan Girgin, Olivier Bachem, Alek Andreev, Kathleen Kenealy, Thomas Mesnard, Cassidy Hardin, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivi{\`{e}}re, Mihir~Sanjay Kale, Juliette Love, Pouya Tafti, Armand Joulin, Noah Fiedel, Evan Senter, Yutian Chen, Srivatsan Srinivasan, Guillaume Desjardins, David Budden, Arnaud Doucet, Sharad Vikram, Adam Paszke, Trevor Gale, Sebastian Borgeaud, Charlie Chen, Andy Brock, Antonia Paterson, Jenny Brennan, Meg Risdal, Raj Gundluru, Nesh Devanathan, Paul Mooney, Nilay Chauhan, Phil Culliton, Luiz~GUStavo Martins, Elisa Bandy, David Huntsperger, Glenn Cameron, Arthur Zucker, Tris Warkentin, Ludovic Peran, Minh Giang, Zoubin Ghahramani, Cl{\'{e}}ment Farabet, Koray Kavukcuoglu, Demis Hassabis, Raia Hadsell, Yee~Whye Teh, and Nando de~Frietas.
\newblock Recurrentgemma: Moving past transformers for efficient open language models.
\newblock \emph{CoRR}, abs/2404.07839, 2024.

\bibitem[Brakel et~al.(2024)Brakel, Odyurt, and Varbanescu]{brakel2024model}
Felix Brakel, Uraz Odyurt, and Ana-Lucia Varbanescu.
\newblock Model parallelism on distributed infrastructure: A literature review from theory to llm case-studies.
\newblock \emph{arXiv preprint arXiv:2403.03699}, 2024.

\bibitem[Brandon et~al.(2023)Brandon, Nrusimha, Qian, Ankner, Jin, Song, and Ragan-Kelley]{brandon2023striped}
William Brandon, Aniruddha Nrusimha, Kevin Qian, Zachary Ankner, Tian Jin, Zhiye Song, and Jonathan Ragan-Kelley.
\newblock Striped attention: Faster ring attention for causal transformers.
\newblock \emph{arXiv preprint arXiv:2311.09431}, 2023.

\bibitem[Brants et~al.(2007)Brants, Popat, Xu, Och, and Dean]{brants2007large}
Thorsten Brants, Ashok Popat, Peng Xu, Franz~Josef Och, and Jeffrey Dean.
\newblock Large language models in machine translation.
\newblock In \emph{Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)}, pages 858--867, 2007.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell, Agarwal, Herbert-Voss, Krueger, Henighan, Child, Ramesh, Ziegler, Wu, Winter, Hesse, Chen, Sigler, Litwin, Gray, Chess, Clark, Berner, McCandlish, Radford, Sutskever, and Amodei]{10.5555/3495724.3495883}
Tom~B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel~M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei.
\newblock Language models are few-shot learners.
\newblock In \emph{Proceedings of the 34th International Conference on Neural Information Processing Systems}, NIPS '20, Red Hook, NY, USA, 2020. Curran Associates Inc.
\newblock ISBN 9781713829546.

\bibitem[Bulatov et~al.(2023)Bulatov, Kuratov, and Burtsev]{rmt}
Aydar Bulatov, Yuri Kuratov, and Mikhail~S. Burtsev.
\newblock Scaling transformer to 1m tokens and beyond with {RMT}.
\newblock \emph{CoRR}, abs/2304.11062, 2023.

\bibitem[Cai et~al.(2024{\natexlab{a}})Cai, Li, Geng, Peng, Lee, Chen, and Dao]{cai2024medusa}
Tianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng, Jason~D Lee, Deming Chen, and Tri Dao.
\newblock Medusa: Simple llm inference acceleration framework with multiple decoding heads.
\newblock In \emph{Proceedings of the 41st International Conference on Machine Learning}, pages 5209--5235, 2024{\natexlab{a}}.

\bibitem[Cai et~al.(2024{\natexlab{b}})Cai, Zhang, Gao, Liu, Liu, Lu, Xiong, Dong, Chang, Hu, and Xiao]{cai2024pyramidkvdynamickvcache}
Zefan Cai, Yichi Zhang, Bofei Gao, Yuliang Liu, Tianyu Liu, Keming Lu, Wayne Xiong, Yue Dong, Baobao Chang, Junjie Hu, and Wen Xiao.
\newblock Pyramidkv: Dynamic kv cache compression based on pyramidal information funneling, 2024{\natexlab{b}}.
\newblock URL \url{https://arxiv.org/abs/2406.02069}.

\bibitem[Cao et~al.(2024)Cao, Kang, Wang, and Sun]{cao2024instruction}
Yihan Cao, Yanbin Kang, Chi Wang, and Lichao Sun.
\newblock Instruction mining: Instruction data selection for tuning large language models.
\newblock In \emph{First Conference on Language Modeling}, 2024.

\bibitem[Chai et~al.(2024)Chai, Liu, Yang, Yin, Jin, Liu, Sun, Zhang, Ren, Guo, et~al.]{mceval}
Linzheng Chai, Shukai Liu, Jian Yang, Yuwei Yin, Ke~Jin, Jiaheng Liu, Tao Sun, Ge~Zhang, Changyu Ren, Hongcheng Guo, et~al.
\newblock Mceval: Massively multilingual code evaluation.
\newblock \emph{arXiv preprint arXiv:2406.07436}, 2024.

\bibitem[Chan et~al.(2024)Chan, Chowdhury, Jaffe, Aung, Sherburn, Mays, Starace, Liu, Maksin, Patwardhan, Weng, and Madry]{DBLP:journals/corr/abs-2410-07095}
Jun~Shern Chan, Neil Chowdhury, Oliver Jaffe, James Aung, Dane Sherburn, Evan Mays, Giulio Starace, Kevin Liu, Leon Maksin, Tejal Patwardhan, Lilian Weng, and Aleksander Madry.
\newblock Mle-bench: Evaluating machine learning agents on machine learning engineering.
\newblock \emph{CoRR}, abs/2410.07095, 2024.
\newblock \doi{10.48550/ARXIV.2410.07095}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2410.07095}.

\bibitem[Chandrasegaran et~al.(2024)Chandrasegaran, Gupta, Hadzic, Kota, He, Eyzaguirre, Durante, Li, Wu, and Fei{-}Fei]{Chandrasegaran-2024-arxiv-videohour}
Keshigeyan Chandrasegaran, Agrim Gupta, Lea~M. Hadzic, Taran Kota, Jimming He, Crist{\'{o}}bal Eyzaguirre, Zane Durante, Manling Li, Jiajun Wu, and Li~Fei{-}Fei.
\newblock Hourvideo: 1-hour video-language understanding.
\newblock \emph{CoRR}, abs/2411.04998, 2024.
\newblock \doi{10.48550/ARXIV.2411.04998}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2411.04998}.

\bibitem[Chang et~al.(2024)Chang, Bao, Hou, Jiang, Zheng, Zhong, Zhang, Song, Yao, Jiang, et~al.]{chang2024flux}
Li-Wen Chang, Wenlei Bao, Qi~Hou, Chengquan Jiang, Ningxin Zheng, Yinmin Zhong, Xuanrun Zhang, Zuquan Song, Chengji Yao, Ziheng Jiang, et~al.
\newblock Flux: fast software-based communication overlap on gpus through kernel fusion.
\newblock \emph{arXiv preprint arXiv:2406.06858}, 2024.

\bibitem[{Character AI}(2023)]{character_ai}
{Character AI}.
\newblock Character ai.
\newblock Retrieved September 14, 2023 from \url{https://character.ai/}, 2023.
\newblock URL \url{https://character.ai/}.

\bibitem[Chen et~al.(2024{\natexlab{a}})Chen, Ge, Fu, Xiao, and Chen]{chen2024travelagent}
Aili Chen, Xuyang Ge, Ziquan Fu, Yanghua Xiao, and Jiangjie Chen.
\newblock Travelagent: An {AI} assistant for personalized travel planning.
\newblock \emph{CoRR}, abs/2409.08069, 2024{\natexlab{a}}.
\newblock \doi{10.48550/ARXIV.2409.08069}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2409.08069}.

\bibitem[Chen et~al.(2017)Chen, Fisch, Weston, and Bordes]{drqa}
Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes.
\newblock Reading {Wikipedia} to answer open-domain questions.
\newblock In \emph{Association for Computational Linguistics (ACL)}, 2017.

\bibitem[Chen et~al.(2023{\natexlab{a}})Chen, Li, Meng, Liang, and Bing]{chen2023clex}
Guanzheng Chen, Xin Li, Zaiqiao Meng, Shangsong Liang, and Lidong Bing.
\newblock Clex: Continuous length extrapolation for large language models.
\newblock \emph{arXiv preprint arXiv:2310.16450}, 2023{\natexlab{a}}.

\bibitem[Chen et~al.(2024{\natexlab{b}})Chen, Li, Meng, Liang, and Bing]{chen2024clex}
Guanzheng Chen, Xin Li, Zaiqiao Meng, Shangsong Liang, and Lidong Bing.
\newblock Clex: Continuous length extrapolation for large language models.
\newblock In \emph{The Twelfth International Conference on Learning Representations}, 2024{\natexlab{b}}.

\bibitem[Chen et~al.(2023{\natexlab{b}})Chen, Pasunuru, Weston, and Celikyilmaz]{memwalker}
Howard Chen, Ramakanth Pasunuru, Jason Weston, and Asli Celikyilmaz.
\newblock Walking down the memory maze: Beyond context limit through interactive reading.
\newblock \emph{arXiv preprint arXiv: 2310.05029}, 2023{\natexlab{b}}.

\bibitem[Chen et~al.(2024{\natexlab{c}})Chen, Tiwari, Sadhukhan, Chen, Shi, Yen, and Chen]{chen2024magicdec}
Jian Chen, Vashisth Tiwari, Ranajoy Sadhukhan, Zhuoming Chen, Jinyuan Shi, Ian En-Hsu Yen, and Beidi Chen.
\newblock Magicdec: Breaking the latency-throughput tradeoff for long context generation with speculative decoding.
\newblock \emph{arXiv preprint arXiv:2408.11049}, 2024{\natexlab{c}}.

\bibitem[Chen et~al.(2024{\natexlab{d}})Chen, Zhou, Hua, Loh, Chen, Li, Zhu, and Liang]{chen2024fintextqa}
Jian Chen, Peilin Zhou, Yining Hua, Yingxin Loh, Kehui Chen, Ziyuan Li, Bing Zhu, and Junwei Liang.
\newblock Fintextqa: A dataset for long-form financial question answering.
\newblock \emph{arXiv preprint arXiv:2405.09980}, 2024{\natexlab{d}}.

\bibitem[Chen et~al.(2025{\natexlab{a}})Chen, Wu, Xu, and Zhang]{Chen2025LADMLT}
Jianghao Chen, Junhong Wu, Yangyifan Xu, and Jiajun Zhang.
\newblock Ladm: Long-context training data selection with attention-based dependency measurement for llms.
\newblock 2025{\natexlab{a}}.
\newblock URL \url{https://api.semanticscholar.org/CorpusID:276768135}.

\bibitem[Chen et~al.(2024{\natexlab{e}})Chen, Xiao, Zhang, Luo, Lian, and Liu]{chen2024bge}
Jianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, and Zheng Liu.
\newblock {BGE} m3-embedding: Multi-lingual, multi-functionality, multi-granularity text embeddings through self-knowledge distillation.
\newblock \emph{CoRR}, abs/2402.03216, 2024{\natexlab{e}}.
\newblock \doi{10.48550/ARXIV.2402.03216}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2402.03216}.

\bibitem[Chen et~al.(2025{\natexlab{b}})Chen, Li, Zhao, Song, and Vinci]{chen2025r1v}
Liang Chen, Lei Li, Haozhe Zhao, Yifan Song, and Vinci.
\newblock R1-v: Reinforcing super generalization ability in vision-language models with less than \$3.
\newblock \url{https://github.com/Deep-Agent/R1-V}, 2025{\natexlab{b}}.
\newblock Accessed: 2025-02-02.

\bibitem[Chen et~al.(2024{\natexlab{f}})Chen, Li, Yan, Wang, Gunaratna, Yadav, Tang, Srinivasan, Zhou, Huang, and Jin]{chen2024alpagasus}
Lichang Chen, Shiyang Li, Jun Yan, Hai Wang, Kalpa Gunaratna, Vikas Yadav, Zheng Tang, Vijay Srinivasan, Tianyi Zhou, Heng Huang, and Hongxia Jin.
\newblock Alpagasus: Training a better alpaca with fewer data.
\newblock In \emph{The Twelfth International Conference on Learning Representations}, 2024{\natexlab{f}}.

\bibitem[Chen et~al.(2024{\natexlab{g}})Chen, Liu, He, Zheng, Sun, Li, Luo, and Yang]{prolong}
Longze Chen, Ziqiang Liu, Wanwei He, Yinhe Zheng, Hao Sun, Yunshui Li, Run Luo, and Min Yang.
\newblock Long context is not long at all: A prospector of long-dependency data for large language models.
\newblock In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, \emph{Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pages 8222--8234, Bangkok, Thailand, August 2024{\natexlab{g}}. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2024.acl-long.447}.
\newblock URL \url{https://aclanthology.org/2024.acl-long.447}.

\bibitem[Chen et~al.(2022)Chen, Chu, Wiseman, and Gimpel]{chen2022summscreen}
Mingda Chen, Zewei Chu, Sam Wiseman, and Kevin Gimpel.
\newblock Summscreen: A dataset for abstractive screenplay summarization.
\newblock In \emph{Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pages 8602--8615, 2022.

\bibitem[Chen et~al.(2023{\natexlab{c}})Chen, Wong, Chen, and Tian]{chen2023extending}
Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian.
\newblock Extending context window of large language models via positional interpolation.
\newblock \emph{arXiv preprint arXiv:2306.15595}, 2023{\natexlab{c}}.

\bibitem[Chen and Goodman(1999)]{chen1999empirical}
Stanley~F Chen and Joshua Goodman.
\newblock An empirical study of smoothing techniques for language modeling.
\newblock \emph{Computer Speech \& Language}, 13\penalty0 (4):\penalty0 359--394, 1999.

\bibitem[Chen et~al.(2024{\natexlab{h}})Chen, Fang, Xia, Liu, Durme, Zettlemoyer, Gao, and Cheng]{chen2024generative}
Tong Chen, Hao Fang, Patrick Xia, Xiaodong Liu, Benjamin~Van Durme, Luke Zettlemoyer, Jianfeng Gao, and Hao Cheng.
\newblock Generative adapter: Contextualizing language models in parameters with a single forward pass.
\newblock \emph{arXiv preprint arXiv: 2411.05877}, 2024{\natexlab{h}}.

\bibitem[Chen et~al.(2024{\natexlab{i}})Chen, Xu, Liang, He, Pang, Yu, Song, Liu, Zhou, Zhang, et~al.]{chen2024not}
Xingyu Chen, Jiahao Xu, Tian Liang, Zhiwei He, Jianhui Pang, Dian Yu, Linfeng Song, Qiuzhi Liu, Mengfei Zhou, Zhuosheng Zhang, et~al.
\newblock Do not think that much for 2+ 3=? on the overthinking of o1-like llms.
\newblock \emph{arXiv preprint arXiv:2412.21187}, 2024{\natexlab{i}}.

\bibitem[Chen et~al.(2024{\natexlab{j}})Chen, Zhang, Hu, Han, Liu, and Sun]{chen2024stuffed}
Yingfa Chen, Xinrong Zhang, Shengding Hu, Xu~Han, Zhiyuan Liu, and Maosong Sun.
\newblock Stuffed mamba: State collapse and state capacity of rnn-based long-context modeling.
\newblock \emph{arXiv preprint arXiv:2410.07145}, 2024{\natexlab{j}}.

\bibitem[Chen et~al.(2024{\natexlab{k}})Chen, Zhang, Hu, Han, Liu, and Sun]{chen2024stuffedmambastatecollapse}
Yingfa Chen, Xinrong Zhang, Shengding Hu, Xu~Han, Zhiyuan Liu, and Maosong Sun.
\newblock Stuffed mamba: State collapse and state capacity of rnn-based long-context modeling, 2024{\natexlab{k}}.
\newblock URL \url{https://arxiv.org/abs/2410.07145}.

\bibitem[Chen et~al.(2023{\natexlab{d}})Chen, Lv, Lin, Chen, Wu, Huang, Li, and Yan]{chen2023fortify}
Yuhan Chen, Ang Lv, Ting-En Lin, Changyu Chen, Yuchuan Wu, Fei Huang, Yongbin Li, and Rui Yan.
\newblock Fortify the shortest stave in attention: Enhancing context awareness of large language models for effective tool use.
\newblock \emph{arXiv preprint arXiv:2312.04455}, 2023{\natexlab{d}}.

\bibitem[Chen et~al.(2024{\natexlab{l}})Chen, Lv, Luan, Wang, and Liu]{chen2024hope}
Yuhan Chen, Ang Lv, Jian Luan, Bin Wang, and Wei Liu.
\newblock Hope: A novel positional encoding without long-term decay for enhanced context awareness and extrapolation.
\newblock \emph{arXiv preprint arXiv:2410.21216}, 2024{\natexlab{l}}.

\bibitem[Chen et~al.(2023{\natexlab{e}})Chen, Qian, Tang, Lai, Liu, Han, and Jia]{chen2023longlora}
Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, and Jiaya Jia.
\newblock Longlora: Efficient fine-tuning of long-context large language models.
\newblock \emph{arXiv preprint arXiv:2309.12307}, 2023{\natexlab{e}}.

\bibitem[Chen et~al.(2024{\natexlab{m}})Chen, Qian, Tang, Lai, Liu, Han, and Jia]{longlora}
Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, and Jiaya Jia.
\newblock Longlora: Efficient fine-tuning of long-context large language models.
\newblock In \emph{The International Conference on Learning Representations (ICLR)}, 2024{\natexlab{m}}.

\bibitem[Chen et~al.(2024{\natexlab{n}})Chen, Wang, Tian, Ye, Gao, Cui, Tong, Hu, Luo, Ma, Ma, Wang, Dong, Yan, Guo, He, Shi, Jin, Xu, Wang, Wei, Li, Zhang, Zhang, Cai, Wen, Yan, Dou, Lu, Zhu, Lu, Lin, Qiao, Dai, and Wang]{chen-arxiv-2024-internvl15}
Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, Ji~Ma, Jiaqi Wang, Xiaoyi Dong, Hang Yan, Hewei Guo, Conghui He, Botian Shi, Zhenjiang Jin, Chao Xu, Bin Wang, Xingjian Wei, Wei Li, Wenjian Zhang, Bo~Zhang, Pinlong Cai, Licheng Wen, Xiangchao Yan, Min Dou, Lewei Lu, Xizhou Zhu, Tong Lu, Dahua Lin, Yu~Qiao, Jifeng Dai, and Wenhai Wang.
\newblock How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites.
\newblock \emph{CoRR}, abs/2404.16821, 2024{\natexlab{n}}.
\newblock \doi{10.48550/ARXIV.2404.16821}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2404.16821}.

\bibitem[Chen et~al.(2024{\natexlab{o}})Chen, Chen, Qin, Guo, Lv, Zou, Che, Yan, Chen, and Lin]{Chen2024WhatAT}
Zhi Chen, Qiguang Chen, Libo Qin, Qipeng Guo, Haijun Lv, Yicheng Zou, Wanxiang Che, Hang Yan, Kai Chen, and Dahua Lin.
\newblock What are the essential factors in crafting effective long context multi-hop instruction datasets? insights and best practices.
\newblock \emph{ArXiv}, abs/2409.01893, 2024{\natexlab{o}}.

\bibitem[Cheng et~al.(2024)Cheng, Wang, Zhang, Ge, Chen, Wei, Zhang, and Zhao]{cheng2024xrag}
Xin Cheng, Xun Wang, Xingxing Zhang, Tao Ge, Si-Qing Chen, Furu Wei, Huishuai Zhang, and Dongyan Zhao.
\newblock x{RAG}: Extreme context compression for retrieval-augmented generation with one token.
\newblock In \emph{The Thirty-eighth Annual Conference on Neural Information Processing Systems}, 2024.
\newblock URL \url{https://openreview.net/forum?id=6pTlXqrO0p}.

\bibitem[Chevalier et~al.(2023)Chevalier, Wettig, Ajith, and Chen]{chevalier-etal-2023-adapting}
Alexis Chevalier, Alexander Wettig, Anirudh Ajith, and Danqi Chen.
\newblock Adapting language models to compress contexts.
\newblock In Houda Bouamor, Juan Pino, and Kalika Bali, editors, \emph{Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing}, pages 3829--3846, Singapore, December 2023. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2023.emnlp-main.232}.
\newblock URL \url{https://aclanthology.org/2023.emnlp-main.232/}.

\bibitem[Chi et~al.(2022{\natexlab{a}})Chi, Fan, Ramadge, and Rudnicky]{chi2022kerple}
Ta-Chung Chi, Ting-Han Fan, Peter~J Ramadge, and Alexander Rudnicky.
\newblock Kerple: Kernelized relative positional embedding for length extrapolation.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 8386--8399, 2022{\natexlab{a}}.

\bibitem[Chi et~al.(2022{\natexlab{b}})Chi, Fan, Rudnicky, and Ramadge]{chi2022dissecting}
Ta-Chung Chi, Ting-Han Fan, Alexander~I Rudnicky, and Peter~J Ramadge.
\newblock Dissecting transformer length extrapolation via the lens of receptive field analysis.
\newblock \emph{arXiv preprint arXiv:2212.10356}, 2022{\natexlab{b}}.

\bibitem[Chi et~al.(2023)Chi, Fan, Chen, Rudnicky, and Ramadge]{chi2023latent}
Ta-Chung Chi, Ting-Han Fan, Li-Wei Chen, Alexander~I Rudnicky, and Peter~J Ramadge.
\newblock Latent positional information is in the self-attention variance of transformer language models without positional embeddings.
\newblock \emph{arXiv preprint arXiv:2305.13571}, 2023.

\bibitem[Chia et~al.(2024)Chia, Cheng, Chan, Liu, Song, Aljunied, Poria, and Bing]{chia-2024-arxiv-mlongdoc}
Yew~Ken Chia, Liying Cheng, Hou~Pong Chan, Chaoqun Liu, Maojia Song, Sharifah~Mahani Aljunied, Soujanya Poria, and Lidong Bing.
\newblock M-longdoc: {A} benchmark for multimodal super-long document understanding and {A} retrieval-aware tuning framework.
\newblock \emph{CoRR}, abs/2411.06176, 2024.
\newblock \doi{10.48550/ARXIV.2411.06176}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2411.06176}.

\bibitem[Child et~al.(2019)Child, Gray, Radford, and Sutskever]{child2019generatinglongsequencessparse}
Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever.
\newblock Generating long sequences with sparse transformers, 2019.
\newblock URL \url{https://arxiv.org/abs/1904.10509}.

\bibitem[Chinnakonduru and Mohapatra(2024)]{chinnakonduru2024weightedgroupedqueryattention}
Sai~Sena Chinnakonduru and Astarag Mohapatra.
\newblock Weighted grouped query attention in transformers, 2024.
\newblock URL \url{https://arxiv.org/abs/2407.10855}.

\bibitem[Cho et~al.(2018)Cho, Zhang, Zhang, Li, Galley, Brockett, Wang, and Gao]{cho2018towards}
Woon~Sang Cho, Pengchuan Zhang, Yizhe Zhang, Xiujun Li, Michel Galley, Chris Brockett, Mengdi Wang, and Jianfeng Gao.
\newblock Towards coherent and cohesive long-form text generation.
\newblock \emph{arXiv preprint arXiv:1811.00511}, 2018.

\bibitem[Choromanski et~al.(2021)Choromanski, Likhosherstov, Dohan, Song, Gane, Sarl{\'{o}}s, Hawkins, Davis, Mohiuddin, Kaiser, Belanger, Colwell, and Weller]{choromanski2022rethinkingattentionperformers}
Krzysztof~Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tam{\'{a}}s Sarl{\'{o}}s, Peter Hawkins, Jared~Quincy Davis, Afroz Mohiuddin, Lukasz Kaiser, David~Benjamin Belanger, Lucy~J. Colwell, and Adrian Weller.
\newblock Rethinking attention with performers.
\newblock In \emph{{ICLR}}. OpenReview.net, 2021.

\bibitem[Chowdhery et~al.(2024)Chowdhery, Narang, Devlin, Bosma, Mishra, Roberts, Barham, Chung, Sutton, Gehrmann, Schuh, Shi, Tsvyashchenko, Maynez, Rao, Barnes, Tay, Shazeer, Prabhakaran, Reif, Du, Hutchinson, Pope, Bradbury, Austin, Isard, Gur-Ari, Yin, Duke, Levskaya, Ghemawat, Dev, Michalewski, Garcia, Misra, Robinson, Fedus, Zhou, Ippolito, Luan, Lim, Zoph, Spiridonov, Sepassi, Dohan, Agrawal, Omernick, Dai, Pillai, Pellat, Lewkowycz, Moreira, Child, Polozov, Lee, Zhou, Wang, Saeta, Diaz, Firat, Catasta, Wei, Meier-Hellstern, Eck, Dean, Petrov, and Fiedel]{10.5555/3648699.3648939}
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung~Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sashank Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi~Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew~M. Dai, Thanumalayan~Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel.
\newblock Palm: scaling language modeling with pathways.
\newblock \emph{J. Mach. Learn. Res.}, 24\penalty0 (1), March 2024.
\newblock ISSN 1532-4435.

\bibitem[Chuang et~al.(2024)Chuang, Xing, Chang, Liu, Chen, and Hu]{chuang-etal-2024-learning}
Yu-Neng Chuang, Tianwei Xing, Chia-Yuan Chang, Zirui Liu, Xun Chen, and Xia Hu.
\newblock Learning to compress prompt in natural language formats.
\newblock In Kevin Duh, Helena Gomez, and Steven Bethard, editors, \emph{Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)}, pages 7756--7767, Mexico City, Mexico, June 2024. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2024.naacl-long.429}.
\newblock URL \url{https://aclanthology.org/2024.naacl-long.429}.

\bibitem[Churin et~al.(2024)Churin, Apishev, Tikhonova, Shevelev, Bulatov, Kuratov, Averkiev, and Fenogenova]{churin2024long}
Igor Churin, Murat Apishev, Maria Tikhonova, Denis Shevelev, Aydar Bulatov, Yuri Kuratov, Sergej Averkiev, and Alena Fenogenova.
\newblock Long input benchmark for russian analysis.
\newblock \emph{arXiv preprint arXiv:2408.02439}, 2024.

\bibitem[Cobbe et~al.(2021)Cobbe, Kosaraju, Bavarian, Chen, Jun, Kaiser, Plappert, Tworek, Hilton, Nakano, et~al.]{cobbe2021training}
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et~al.
\newblock Training verifiers to solve math word problems.
\newblock \emph{arXiv preprint arXiv:2110.14168}, 2021.

\bibitem[Cohere and AI(2024)]{c4ai-command-r7b-12-2024}
Cohere and Cohere~For AI.
\newblock C4ai command r7b: A 7 billion parameter multilingual model, December 2024.
\newblock URL \url{https://huggingface.co/CohereForAI/c4ai-command-r7b-12-2024}.
\newblock Access requires agreement to the License Agreement and adherence to C4AI's Acceptable Use Policy.

\bibitem[Costa-juss{\`a} et~al.(2024)Costa-juss{\`a}, Andrews, Meglioli, Chen, Chuang, Dale, Ropers, Mourachko, S{\'a}nchez, Schwenk, et~al.]{costa2024lcfo}
Marta~R Costa-juss{\`a}, Pierre Andrews, Mariano~Coria Meglioli, Joy Chen, Joe Chuang, David Dale, Christophe Ropers, Alexandre Mourachko, Eduardo S{\'a}nchez, Holger Schwenk, et~al.
\newblock Lcfo: Long context and long form output dataset and benchmarking.
\newblock \emph{arXiv preprint arXiv:2412.08268}, 2024.

\bibitem[Dai et~al.(2024{\natexlab{a}})Dai, Pechi, Yang, Banga, and Mantri]{dai2024deniahl}
Hui Dai, Dan Pechi, Xinyi Yang, Garvit Banga, and Raghav Mantri.
\newblock Deniahl: In-context features influence llm needle-in-a-haystack abilities.
\newblock \emph{arXiv preprint arXiv:2411.19360}, 2024{\natexlab{a}}.

\bibitem[Dai et~al.(2024{\natexlab{b}})Dai, Huang, Jiang, Chen, Cai, Bi, and Shi]{dai2024cormcacheoptimizationrecent}
Jincheng Dai, Zhuowei Huang, Haiyun Jiang, Chen Chen, Deng Cai, Wei Bi, and Shuming Shi.
\newblock Corm: Cache optimization with recent message for large language model inference, 2024{\natexlab{b}}.
\newblock URL \url{https://arxiv.org/abs/2404.15949}.

\bibitem[Dai et~al.(2019)Dai, Yang, Yang, Carbonell, Le, and Salakhutdinov]{TransformerXL}
Zihang Dai, Zhilin Yang, Yiming Yang, Jaime~G. Carbonell, Quoc~Viet Le, and Ruslan Salakhutdinov.
\newblock Transformer-xl: Attentive language models beyond a fixed-length context.
\newblock In \emph{{ACL} {(1)}}, pages 2978--2988. Association for Computational Linguistics, 2019.

\bibitem[Dalvi~Mishra et~al.(2022)Dalvi~Mishra, Tafjord, and Clark]{towards-teachable-reasoning-systems}
Bhavana Dalvi~Mishra, Oyvind Tafjord, and Peter Clark.
\newblock Towards teachable reasoning systems: Using a dynamic memory of user feedback for continual system improvement.
\newblock In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, \emph{Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing}, pages 9465--9480, Abu Dhabi, United Arab Emirates, dec 2022. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2022.emnlp-main.644}.
\newblock URL \url{https://aclanthology.org/2022.emnlp-main.644}.

\bibitem[Dao(2024)]{dao2024flashattention}
Tri Dao.
\newblock Flashattention-2: Faster attention with better parallelism and work partitioning.
\newblock In \emph{The Twelfth International Conference on Learning Representations}, 2024.

\bibitem[Dao et~al.(2022)Dao, Fu, Ermon, Rudra, and R{\'e}]{dao2022flashattention}
Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R{\'e}.
\newblock Flashattention: Fast and memory-efficient exact attention with io-awareness.
\newblock \emph{Advances in neural information processing systems}, 35:\penalty0 16344--16359, 2022.

\bibitem[Dasigi et~al.(2021)Dasigi, Lo, Beltagy, Cohan, Smith, and Gardner]{dasigi2021dataset}
Pradeep Dasigi, Kyle Lo, Iz~Beltagy, Arman Cohan, Noah~A Smith, and Matt Gardner.
\newblock A dataset of information-seeking questions and answers anchored in research papers.
\newblock In \emph{Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies}, pages 4599--4610, 2021.

\bibitem[DeepSeek(2023)]{deepseek_web_2023}
DeepSeek.
\newblock Deepseek official website, 2023.
\newblock URL \url{https://www.deepseek.com/}.

\bibitem[DeepSeek(2025)]{deepseek3fs}
DeepSeek.
\newblock 3fs: Fire-flyer file system.
\newblock \url{https://github.com/deepseek-ai/3FS}, 2025.

\bibitem[DeepSeek-AI(2024)]{deepseekai2024deepseekv3technicalreport}
DeepSeek-AI.
\newblock Deepseek-v3 technical report, 2024.
\newblock URL \url{https://arxiv.org/abs/2412.19437}.

\bibitem[Dehghani et~al.(2018)Dehghani, Gouws, Vinyals, Uszkoreit, and Kaiser]{dehghani2018universal}
Mostafa Dehghani, Stephan Gouws, O.~Vinyals, Jakob Uszkoreit, and Lukasz Kaiser.
\newblock Universal transformers.
\newblock \emph{International Conference on Learning Representations}, 2018.

\bibitem[Dettmers et~al.(2024)Dettmers, Pagnoni, Holtzman, and Zettlemoyer]{dettmers2024qlora}
Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer.
\newblock Qlora: Efficient finetuning of quantized llms.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[Devlin et~al.(2019{\natexlab{a}})Devlin, Chang, Lee, and Toutanova]{Devlin2019BERTPO}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language understanding.
\newblock In \emph{North American Chapter of the Association for Computational Linguistics}, 2019{\natexlab{a}}.

\bibitem[Devlin et~al.(2019{\natexlab{b}})Devlin, Chang, Lee, and Toutanova]{devlin-etal-2019-bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock {BERT}: Pre-training of deep bidirectional transformers for language understanding.
\newblock In Jill Burstein, Christy Doran, and Thamar Solorio, editors, \emph{Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)}, pages 4171--4186, Minneapolis, Minnesota, June 2019{\natexlab{b}}. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/N19-1423}.
\newblock URL \url{https://aclanthology.org/N19-1423/}.

\bibitem[Ding et~al.(2024)Ding, Zhang, Zhang, Xu, Shang, Xu, Yang, and Yang]{ding2024longrope}
Yiran Ding, Li~Lyna Zhang, Chengruidong Zhang, Yuanyuan Xu, Ning Shang, Jiahang Xu, Fan Yang, and Mao Yang.
\newblock Longrope: Extending llm context window beyond 2 million tokens.
\newblock \emph{arXiv preprint arXiv:2402.13753}, 2024.

\bibitem[Dong et~al.(2020)Dong, Cao, Zhang, Ye, Wang, Feng, Zhao, Liu, Song, Peng, et~al.]{dong2020eflops}
Jianbo Dong, Zheng Cao, Tao Zhang, Jianxi Ye, Shaochuang Wang, Fei Feng, Li~Zhao, Xiaoyong Liu, Liuyihan Song, Liwei Peng, et~al.
\newblock Eflops: Algorithm and system co-design for a high performance distributed training platform.
\newblock In \emph{2020 IEEE International Symposium on High Performance Computer Architecture (HPCA)}, pages 610--622. IEEE, 2020.

\bibitem[Dong et~al.(2024{\natexlab{a}})Dong, Cheng, Qin, and Wang]{dong2024qaqqualityadaptivequantization}
Shichen Dong, Wen Cheng, Jiayu Qin, and Wei Wang.
\newblock Qaq: Quality adaptive quantization for llm kv cache, 2024{\natexlab{a}}.
\newblock URL \url{https://arxiv.org/abs/2403.04643}.

\bibitem[Dong et~al.(2024{\natexlab{b}})Dong, Fu, Diao, Byeon, Chen, Mahabaleshwarkar, Liu, Van~Keirsbilck, Chen, Suhara, et~al.]{dong2024hymba}
Xin Dong, Yonggan Fu, Shizhe Diao, Wonmin Byeon, Zijia Chen, Ameya~Sunil Mahabaleshwarkar, Shih-Yang Liu, Matthijs Van~Keirsbilck, Min-Hung Chen, Yoshi Suhara, et~al.
\newblock Hymba: A hybrid-head architecture for small language models.
\newblock \emph{arXiv preprint arXiv:2411.13676}, 2024{\natexlab{b}}.

\bibitem[Dong et~al.(2023{\natexlab{a}})Dong, Tang, Li, and Zhao]{DBLP:journals/corr/abs-2302-14502}
Zican Dong, Tianyi Tang, Junyi Li, and Wayne~Xin Zhao.
\newblock A survey on long text modeling with transformers.
\newblock \emph{CoRR}, abs/2302.14502, 2023{\natexlab{a}}.
\newblock \doi{10.48550/ARXIV.2302.14502}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2302.14502}.

\bibitem[Dong et~al.(2023{\natexlab{b}})Dong, Tang, Li, Zhao, and Wen]{dong2023bamboo}
Zican Dong, Tianyi Tang, Junyi Li, Wayne~Xin Zhao, and Ji-Rong Wen.
\newblock Bamboo: A comprehensive benchmark for evaluating long text modeling capacities of large language models.
\newblock \emph{arXiv preprint arXiv:2309.13345}, 2023{\natexlab{b}}.

\bibitem[Dou et~al.(2025)Dou, Liu, Zhou, Chen, Wang, Jin, Liu, Zhu, Du, Yang, Wang, Liu, Zhao, Feng, Mao, Yeung, Pipatanakul, Koto, Thu, Kydl{\'\i}{\v{c}}ek, Liu, Lin, Sripaisarnmongkol, Sae-Khow, Thongchim, Konkaew, Borijindargoon, Dao, Maneegard, Artkaew, Yong, Nguyen, Phatthiyaphaibun, Tran, Zhang, Chen, Pang, Du, Wan, Lu, and Lin]{sailor2report}
Longxu Dou, Qian Liu, Fan Zhou, Changyu Chen, Zili Wang, Ziqi Jin, Zichen Liu, Tongyao Zhu, Cunxiao Du, Penghui Yang, Haonan Wang, Jiaheng Liu, Yongchi Zhao, Xiachong Feng, Xin Mao, Man~Tsung Yeung, Kunat Pipatanakul, Fajri Koto, Min~Si Thu, Hynek Kydl{\'\i}{\v{c}}ek, Zeyi Liu, Qunshu Lin, Sittipong Sripaisarnmongkol, Kridtaphad Sae-Khow, Nirattisai Thongchim, Taechawat Konkaew, Narong Borijindargoon, Anh Dao, Matichon Maneegard, Phakphum Artkaew, Zheng-Xin Yong, Quan Nguyen, Wannaphong Phatthiyaphaibun, Hoang~H. Tran, Mike Zhang, Shiqi Chen, Tianyu Pang, Chao Du, Xinyi Wan, Wei Lu, and Min Lin.
\newblock Sailor2: Sailing in south-east asia with inclusive multilingual llm.
\newblock \emph{arXiv preprint arXiv:2502.12982}, 2025.

\bibitem[Du et~al.(2021)Du, Huang, Dai, Tong, Lepikhin, Xu, Krikun, Zhou, Yu, Firat, Zoph, Fedus, Bosma, Zhou, Wang, Wang, Webster, Pellat, Robinson, Meier-Hellstern, Duke, Dixon, Zhang, Le, Wu, Chen, and Cui]{Du2021GLaMES}
Nan Du, Yanping Huang, Andrew~M. Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun, Yanqi Zhou, Adams~Wei Yu, Orhan Firat, Barret Zoph, Liam Fedus, Maarten Bosma, Zongwei Zhou, Tao Wang, Yu~Emma Wang, Kellie Webster, Marie Pellat, Kevin Robinson, Kathleen~S. Meier-Hellstern, Toju Duke, Lucas Dixon, Kun Zhang, Quoc~V. Le, Yonghui Wu, Z.~Chen, and Claire Cui.
\newblock Glam: Efficient scaling of language models with mixture-of-experts.
\newblock In \emph{International Conference on Machine Learning}, 2021.

\bibitem[Du et~al.(2022{\natexlab{a}})Du, Huang, Dai, Tong, Lepikhin, Xu, Krikun, Zhou, Yu, Firat, Zoph, Fedus, Bosma, Zhou, Wang, Wang, Webster, Pellat, Robinson, Meier{-}Hellstern, Duke, Dixon, Zhang, Le, Wu, Chen, and Cui]{glam}
Nan Du, Yanping Huang, Andrew~M. Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun, Yanqi Zhou, Adams~Wei Yu, Orhan Firat, Barret Zoph, Liam Fedus, Maarten~P. Bosma, Zongwei Zhou, Tao Wang, Yu~Emma Wang, Kellie Webster, Marie Pellat, Kevin Robinson, Kathleen~S. Meier{-}Hellstern, Toju Duke, Lucas Dixon, Kun Zhang, Quoc~V. Le, Yonghui Wu, Zhifeng Chen, and Claire Cui.
\newblock Glam: Efficient scaling of language models with mixture-of-experts.
\newblock In Kamalika Chaudhuri, Stefanie Jegelka, Le~Song, Csaba Szepesv{\'{a}}ri, Gang Niu, and Sivan Sabato, editors, \emph{International Conference on Machine Learning, {ICML} 2022, 17-23 July 2022, Baltimore, Maryland, {USA}}, volume 162 of \emph{Proceedings of Machine Learning Research}, pages 5547--5569. {PMLR}, 2022{\natexlab{a}}.
\newblock URL \url{https://proceedings.mlr.press/v162/du22c.html}.

\bibitem[Du et~al.(2024)Du, Zhou, Huo, Li, Zhao, Lu, Zhao, Wang, Chen, and Wen]{DBLP:journals/corr/abs-2406-14129}
Yifan Du, Kun Zhou, Yuqi Huo, Yifan Li, Wayne~Xin Zhao, Haoyu Lu, Zijia Zhao, Bingning Wang, Weipeng Chen, and Ji{-}Rong Wen.
\newblock Towards event-oriented long video understanding.
\newblock \emph{CoRR}, abs/2406.14129, 2024.
\newblock \doi{10.48550/ARXIV.2406.14129}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2406.14129}.

\bibitem[Du et~al.(2022{\natexlab{b}})Du, Qian, Liu, Ding, Qiu, Yang, and Tang]{du-etal-2022-glm}
Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang.
\newblock {GLM}: General language model pretraining with autoregressive blank infilling.
\newblock In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors, \emph{Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pages 320--335, Dublin, Ireland, May 2022{\natexlab{b}}. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2022.acl-long.26}.
\newblock URL \url{https://aclanthology.org/2022.acl-long.26/}.

\bibitem[Duan et~al.(2024)Duan, Zhang, Wang, Jiang, Qu, Hu, Wang, Weng, Yan, Zhang, et~al.]{duan2024efficient}
Jiangfei Duan, Shuo Zhang, Zerui Wang, Lijuan Jiang, Wenwen Qu, Qinghao Hu, Guoteng Wang, Qizhen Weng, Hang Yan, Xingcheng Zhang, et~al.
\newblock Efficient training of large language models on distributed infrastructures: a survey.
\newblock \emph{arXiv preprint arXiv:2407.20018}, 2024.

\bibitem[Duanmu et~al.()Duanmu, Yuan, Li, Duan, ZHANG, and Lin]{duanmuskvq}
Haojie Duanmu, Zhihang Yuan, Xiuhong Li, Jiangfei Duan, Xingcheng ZHANG, and Dahua Lin.
\newblock Skvq: Sliding-window key and value cache quantization for large language models.
\newblock In \emph{First Conference on Language Modeling}.

\bibitem[Elhoushi et~al.(2024)Elhoushi, Shrivastava, Liskovich, Hosmer, Wasti, Lai, Mahmoud, Acun, Agarwal, Roman, Aly, Chen, and Wu]{elhoushi-etal-2024-layerskip}
Mostafa Elhoushi, Akshat Shrivastava, Diana Liskovich, Basil Hosmer, Bram Wasti, Liangzhen Lai, Anas Mahmoud, Bilge Acun, Saurabh Agarwal, Ahmed Roman, Ahmed Aly, Beidi Chen, and Carole-Jean Wu.
\newblock {L}ayer{S}kip: Enabling early exit inference and self-speculative decoding.
\newblock In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, \emph{Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, August 2024.

\bibitem[Fabbri et~al.(2021)Fabbri, Wu, Liu, and Xiong]{fabbri2021qafacteval}
Alexander~R Fabbri, Chien-Sheng Wu, Wenhao Liu, and Caiming Xiong.
\newblock Qafacteval: Improved qa-based factual consistency evaluation for summarization.
\newblock \emph{arXiv preprint arXiv:2112.08542}, 2021.

\bibitem[Fabbri et~al.(2019)Fabbri, Li, She, Li, and Radev]{fabbri2019multi}
Alexander~Richard Fabbri, Irene Li, Tianwei She, Suyi Li, and Dragomir Radev.
\newblock Multi-news: A large-scale multi-document summarization dataset and abstractive hierarchical model.
\newblock In \emph{Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics}, pages 1074--1084, 2019.

\bibitem[Fan et~al.(2019)Fan, Jernite, Perez, Grangier, Weston, and Auli]{fan2019eli5}
Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli.
\newblock Eli5: Long form question answering.
\newblock \emph{arXiv preprint arXiv:1907.09190}, 2019.

\bibitem[Fan et~al.(2024{\natexlab{a}})Fan, Sun, Xue, Zhang, Zhang, and Ruan]{DBLP:journals/corr/abs-2406-15019}
Yongqi Fan, Hongli Sun, Kui Xue, Xiaofan Zhang, Shaoting Zhang, and Tong Ruan.
\newblock Medodyssey: {A} medical domain benchmark for long context evaluation up to 200k tokens.
\newblock \emph{CoRR}, abs/2406.15019, 2024{\natexlab{a}}.
\newblock \doi{10.48550/ARXIV.2406.15019}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2406.15019}.

\bibitem[Fan et~al.(2024{\natexlab{b}})Fan, Sun, Xue, Zhang, Zhang, and Ruan]{fan2024medodyssey}
Yongqi Fan, Hongli Sun, Kui Xue, Xiaofan Zhang, Shaoting Zhang, and Tong Ruan.
\newblock Medodyssey: A medical domain benchmark for long context evaluation up to 200k tokens.
\newblock \emph{arXiv preprint arXiv:2406.15019}, 2024{\natexlab{b}}.

\bibitem[Fang et~al.(2024{\natexlab{a}})Fang, Wang, Liu, Zhang, Jegelka, Gao, Ding, and Wang]{fang2024wrong}
Lizhe Fang, Yifei Wang, Zhaoyang Liu, Chenheng Zhang, Stefanie Jegelka, Jinyang Gao, Bolin Ding, and Yisen Wang.
\newblock What is wrong with perplexity for long-context language modeling?
\newblock \emph{arXiv preprint arXiv:2410.23771}, 2024{\natexlab{a}}.

\bibitem[Fang et~al.(2024{\natexlab{b}})Fang, Zhan, Ai, Mao, Su, Chen, and Liu]{fang2024scaling}
Yan Fang, Jingtao Zhan, Qingyao Ai, Jiaxin Mao, Weihang Su, Jia Chen, and Yiqun Liu.
\newblock Scaling laws for dense retrieval.
\newblock \emph{arXiv preprint arXiv: 2403.18684}, 2024{\natexlab{b}}.

\bibitem[Faure et~al.(2024)Faure, Yeh, Chen, Su, Lai, and Hsu]{faure-2024-arxiv-hermes}
Gueter~Josmy Faure, Jia-Fong Yeh, Min-Hung Chen, Hung-Ting Su, Shang-Hong Lai, and Winston~H. Hsu.
\newblock Hermes: temporal-coherent long-form understanding with episodes and semantics.
\newblock \emph{CoRR}, abs/2408.17443, 2024.
\newblock \doi{10.48550/ARXIV.2408.17443}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2408.17443}.

\bibitem[Feng et~al.(2025)Feng, Lv, Cao, Xie, and Zhou]{feng2025adakvoptimizingkvcache}
Yuan Feng, Junlin Lv, Yukun Cao, Xike Xie, and S.~Kevin Zhou.
\newblock Ada-kv: Optimizing kv cache eviction by adaptive budget allocation for efficient llm inference, 2025.
\newblock URL \url{https://arxiv.org/abs/2407.11550}.

\bibitem[Fu et~al.(2024{\natexlab{a}})Fu, Cho, Merth, Mehta, Rastegari, and Najibi]{fu2024lazyllmdynamictokenpruning}
Qichen Fu, Minsik Cho, Thomas Merth, Sachin Mehta, Mohammad Rastegari, and Mahyar Najibi.
\newblock Lazyllm: Dynamic token pruning for efficient long context llm inference, 2024{\natexlab{a}}.
\newblock URL \url{https://arxiv.org/abs/2407.14057}.

\bibitem[Fu et~al.(2024{\natexlab{b}})Fu, Huang, Ning, Zhang, Chen, Wu, Wang, Huang, Li, Yan, Dai, Yang, and Wang]{fu2024moamixturesparseattention}
Tianyu Fu, Haofeng Huang, Xuefei Ning, Genghan Zhang, Boju Chen, Tianqi Wu, Hongyi Wang, Zixiao Huang, Shiyao Li, Shengen Yan, Guohao Dai, Huazhong Yang, and Yu~Wang.
\newblock Moa: Mixture of sparse attention for automatic large language model compression, 2024{\natexlab{b}}.
\newblock URL \url{https://arxiv.org/abs/2406.14909}.

\bibitem[Fu et~al.(2024{\natexlab{c}})Fu, Panda, Niu, Yue, Hajishirzi, Kim, and Peng]{Fu2024DataEF}
Yao Fu, Rameswar Panda, Xinyao Niu, Xiang Yue, Hanna Hajishirzi, Yoon Kim, and Hao Peng.
\newblock Data engineering for scaling language models to 128k context.
\newblock \emph{ArXiv}, abs/2402.10171, 2024{\natexlab{c}}.

\bibitem[Fu et~al.(2024{\natexlab{d}})Fu, Cai, Asi, Xiong, Dong, and Xiao]{fu2024headsmatterheadlevelkv}
Yu~Fu, Zefan Cai, Abedelkadir Asi, Wayne Xiong, Yue Dong, and Wen Xiao.
\newblock Not all heads matter: A head-level kv cache compression method with integrated retrieval and reasoning, 2024{\natexlab{d}}.
\newblock URL \url{https://arxiv.org/abs/2410.19258}.

\bibitem[Fu et~al.(2025)Fu, Cai, Asi, Xiong, Dong, and Xiao]{fu2025not}
Yu~Fu, Zefan Cai, Abedelkadir Asi, Wayne Xiong, Yue Dong, and Wen Xiao.
\newblock Not all heads matter: A head-level {KV} cache compression method with integrated retrieval and reasoning.
\newblock In \emph{The Thirteenth International Conference on Learning Representations}, 2025.
\newblock URL \url{https://openreview.net/forum?id=FJFVmeXusW}.

\bibitem[Gao et~al.(2024{\natexlab{a}})Gao, Wu, Fu, and Hu]{quest}
Chaochen Gao, Xing Wu, Qingfang Fu, and Songlin Hu.
\newblock Quest: Query-centric data synthesis approach for long-context scaling of large language model.
\newblock \emph{ArXiv}, abs/2405.19846, 2024{\natexlab{a}}.

\bibitem[Gao et~al.(2024{\natexlab{b}})Gao, Cao, and Li]{gao2024unifyingdemonstrationselectioncompression}
Jun Gao, Ziqiang Cao, and Wenjie Li.
\newblock Unifying demonstration selection and compression for in-context learning, 2024{\natexlab{b}}.
\newblock URL \url{https://arxiv.org/abs/2405.17062}.

\bibitem[Gao et~al.(2020)Gao, Biderman, Black, Golding, Hoppe, Foster, Phang, He, Thite, Nabeshima, Presser, and Leahy]{Gao2020ThePA}
Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy.
\newblock The pile: An 800gb dataset of diverse text for language modeling.
\newblock \emph{ArXiv}, abs/2101.00027, 2020.

\bibitem[Gao et~al.(2022)Gao, Ma, Lin, and Callan]{hyde}
Luyu Gao, Xueguang Ma, Jimmy Lin, and Jamie Callan.
\newblock Precise zero-shot dense retrieval without relevance labels.
\newblock \emph{arXiv preprint arXiv: 2212.10496}, 2022.

\bibitem[Gao et~al.(2019)Gao, Chen, Li, Ren, Bing, Zhao, and Yan]{gao2019abstractive}
Shen Gao, Xiuying Chen, Piji Li, Zhaochun Ren, Lidong Bing, Dongyan Zhao, and Rui Yan.
\newblock Abstractive text summarization by incorporating reader comments.
\newblock In \emph{The Thirty-Third {AAAI} Conference on Artificial Intelligence, {AAAI} 2019, The Thirty-First Innovative Applications of Artificial Intelligence Conference, {IAAI} 2019, The Ninth {AAAI} Symposium on Educational Advances in Artificial Intelligence, {EAAI} 2019, Honolulu, Hawaii, USA, January 27 - February 1, 2019}, pages 6399--6406. {AAAI} Press, 2019.
\newblock \doi{10.1609/AAAI.V33I01.33016399}.
\newblock URL \url{https://doi.org/10.1609/aaai.v33i01.33016399}.

\bibitem[Gao et~al.(2024{\natexlab{c}})Gao, Wettig, Yen, and Chen]{gao2024prolong}
Tianyu Gao, Alexander Wettig, Howard Yen, and Danqi Chen.
\newblock How to train long-context language models (effectively).
\newblock \emph{arXiv preprint arXiv:2410.02660}, 2024{\natexlab{c}}.

\bibitem[Gao et~al.(2024{\natexlab{d}})Gao, Zeng, Du, Cao, So, Cao, Yang, and Yang]{gao2024seerattentionlearningintrinsicsparse}
Yizhao Gao, Zhichen Zeng, Dayou Du, Shijie Cao, Hayden Kwok-Hay So, Ting Cao, Fan Yang, and Mao Yang.
\newblock Seerattention: Learning intrinsic sparse attention in your llms, 2024{\natexlab{d}}.
\newblock URL \url{https://arxiv.org/abs/2410.13276}.

\bibitem[Gavin et~al.(2024)Gavin, Zheng, Liu, Que, Wang, Yang, Zhang, Huang, Chen, and Zhang]{gavin2024longins}
Shawn Gavin, Tuney Zheng, Jiaheng Liu, Quehry Que, Noah Wang, Jian Yang, Chenchen Zhang, Wenhao Huang, Wenhu Chen, and Ge~Zhang.
\newblock Longins: A challenging long-context instruction-based exam for llms.
\newblock \emph{arXiv preprint arXiv:2406.17588}, 2024.

\bibitem[Ge et~al.(2024{\natexlab{a}})Ge, Chen, Lin, Zhu, Liu, Dai, and Zhu]{ge-2024-arxiv-v2pe}
Junqi Ge, Ziyi Chen, Jintao Lin, Jinguo Zhu, Xihui Liu, Jifeng Dai, and Xizhou Zhu.
\newblock V2pe: Improving multimodal long-context capability of vision-language models with variable visual position encoding.
\newblock \emph{CoRR}, abs/2412.09616, 2024{\natexlab{a}}.
\newblock \doi{10.48550/ARXIV.2412.09616}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2412.09616}.

\bibitem[Ge et~al.(2024{\natexlab{b}})Ge, Zhang, Liu, Zhang, Han, and Gao]{ge2024model}
Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, and Jianfeng Gao.
\newblock Model tells you what to discard: Adaptive {KV} cache compression for {LLM}s.
\newblock In \emph{The Twelfth International Conference on Learning Representations}, 2024{\natexlab{b}}.
\newblock URL \url{https://openreview.net/forum?id=uNrFpDPMyo}.

\bibitem[Ge et~al.(2024{\natexlab{c}})Ge, Jing, Wang, Wang, Chen, and Wei]{ge2024incontext}
Tao Ge, Hu~Jing, Lei Wang, Xun Wang, Si-Qing Chen, and Furu Wei.
\newblock In-context autoencoder for context compression in a large language model.
\newblock In \emph{The Twelfth International Conference on Learning Representations}, 2024{\natexlab{c}}.
\newblock URL \url{https://openreview.net/forum?id=uREj4ZuGJE}.

\bibitem[Ge et~al.(2024{\natexlab{d}})Ge, Liu, Hu, Meng, Tao, Zhao, Xia, Li, Chen, Yang, Li, Xiao, and Zhu]{ge-etal-2024-clustering}
Yuan Ge, Yilun Liu, Chi Hu, Weibin Meng, Shimin Tao, Xiaofeng Zhao, Mahong Xia, Zhang Li, Boxing Chen, Hao Yang, Bei Li, Tong Xiao, and JingBo Zhu.
\newblock Clustering and ranking: Diversity-preserved instruction selection through expert-aligned quality estimation.
\newblock In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen, editors, \emph{Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing}, pages 464--478, Miami, Florida, USA, November 2024{\natexlab{d}}. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2024.emnlp-main.28}.
\newblock URL \url{https://aclanthology.org/2024.emnlp-main.28}.

\bibitem[Gehring et~al.(2017)Gehring, Auli, Grangier, Yarats, and Dauphin]{gehring2017convolutional}
Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann~N Dauphin.
\newblock Convolutional sequence to sequence learning.
\newblock In \emph{International conference on machine learning}, pages 1243--1252. PMLR, 2017.

\bibitem[Geng et~al.(2024)Geng, Zhang, Wang, Wang, Duan, and Zheng]{gang-2024-arxiv-longvale}
Tiantian Geng, Jinrui Zhang, Qingni Wang, Teng Wang, Jinming Duan, and Feng Zheng.
\newblock Longvale: Vision-audio-language-event benchmark towards time-aware omni-modal perception of long videos.
\newblock \emph{CoRR}, abs/2411.19772, 2024.
\newblock \doi{10.48550/ARXIV.2411.19772}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2411.19772}.

\bibitem[genspark()]{genspark}
genspark.
\newblock genspark.ai.
\newblock URL \url{https://www.genspark.ai/}.

\bibitem[Gholami et~al.(2024)Gholami, Yao, Kim, Hooper, Mahoney, and Keutzer]{gholami2024ai}
Amir Gholami, Zhewei Yao, Sehoon Kim, Coleman Hooper, Michael~W Mahoney, and Kurt Keutzer.
\newblock Ai and memory wall.
\newblock \emph{IEEE Micro}, 2024.

\bibitem[GitHub(2022)]{github_copilot}
GitHub.
\newblock Github copilot, 2022.
\newblock URL \url{https://github.com/copilot}.

\bibitem[GLM et~al.(2024)GLM, Zeng, Xu, Wang, Zhang, Yin, Rojas, Feng, Zhao, Lai, Yu, Wang, Sun, Zhang, Cheng, Gui, Tang, Zhang, Li, Zhao, Wu, Zhong, Liu, Huang, Zhang, Zheng, Lu, Duan, Zhang, Cao, Yang, Tam, Zhao, Liu, Xia, Zhang, Gu, Lv, Liu, Liu, Yang, Song, Zhang, An, Xu, Niu, Yang, Li, Bai, Dong, Qi, Wang, Yang, Du, Hou, and Wang]{glm2024chatglm}
Team GLM, Aohan Zeng, Bin Xu, Bowen Wang, Chenhui Zhang, Da~Yin, Diego Rojas, Guanyu Feng, Hanlin Zhao, Hanyu Lai, Hao Yu, Hongning Wang, Jiadai Sun, Jiajie Zhang, Jiale Cheng, Jiayi Gui, Jie Tang, Jing Zhang, Juanzi Li, Lei Zhao, Lindong Wu, Lucen Zhong, Mingdao Liu, Minlie Huang, Peng Zhang, Qinkai Zheng, Rui Lu, Shuaiqi Duan, Shudan Zhang, Shulin Cao, Shuxun Yang, Weng~Lam Tam, Wenyi Zhao, Xiao Liu, Xiao Xia, Xiaohan Zhang, Xiaotao Gu, Xin Lv, Xinghan Liu, Xinyi Liu, Xinyue Yang, Xixuan Song, Xunkai Zhang, Yifan An, Yifan Xu, Yilin Niu, Yuantao Yang, Yueyan Li, Yushi Bai, Yuxiao Dong, Zehan Qi, Zhaoyu Wang, Zhen Yang, Zhengxiao Du, Zhenyu Hou, and Zihan Wang.
\newblock Chatglm: A family of large language models from glm-130b to glm-4 all tools, 2024.

\bibitem[Glorioso et~al.(2024{\natexlab{a}})Glorioso, Anthony, Tokpanov, Golubeva, Shyam, Whittington, Pilault, and Millidge]{glorioso2024zamba2}
Paolo Glorioso, Quentin Anthony, Yury Tokpanov, Anna Golubeva, Vasudev Shyam, James Whittington, Jonathan Pilault, and Beren Millidge.
\newblock The zamba2 suite: Technical report.
\newblock \emph{arXiv preprint arXiv:2411.15242}, 2024{\natexlab{a}}.

\bibitem[Glorioso et~al.(2024{\natexlab{b}})Glorioso, Anthony, Tokpanov, Whittington, Pilault, Ibrahim, and Millidge]{glorioso2024zamba}
Paolo Glorioso, Quentin Anthony, Yury Tokpanov, James Whittington, Jonathan Pilault, Adam Ibrahim, and Beren Millidge.
\newblock Zamba: A compact 7b ssm hybrid model.
\newblock \emph{arXiv preprint arXiv:2405.16712}, 2024{\natexlab{b}}.

\bibitem[Godbole et~al.(2024)Godbole, George, and Shandilya]{godbole2024leveraging}
Aditi~S. Godbole, Jabin~Geevarghese George, and Smita Shandilya.
\newblock Leveraging long-context large language models for multi-document understanding and summarization in enterprise applications.
\newblock \emph{CoRR}, abs/2409.18454, 2024.
\newblock \doi{10.48550/ARXIV.2409.18454}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2409.18454}.

\bibitem[Goldman et~al.(2024)Goldman, Jacovi, Slobodkin, Maimon, Dagan, and Tsarfaty]{goldman2024really}
Omer Goldman, Alon Jacovi, Aviv Slobodkin, Aviya Maimon, Ido Dagan, and Reut Tsarfaty.
\newblock Is it really long context if all you need is retrieval? towards genuinely difficult long context nlp.
\newblock In \emph{Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing}, pages 16576--16586, 2024.

\bibitem[Goldstein et~al.(2024)Goldstein, Obeid, Alcaide, Song, and Cheah]{goldstein2024goldfinch}
Daniel Goldstein, Fares Obeid, Eric Alcaide, Guangyu Song, and Eugene Cheah.
\newblock Goldfinch: High performance rwkv/transformer hybrid with linear pre-fill and extreme kv-cache compression.
\newblock \emph{arXiv preprint arXiv:2407.12077}, 2024.

\bibitem[Golovneva et~al.(2024)Golovneva, Wang, Weston, and Sukhbaatar]{golovneva2024contextual}
Olga Golovneva, Tianlu Wang, Jason Weston, and Sainbayar Sukhbaatar.
\newblock Contextual position encoding: Learning to count what's important.
\newblock \emph{arXiv preprint arXiv:2405.18719}, 2024.

\bibitem[Grattafiori et~al.(2024)Grattafiori, Dubey, Jauhri, Pandey, Kadian, Al-Dahle, Letman, Mathur, Schelten, Vaughan, et~al.]{grattafiori2024llama}
Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et~al.
\newblock The llama 3 herd of models.
\newblock \emph{arXiv preprint arXiv:2407.21783}, 2024.

\bibitem[Gu and Dao(2023)]{gu2023mamba}
Albert Gu and Tri Dao.
\newblock Mamba: Linear-time sequence modeling with selective state spaces.
\newblock \emph{arXiv preprint arXiv:2312.00752}, 2023.

\bibitem[Gu and Dao(2024)]{gu2024mamba}
Albert Gu and Tri Dao.
\newblock Mamba: Linear-time sequence modeling with selective state spaces.
\newblock In \emph{First Conference on Language Modeling}, 2024.
\newblock URL \url{https://openreview.net/forum?id=tEYskw1VY2}.

\bibitem[Gu et~al.(2020)Gu, Dao, Ermon, Rudra, and R{\'{e}}]{Gu2020HiPPORM}
Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher R{\'{e}}.
\newblock Hippo: Recurrent memory with optimal polynomial projections.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[Gu et~al.(2021)Gu, Johnson, Goel, Saab, Dao, Rudra, and R{\'{e}}]{gu2021combining}
Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher R{\'{e}}.
\newblock Combining recurrent, convolutional, and continuous-time models with linear state space layers.
\newblock In \emph{NeurIPS}, pages 572--585, 2021.

\bibitem[Gu et~al.(2022)Gu, Goel, and R\'e]{gu2022efficiently}
Albert Gu, Karan Goel, and Christopher R\'e.
\newblock Efficiently modeling long sequences with structured state spaces.
\newblock In \emph{The International Conference on Learning Representations ({ICLR})}, 2022.

\bibitem[Guan et~al.(2022)Guan, Feng, Chen, He, Mao, Fan, and Huang]{guan2022lot}
Jian Guan, Zhuoer Feng, Yamei Chen, Ruilin He, Xiaoxi Mao, Changjie Fan, and Minlie Huang.
\newblock Lot: A story-centric benchmark for evaluating chinese long text understanding and generation.
\newblock \emph{Transactions of the Association for Computational Linguistics}, 10:\penalty0 434--451, 2022.

\bibitem[Guan et~al.(2024)Guan, Huang, Su, Huang, Wong, and Yu]{guan2024aptq}
Ziyi Guan, Hantao Huang, Yupeng Su, Hong Huang, Ngai Wong, and Hao Yu.
\newblock Aptq: Attention-aware post-training mixed-precision quantization for large language models.
\newblock In \emph{Proceedings of the 61st ACM/IEEE Design Automation Conference}, pages 1--6, 2024.

\bibitem[Gunasekar et~al.(2023)Gunasekar, Zhang, Aneja, Mendes, Giorno, Gopi, Javaheripi, Kauffmann, de~Rosa, Saarikivi, Salim, Shah, Behl, Wang, Bubeck, Eldan, Kalai, Lee, and Li]{phi}
Suriya Gunasekar, Yi~Zhang, Jyoti Aneja, Caio C{\'{e}}sar~Teodoro Mendes, Allie~Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de~Rosa, Olli Saarikivi, Adil Salim, Shital Shah, Harkirat~Singh Behl, Xin Wang, S{\'{e}}bastien Bubeck, Ronen Eldan, Adam~Tauman Kalai, Yin~Tat Lee, and Yuanzhi Li.
\newblock Textbooks are all you need.
\newblock \emph{CoRR}, abs/2306.11644, 2023.
\newblock \doi{10.48550/ARXIV.2306.11644}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2306.11644}.

\bibitem[G{\"{u}}nther et~al.(2023)G{\"{u}}nther, Ong, Mohr, Abdessalem, Abel, Akram, Guzman, Mastrapas, Sturua, Wang, Werk, Wang, and Xiao]{günther2024jinaembeddings28192token}
Michael G{\"{u}}nther, Jackmin Ong, Isabelle Mohr, Alaeddine Abdessalem, Tanguy Abel, Mohammad~Kalim Akram, Susana Guzman, Georgios Mastrapas, Saba Sturua, Bo~Wang, Maximilian Werk, Nan Wang, and Han Xiao.
\newblock Jina embeddings 2: 8192-token general-purpose text embeddings for long documents.
\newblock \emph{CoRR}, abs/2310.19923, 2023.
\newblock \doi{10.48550/ARXIV.2310.19923}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2310.19923}.

\bibitem[Guo et~al.(2025)Guo, Yang, Zhang, Song, Zhang, Xu, Zhu, Ma, Wang, Bi, et~al.]{guo2025deepseek}
Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et~al.
\newblock Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2501.12948}, 2025.

\bibitem[Guo et~al.(2022)Guo, Ainslie, Uthus, Onta{\~{n}}{\'{o}}n, Ni, Sung, and Yang]{DBLP:conf/naacl/GuoAUONSY22}
Mandy Guo, Joshua Ainslie, David~C. Uthus, Santiago Onta{\~{n}}{\'{o}}n, Jianmo Ni, Yun{-}Hsuan Sung, and Yinfei Yang.
\newblock Longt5: Efficient text-to-text transformer for long sequences.
\newblock In Marine Carpuat, Marie{-}Catherine de~Marneffe, and Iv{\'{a}}n Vladimir~Meza Ru{\'{\i}}z, editors, \emph{Findings of the Association for Computational Linguistics: {NAACL} 2022, Seattle, WA, United States, July 10-15, 2022}, pages 724--736. Association for Computational Linguistics, 2022.
\newblock \doi{10.18653/V1/2022.FINDINGS-NAACL.55}.
\newblock URL \url{https://doi.org/10.18653/v1/2022.findings-naacl.55}.

\bibitem[Guu et~al.(2020)Guu, Lee, Tung, Pasupat, and Chang]{realm}
Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang.
\newblock Realm: retrieval-augmented language model pre-training.
\newblock In \emph{Proceedings of the 37th International Conference on Machine Learning}, ICML'20. JMLR.org, 2020.

\bibitem[Günther et~al.(2024)Günther, Mohr, Williams, Wang, and Xiao]{günther2024late}
Michael Günther, Isabelle Mohr, Daniel~James Williams, Bo~Wang, and Han Xiao.
\newblock Late chunking: Contextual chunk embeddings using long-context embedding models.
\newblock \emph{arXiv preprint arXiv: 2409.04701}, 2024.

\bibitem[Han et~al.(2024{\natexlab{a}})Han, Wang, Peng, Xiong, Chen, Ji, and Wang]{han-etal-2024-lm}
Chi Han, Qifan Wang, Hao Peng, Wenhan Xiong, Yu~Chen, Heng Ji, and Sinong Wang.
\newblock {LM}-infinite: Zero-shot extreme length generalization for large language models.
\newblock In Kevin Duh, Helena Gomez, and Steven Bethard, editors, \emph{Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)}, pages 3991--4008, Mexico City, Mexico, June 2024{\natexlab{a}}. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2024.naacl-long.222}.
\newblock URL \url{https://aclanthology.org/2024.naacl-long.222/}.

\bibitem[Han et~al.(2023)Han, Pan, Han, Song, and Huang]{han2023flattentransformervisiontransformer}
Dongchen Han, Xuran Pan, Yizeng Han, Shiji Song, and Gao Huang.
\newblock Flatten transformer: Vision transformer using focused linear attention.
\newblock In \emph{{ICCV}}, pages 5938--5948. {IEEE}, 2023.

\bibitem[Han et~al.(2024{\natexlab{b}})Han, Liu, Ding, Wang, Chen, Yan, and Huang]{han-2024-arixv-ficoco}
Yuhang Han, Xuyang Liu, Pengxiang Ding, Donglin Wang, Honggang Chen, Qingsen Yan, and Siteng Huang.
\newblock Rethinking token reduction in mllms: Towards a unified paradigm for training-free acceleration.
\newblock \emph{CoRR}, abs/2411.17686, 2024{\natexlab{b}}.
\newblock \doi{10.48550/ARXIV.2411.17686}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2411.17686}.

\bibitem[Hannan et~al.(2024)Hannan, Islam, Gu, Seidl, and Bertasius]{hannan-2024-arxiv-revisionllm}
Tanveer Hannan, Md~Mohaiminul Islam, Jindong Gu, Thomas Seidl, and Gedas Bertasius.
\newblock Revisionllm: Recursive vision-language model for temporal grounding in hour-long videos.
\newblock \emph{CoRR}, abs/2411.14901, 2024.
\newblock \doi{10.48550/ARXIV.2411.14901}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2411.14901}.

\bibitem[He et~al.(2023{\natexlab{a}})He, Jin, Xu, Qiu, Wang, Li, Yan, Wang, and Lin]{He2023WanJuanAC}
Conghui He, Zhenjiang Jin, Chaoxi Xu, Jiantao Qiu, Bin Wang, Wei Li, Hang Yan, Jiaqi Wang, and Da~Lin.
\newblock Wanjuan: A comprehensive multimodal dataset for advancing english and chinese large models.
\newblock \emph{ArXiv}, abs/2308.10755, 2023{\natexlab{a}}.
\newblock URL \url{https://api.semanticscholar.org/CorpusID:261049100}.

\bibitem[He and Zhai(2024)]{he2024fastdecode}
Jiaao He and Jidong Zhai.
\newblock Fastdecode: High-throughput gpu-efficient llm serving using heterogeneous pipelines.
\newblock \emph{arXiv preprint arXiv:2403.11421}, 2024.

\bibitem[He et~al.(2023{\natexlab{b}})He, Pan, Dong, Song, Liu, Sun, Liang, Wang, Zhang, and Zhang]{he2023never}
Junqing He, Kunhao Pan, Xiaoqun Dong, Zhuoyang Song, Yibo Liu, Qianguo Sun, Yuxin Liang, Hao Wang, Enming Zhang, and Jiaxing Zhang.
\newblock Never lost in the middle: Mastering long-context question answering with position-agnostic decompositional training.
\newblock \emph{arXiv preprint arXiv:2311.09198}, 2023{\natexlab{b}}.

\bibitem[He et~al.(2024{\natexlab{a}})He, Pan, Dong, Song, LiuYiBo, Qianguosun, Liang, Wang, Zhang, and Zhang]{pamqa}
Junqing He, Kunhao Pan, Xiaoqun Dong, Zhuoyang Song, LiuYiBo LiuYiBo, Qianguosun Qianguosun, Yuxin Liang, Hao Wang, Enming Zhang, and Jiaxing Zhang.
\newblock Never lost in the middle: Mastering long-context question answering with position-agnostic decompositional training.
\newblock In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, \emph{Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pages 13628--13642, Bangkok, Thailand, August 2024{\natexlab{a}}. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2024.acl-long.736}.
\newblock URL \url{https://aclanthology.org/2024.acl-long.736}.

\bibitem[He et~al.(2018)He, Liu, Liu, Lyu, Zhao, Xiao, Liu, Wang, Wu, She, et~al.]{he2018dureader}
Wei He, Kai Liu, Jing Liu, Yajuan Lyu, Shiqi Zhao, Xinyan Xiao, Yuan Liu, Yizhong Wang, Hua Wu, Qiaoqiao She, et~al.
\newblock Dureader: a chinese machine reading comprehension dataset from real-world applications.
\newblock In \emph{Proceedings of the Workshop on Machine Reading for Question Answering}, pages 37--46, 2018.

\bibitem[He et~al.(2025)He, Li, Liu, Wang, Bu, Zhang, Peng, Zhang, Su, and Zheng]{he2025can}
Yancheng He, Shilong Li, Jiaheng Liu, Weixun Wang, Xingyuan Bu, Ge~Zhang, Zhongyuan Peng, Zhaoxiang Zhang, Wenbo Su, and Bo~Zheng.
\newblock Can large language models detect errors in long chain-of-thought reasoning?
\newblock \emph{arXiv preprint arXiv:2502.19361}, 2025.

\bibitem[He et~al.(2024{\natexlab{b}})He, Feng, Luo, Yang, Wang, Xu, Zhang, Yang, and He]{he2024two}
Zhenyu He, Guhao Feng, Shengjie Luo, Kai Yang, Liwei Wang, Jingjing Xu, Zhi Zhang, Hongxia Yang, and Di~He.
\newblock Two stones hit one bird: Bilevel positional encoding for better length extrapolation.
\newblock \emph{arXiv preprint arXiv:2401.16421}, 2024{\natexlab{b}}.

\bibitem[Hendrycks et~al.(2020)Hendrycks, Burns, Basart, Zou, Mazeika, Song, and Steinhardt]{hendrycks2020measuring}
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt.
\newblock Measuring massive multitask language understanding.
\newblock \emph{arXiv preprint arXiv:2009.03300}, 2020.

\bibitem[Hengle et~al.(2024)Hengle, Bajpai, Dan, and Chakraborty]{hengle2024multilingualneedlehaystackinvestigating}
Amey Hengle, Prasoon Bajpai, Soham Dan, and Tanmoy Chakraborty.
\newblock Multilingual needle in a haystack: Investigating long-context behavior of multilingual large language models, 2024.
\newblock URL \url{https://arxiv.org/abs/2408.10151}.

\bibitem[Herold and Ney(2023)]{herold2023improving}
Christian Herold and Hermann Ney.
\newblock Improving long context document-level machine translation.
\newblock \emph{CoRR}, abs/2306.05183, 2023.
\newblock \doi{10.48550/ARXIV.2306.05183}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2306.05183}.

\bibitem[Hilgert et~al.(2024)Hilgert, Liu, and Niehues]{hilgert2024evaluating}
Lukas Hilgert, Danni Liu, and Jan Niehues.
\newblock Evaluating and training long-context large language models for question answering on scientific papers.
\newblock In Sachin Kumar, Vidhisha Balachandran, Chan~Young Park, Weijia Shi, Shirley~Anugrah Hayati, Yulia Tsvetkov, Noah Smith, Hannaneh Hajishirzi, Dongyeop Kang, and David Jurgens, editors, \emph{Proceedings of the 1st Workshop on Customizable NLP: Progress and Challenges in Customizing NLP for a Domain, Application, Group, or Individual (CustomNLP4U)}, pages 220--236, Miami, Florida, USA, November 2024. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2024.customnlp4u-1.17}.
\newblock URL \url{https://aclanthology.org/2024.customnlp4u-1.17/}.

\bibitem[Ho et~al.(2020)Ho, Nguyen, Sugawara, and Aizawa]{ho2020constructing}
Xanh Ho, Anh-Khoa~Duong Nguyen, Saku Sugawara, and Akiko Aizawa.
\newblock Constructing a multi-hop qa dataset for comprehensive evaluation of reasoning steps.
\newblock In \emph{Proceedings of the 28th International Conference on Computational Linguistics}, pages 6609--6625, 2020.

\bibitem[Hoffmann et~al.(2022)Hoffmann, Borgeaud, Mensch, Buchatskaya, Cai, Rutherford, de~las Casas, Hendricks, Welbl, Clark, Hennigan, Noland, Millican, van~den Driessche, Damoc, Guy, Osindero, Simonyan, Elsen, Vinyals, Rae, and Sifre]{hoffmann2022an}
Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de~las Casas, Lisa~Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katherine Millican, George van~den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Oriol Vinyals, Jack~William Rae, and Laurent Sifre.
\newblock An empirical analysis of compute-optimal large language model training.
\newblock In Alice~H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, \emph{Advances in Neural Information Processing Systems}, 2022.

\bibitem[Hong et~al.(2024)Hong, Lee, and Thorne]{orpo}
Jiwoo Hong, Noah Lee, and James Thorne.
\newblock {ORPO}: Monolithic preference optimization without reference model.
\newblock In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen, editors, \emph{Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing}, pages 11170--11189, Miami, Florida, USA, November 2024. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2024.emnlp-main.626}.
\newblock URL \url{https://aclanthology.org/2024.emnlp-main.626}.

\bibitem[Hooper et~al.(2023)Hooper, Kim, Mohammadzadeh, Genc, Keutzer, Gholami, and Shao]{hooper2023speed}
Coleman Hooper, Sehoon Kim, Hiva Mohammadzadeh, Hasan Genc, Kurt Keutzer, Amir Gholami, and Sophia Shao.
\newblock Speed: Speculative pipelined execution for efficient decoding.
\newblock \emph{arXiv preprint arXiv:2310.12072}, 2023.

\bibitem[Hooper et~al.(2025)Hooper, Kim, Mohammadzadeh, Mahoney, Shao, Keutzer, and Gholami]{hooper2025kvquant}
Coleman Hooper, Sehoon Kim, Hiva Mohammadzadeh, Michael~W Mahoney, Sophia Shao, Kurt Keutzer, and Amir Gholami.
\newblock Kvquant: Towards 10 million context length llm inference with kv cache quantization.
\newblock \emph{Advances in Neural Information Processing Systems}, 37:\penalty0 1270--1303, 2025.

\bibitem[Hosseini et~al.(2024)Hosseini, Sin, Ren, Thomas, Nouri, Farahanchi, and Hassanpour]{hosseini2024benchmark}
Pedram Hosseini, Jessica~M Sin, Bing Ren, Bryceton~G Thomas, Elnaz Nouri, Ali Farahanchi, and Saeed Hassanpour.
\newblock A benchmark for long-form medical question answering.
\newblock \emph{arXiv preprint arXiv:2411.09834}, 2024.

\bibitem[Hsieh et~al.(2024)Hsieh, Sun, Kriman, Acharya, Rekesh, Jia, and Ginsburg]{hsieh2024ruler}
Cheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya, Dima Rekesh, Fei Jia, and Boris Ginsburg.
\newblock Ruler: What’s the real context size of your long-context language models?
\newblock In \emph{First Conference on Language Modeling}, 2024.

\bibitem[Hu et~al.(2024{\natexlab{a}})Hu, Huang, Hu, Xu, Chen, Xie, Wang, Wang, Bao, Sun, et~al.]{hu2024memserve}
Cunchen Hu, Heyang Huang, Junhao Hu, Jiang Xu, Xusheng Chen, Tao Xie, Chenxi Wang, Sa~Wang, Yungang Bao, Ninghui Sun, et~al.
\newblock Memserve: Context caching for disaggregated llm serving with elastic memory pool.
\newblock \emph{arXiv preprint arXiv:2406.17565}, 2024{\natexlab{a}}.

\bibitem[Hu et~al.(2025)Hu, Huang, Wang, Li, Hu, Liu, Chen, Xie, and Shan]{hu2025efficient}
Junhao Hu, Wenrui Huang, Weidong Wang, Zhenwen Li, Tiancheng Hu, Zhixia Liu, Xusheng Chen, Tao Xie, and Yizhou Shan.
\newblock Efficient long-decoding inference with reasoning-aware attention sparsity.
\newblock \emph{arXiv preprint arXiv:2502.11147}, 2025.

\bibitem[Hu et~al.(2024{\natexlab{b}})Hu, Xiong, Yi, Wei, Xiao, Chen, Ye, Tao, Zhou, Zhao, et~al.]{hu2024agents}
Xueyu Hu, Tao Xiong, Biao Yi, Zishu Wei, Ruixuan Xiao, Yurun Chen, Jiasheng Ye, Meiling Tao, Xiangxin Zhou, Ziyu Zhao, et~al.
\newblock Os agents: A survey on mllm-based agents for general computing devices use, 2024{\natexlab{b}}.

\bibitem[Hu et~al.()Hu, Huang, Tao, Zhang, and Feng]{hu2024can}
Yutong Hu, Quzhe Huang, Mingxu Tao, Chen Zhang, and Yansong Feng.
\newblock Can perplexity reflect large language model's ability in long text understanding?
\newblock In \emph{The Second Tiny Papers Track at ICLR 2024}.

\bibitem[Hu et~al.(2024{\natexlab{c}})Hu, Liu, Zhao, Wang, Wang, Shen, Gu, Luu, Ng, Jiang, et~al.]{hu2024longrecipe}
Zhiyuan Hu, Yuliang Liu, Jinman Zhao, Suyuchen Wang, Yan Wang, Wei Shen, Qing Gu, Anh~Tuan Luu, See-Kiong Ng, Zhiwei Jiang, et~al.
\newblock Longrecipe: Recipe for efficient long context generalization in large language models.
\newblock \emph{arXiv preprint arXiv:2409.00509}, 2024{\natexlab{c}}.

\bibitem[Huang et~al.(2021)Huang, Cao, Parulian, Ji, and Wang]{huang2021efficient}
Luyang Huang, Shuyang Cao, Nikolaus Parulian, Heng Ji, and Lu~Wang.
\newblock Efficient attentions for long document summarization.
\newblock In \emph{Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies}, pages 1419--1436, 2021.

\bibitem[Huang et~al.(2023)Huang, Vora, Liang, and Leskovec]{DBLP:journals/corr/abs-2310-03302}
Qian Huang, Jian Vora, Percy Liang, and Jure Leskovec.
\newblock Benchmarking large language models as {AI} research agents.
\newblock \emph{CoRR}, abs/2310.03302, 2023.
\newblock \doi{10.48550/ARXIV.2310.03302}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2310.03302}.

\bibitem[Huang et~al.(2024{\natexlab{a}})Huang, Cheng, Liu, Hao, Song, Xu, Yang, Liu, Zhang, Chai, Yuan, Zhang, Fu, Liu, Zhang, Wang, Qi, Xu, and Chu]{Huang2024OpenCoderTO}
Siming Huang, Tianhao Cheng, Jason~Klein Liu, Jiaran Hao, Liuyihan Song, Yang Xu, J.~Yang, J.~H. Liu, Chenchen Zhang, Linzheng Chai, Ruifeng Yuan, Zhaoxiang Zhang, Jie Fu, Qian Liu, Ge~Zhang, Zili Wang, Yuan Qi, Yinghui Xu, and Wei Chu.
\newblock Opencoder: The open cookbook for top-tier code large language models.
\newblock 2024{\natexlab{a}}.
\newblock URL \url{https://arxiv.org/pdf/2411.04905}.

\bibitem[Huang et~al.(2024{\natexlab{b}})Huang, Qin, Liu, Li, Liu, Benini, Magno, and Qi]{huang2024slim}
Wei Huang, Haotong Qin, Yangdong Liu, Yawei Li, Xianglong Liu, Luca Benini, Michele Magno, and Xiaojuan Qi.
\newblock Slim-llm: Salience-driven mixed-precision quantization for large language models.
\newblock \emph{arXiv preprint arXiv:2405.14917}, 2024{\natexlab{b}}.

\bibitem[Huang et~al.(2024{\natexlab{c}})Huang, Xu, Lai, Jiang, Chen, Li, Yao, Ma, Yang, Chen, Li, and Zhao]{huang2024advancing}
Yunpeng Huang, Jingwei Xu, Junyu Lai, Zixu Jiang, Taolue Chen, Zenan Li, Yuan Yao, Xiaoxing Ma, Lijuan Yang, Hao Chen, Shupeng Li, and Penghao Zhao.
\newblock Advancing transformer architecture in long-context large language models: A comprehensive survey, 2024{\natexlab{c}}.

\bibitem[Huang et~al.()Huang, Zhang, Shan, and He]{huangcompression}
Yuzhen Huang, Jinghan Zhang, Zifei Shan, and Junxian He.
\newblock Compression represents intelligence linearly.
\newblock In \emph{First Conference on Language Modeling}.

\bibitem[Huang et~al.(2024{\natexlab{d}})Huang, Zou, Li, Liu, Zheng, Chern, Xia, Qin, Yuan, and Liu]{huang2024o1}
Zhen Huang, Haoyang Zou, Xuefeng Li, Yixiu Liu, Yuxiang Zheng, Ethan Chern, Shijie Xia, Yiwei Qin, Weizhe Yuan, and Pengfei Liu.
\newblock O1 replication journey--part 2: Surpassing o1-preview through simple distillation, big progress or bitter lesson?
\newblock \emph{arXiv preprint arXiv:2411.16489}, 2024{\natexlab{d}}.

\bibitem[Hui et~al.(2024)Hui, Yang, Cui, Yang, Liu, Zhang, Liu, Zhang, Yu, Dang, Yang, Men, Huang, Ren, Ren, Zhou, and Lin]{hui2024qwen2}
Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Kai Dang, An~Yang, Rui Men, Fei Huang, Xingzhang Ren, Xuancheng Ren, Jingren Zhou, and Junyang Lin.
\newblock Qwen2.5-coder technical report.
\newblock \emph{CoRR}, abs/2409.12186, 2024.
\newblock \doi{10.48550/ARXIV.2409.12186}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2409.12186}.

\bibitem[Hutchins et~al.(2022)Hutchins, Schlag, Wu, Dyer, and Neyshabur]{BlockRecurrentTransformers}
DeLesley Hutchins, Imanol Schlag, Yuhuai Wu, Ethan Dyer, and Behnam Neyshabur.
\newblock Block-recurrent transformers.
\newblock In \emph{NeurIPS}, 2022.

\bibitem[Inflection(2023)]{inflection2023impi}
Inflection.
\newblock I’m pi, your personal ai.
\newblock \url{https://inflection.ai/}, 2023.
\newblock URL \url{https://inflection.ai/}.

\bibitem[Inoue et~al.(2020)Inoue, Stenetorp, and Inui]{inoue2020r4c}
Naoya Inoue, Pontus Stenetorp, and Kentaro Inui.
\newblock R4c: A benchmark for evaluating rc systems to get the right answer for the right reason.
\newblock In \emph{Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics}, pages 6740--6750, 2020.

\bibitem[Izacard and Grave(2021)]{fusion-in-decoder}
Gautier Izacard and Edouard Grave.
\newblock Leveraging passage retrieval with generative models for open domain question answering.
\newblock In Paola Merlo, Jorg Tiedemann, and Reut Tsarfaty, editors, \emph{Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume}, pages 874--880, Online, apr 2021. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2021.eacl-main.74}.
\newblock URL \url{https://aclanthology.org/2021.eacl-main.74/}.

\bibitem[Izacard et~al.(2023)Izacard, Lewis, Lomeli, Hosseini, Petroni, Schick, Dwivedi{-}Yu, Joulin, Riedel, and Grave]{atlas}
Gautier Izacard, Patrick S.~H. Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi{-}Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave.
\newblock Atlas: Few-shot learning with retrieval augmented language models.
\newblock \emph{J. Mach. Learn. Res.}, 24:\penalty0 251:1--251:43, 2023.
\newblock URL \url{https://jmlr.org/papers/v24/23-0037.html}.

\bibitem[Jacobs et~al.(2023)Jacobs, Tanaka, Zhang, Zhang, Song, Rajbhandari, and He]{jacobs2023deepspeed}
Sam~Ade Jacobs, Masahiro Tanaka, Chengming Zhang, Minjia Zhang, Shuaiwen~Leon Song, Samyam Rajbhandari, and Yuxiong He.
\newblock Deepspeed ulysses: System optimizations for enabling training of extreme long sequence transformer models.
\newblock \emph{arXiv preprint arXiv:2309.14509}, 2023.

\bibitem[Jacot et~al.(2018)Jacot, Gabriel, and Hongler]{jacot2018neural}
Arthur Jacot, Franck Gabriel, and Cl{\'e}ment Hongler.
\newblock Neural tangent kernel: Convergence and generalization in neural networks.
\newblock \emph{Advances in neural information processing systems}, 31, 2018.

\bibitem[Jacovi et~al.(2025)Jacovi, Wang, Alberti, Tao, Lipovetz, Olszewska, Haas, Liu, Keating, Bloniarz, et~al.]{jacovi2025facts}
Alon Jacovi, Andrew Wang, Chris Alberti, Connie Tao, Jon Lipovetz, Kate Olszewska, Lukas Haas, Michelle Liu, Nate Keating, Adam Bloniarz, et~al.
\newblock The facts grounding leaderboard: Benchmarking llms' ability to ground responses to long-form input.
\newblock \emph{arXiv preprint arXiv:2501.03200}, 2025.

\bibitem[Jang et~al.(2024)Jang, Kang, Lee, and Kim]{jang-2024-emnlp-mate}
Young~Kyun Jang, Junmo Kang, Yong~Jae Lee, and Donghyun Kim.
\newblock {MATE:} meet at the embedding - connecting images with long texts.
\newblock In Yaser Al{-}Onaizan, Mohit Bansal, and Yun{-}Nung Chen, editors, \emph{Findings of the Association for Computational Linguistics: {EMNLP} 2024, Miami, Florida, USA, November 12-16, 2024}, pages 1659--1672. Association for Computational Linguistics, 2024.
\newblock URL \url{https://aclanthology.org/2024.findings-emnlp.90}.

\bibitem[Jeong et~al.(2024)Jeong, Hwang, Yoon, Lee, and Kang]{jeong2024olaph}
Minbyul Jeong, Hyeon Hwang, Chanwoong Yoon, Taewhoo Lee, and Jaewoo Kang.
\newblock Olaph: Improving factuality in biomedical long-form question answering.
\newblock \emph{arXiv preprint arXiv:2405.12701}, 2024.

\bibitem[Jeung et~al.(2024)Jeung, Jeon, Yousefpour, and Choi]{jeung2024large}
Wonje Jeung, Dongjae Jeon, Ashkan Yousefpour, and Jonghyun Choi.
\newblock Large language models still exhibit bias in long text.
\newblock \emph{arXiv preprint arXiv:2410.17519}, 2024.

\bibitem[Jiang et~al.(2023{\natexlab{a}})Jiang, Sablayrolles, Mensch, Bamford, Chaplot, de~las Casas, Bressand, Lengyel, Lample, Saulnier, Lavaud, Lachaux, Stock, Scao, Lavril, Wang, Lacroix, and Sayed]{jiang2023mistral7b}
Albert~Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra~Singh Chaplot, Diego de~las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio~Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven~Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William~El Sayed.
\newblock Mistral 7b, 2023{\natexlab{a}}.
\newblock URL \url{https://arxiv.org/abs/2310.06825}.

\bibitem[Jiang et~al.(2024{\natexlab{a}})Jiang, Gao, Zarch, and Annavaram]{jiang2024efficient}
Chaoyi Jiang, Lei Gao, Hossein~Entezari Zarch, and Murali Annavaram.
\newblock Efficient llm inference with i/o-aware partial kv cache recomputation.
\newblock \emph{arXiv preprint arXiv:2411.17089}, 2024{\natexlab{a}}.

\bibitem[Jiang et~al.(2023{\natexlab{b}})Jiang, Wu, Lin, Yang, and Qiu]{jiang-etal-2023-llmlingua}
Huiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing Yang, and Lili Qiu.
\newblock {LLML}ingua: Compressing prompts for accelerated inference of large language models.
\newblock In Houda Bouamor, Juan Pino, and Kalika Bali, editors, \emph{Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing}, pages 13358--13376, Singapore, December 2023{\natexlab{b}}. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2023.emnlp-main.825}.
\newblock URL \url{https://aclanthology.org/2023.emnlp-main.825/}.

\bibitem[Jiang et~al.(2024{\natexlab{b}})Jiang, LI, Zhang, Wu, Luo, Ahn, Han, Abdi, Li, Lin, Yang, and Qiu]{NEURIPS2024_5dfbe6f5}
Huiqiang Jiang, Yucheng LI, Chengruidong Zhang, Qianhui Wu, Xufang Luo, Surin Ahn, Zhenhua Han, Amir Abdi, Dongsheng Li, Chin-Yew Lin, Yuqing Yang, and Lili Qiu.
\newblock Minference 1.0: Accelerating pre-filling for long-context llms via dynamic sparse attention.
\newblock In A.~Globerson, L.~Mackey, D.~Belgrave, A.~Fan, U.~Paquet, J.~Tomczak, and C.~Zhang, editors, \emph{Advances in Neural Information Processing Systems}, volume~37, pages 52481--52515. Curran Associates, Inc., 2024{\natexlab{b}}.
\newblock URL \url{https://proceedings.neurips.cc/paper_files/paper/2024/file/5dfbe6f5671e82c76841ba687a8a9ecb-Paper-Conference.pdf}.

\bibitem[Jiang et~al.(2024{\natexlab{c}})Jiang, Li, Zhang, Wu, Luo, Ahn, Han, Abdi, Li, Lin, et~al.]{jiang2024minference}
Huiqiang Jiang, Yucheng Li, Chengruidong Zhang, Qianhui Wu, Xufang Luo, Surin Ahn, Zhenhua Han, Amir Abdi, Dongsheng Li, Chin-Yew Lin, et~al.
\newblock Minference 1.0: Accelerating pre-filling for long-context llms via dynamic sparse attention.
\newblock \emph{Advances in Neural Information Processing Systems}, 37:\penalty0 52481--52515, 2024{\natexlab{c}}.

\bibitem[Jiang et~al.(2024{\natexlab{d}})Jiang, Wu, Luo, Li, Lin, Yang, and Qiu]{jiang-etal-2024-longllmlingua}
Huiqiang Jiang, Qianhui Wu, Xufang Luo, Dongsheng Li, Chin-Yew Lin, Yuqing Yang, and Lili Qiu.
\newblock {L}ong{LLML}ingua: Accelerating and enhancing {LLM}s in long context scenarios via prompt compression.
\newblock In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, \emph{Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pages 1658--1677, Bangkok, Thailand, August 2024{\natexlab{d}}. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2024.acl-long.91}.
\newblock URL \url{https://aclanthology.org/2024.acl-long.91/}.

\bibitem[Jiang et~al.(2024{\natexlab{e}})Jiang, Wang, Shen, Kim, and Kim]{jiang2024survey}
Juyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim.
\newblock A survey on large language models for code generation.
\newblock \emph{CoRR}, abs/2406.00515, 2024{\natexlab{e}}.
\newblock \doi{10.48550/ARXIV.2406.00515}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2406.00515}.

\bibitem[Jiang et~al.(2024{\natexlab{f}})Jiang, Irvin, Wang, Chaudhry, Chen, and Ng]{jiang-2024-arxiv-manyshots}
Yixing Jiang, Jeremy Irvin, Ji~Hun Wang, Muhammad~Ahmed Chaudhry, Jonathan~H. Chen, and Andrew~Y. Ng.
\newblock Many-shot in-context learning in multimodal foundation models.
\newblock \emph{CoRR}, abs/2405.09798, 2024{\natexlab{f}}.
\newblock \doi{10.48550/ARXIV.2405.09798}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2405.09798}.

\bibitem[Jiang et~al.(2024{\natexlab{g}})Jiang, Ma, and Chen]{DBLP:journals/corr/abs-2406-15319}
Ziyan Jiang, Xueguang Ma, and Wenhu Chen.
\newblock Longrag: Enhancing retrieval-augmented generation with long-context llms.
\newblock \emph{CoRR}, abs/2406.15319, 2024{\natexlab{g}}.
\newblock \doi{10.48550/ARXIV.2406.15319}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2406.15319}.

\bibitem[Jiang et~al.(2024{\natexlab{h}})Jiang, Ma, and Chen]{jiang2024longrag}
Ziyan Jiang, Xueguang Ma, and Wenhu Chen.
\newblock Longrag: Enhancing retrieval-augmented generation with long-context llms.
\newblock \emph{arXiv preprint arXiv:2406.15319}, 2024{\natexlab{h}}.

\bibitem[Jimenez et~al.(2024)Jimenez, Yang, Wettig, Yao, Pei, Press, and Narasimhan]{jimenez2024swebench}
Carlos~E. Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik~R. Narasimhan.
\newblock Swe-bench: Can language models resolve real-world github issues?
\newblock In \emph{The Twelfth International Conference on Learning Representations, {ICLR} 2024, Vienna, Austria, May 7-11, 2024}. OpenReview.net, 2024.
\newblock URL \url{https://openreview.net/forum?id=VTF8yNQM66}.

\bibitem[Jin et~al.(2024{\natexlab{a}})Jin, Yoon, Han, and Arik]{DBLP:journals/corr/abs-2410-05983}
Bowen Jin, Jinsung Yoon, Jiawei Han, and Sercan~{\"{O}}. Arik.
\newblock Long-context llms meet {RAG:} overcoming challenges for long inputs in {RAG}.
\newblock \emph{CoRR}, abs/2410.05983, 2024{\natexlab{a}}.
\newblock \doi{10.48550/ARXIV.2410.05983}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2410.05983}.

\bibitem[Jin et~al.(2024{\natexlab{b}})Jin, Yoon, Han, and Arik]{jin2024long}
Bowen Jin, Jinsung Yoon, Jiawei Han, and Sercan~O Arik.
\newblock Long-context llms meet rag: Overcoming challenges for long inputs in rag.
\newblock \emph{arXiv preprint arXiv:2410.05983}, 2024{\natexlab{b}}.

\bibitem[Jin et~al.(2024{\natexlab{c}})Jin, Zhang, Meng, Wang, and Tan]{jin2024comprehensive}
Hanlei Jin, Yang Zhang, Dan Meng, Jun Wang, and Jinghua Tan.
\newblock A comprehensive survey on process-oriented automatic text summarization with exploration of llm-based methods.
\newblock \emph{CoRR}, abs/2403.02901, 2024{\natexlab{c}}.
\newblock \doi{10.48550/ARXIV.2403.02901}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2403.02901}.

\bibitem[Jin et~al.(2023)Jin, Han, Yang, Jiang, yuan Chang, and Hu]{growlength}
Hongye Jin, Xiaotian Han, Jingfeng Yang, Zhimeng Jiang, Chia yuan Chang, and Xia Hu.
\newblock Growlength: Accelerating llms pretraining by progressively growing training length.
\newblock \emph{ArXiv}, abs/2310.00576, 2023.

\bibitem[Jin et~al.(2024{\natexlab{d}})Jin, Han, Yang, Jiang, Liu, Chang, Chen, and Hu]{jin2024llm}
Hongye Jin, Xiaotian Han, Jingfeng Yang, Zhimeng Jiang, Zirui Liu, Chia-Yuan Chang, Huiyuan Chen, and Xia Hu.
\newblock Llm maybe longlm: Self-extend llm context window without tuning.
\newblock \emph{arXiv preprint arXiv:2401.01325}, 2024{\natexlab{d}}.

\bibitem[Jo et~al.(2024)Jo, Jeong, Park, Epstein, and Kim]{jo2024understanding}
Eunkyung Jo, Yuin Jeong, SoHyun Park, Daniel~A. Epstein, and Young{-}Ho Kim.
\newblock Understanding the impact of long-term memory on self-disclosure with large language model-driven chatbots for public health intervention.
\newblock In Florian~'Floyd' Mueller, Penny Kyburz, Julie~R. Williamson, Corina Sas, Max~L. Wilson, Phoebe O.~Toups Dugas, and Irina Shklovski, editors, \emph{Proceedings of the {CHI} Conference on Human Factors in Computing Systems, {CHI} 2024, Honolulu, HI, USA, May 11-16, 2024}, pages 440:1--440:21. {ACM}, 2024.
\newblock \doi{10.1145/3613904.3642420}.
\newblock URL \url{https://doi.org/10.1145/3613904.3642420}.

\bibitem[Joshi et~al.(2024)Joshi, Sarwar, Varshney, Nag, Agrawal, and Naik]{joshi2024reaper}
Ashutosh Joshi, Sheikh~Muhammad Sarwar, Samarth Varshney, Sreyashi Nag, Shrivats Agrawal, and Juhi Naik.
\newblock Reaper: Reasoning based retrieval planning for complex rag systems.
\newblock \emph{arXiv preprint arXiv: 2407.18553}, 2024.

\bibitem[Joshi et~al.(2017)Joshi, Choi, Weld, and Zettlemoyer]{joshi2017triviaqa}
Mandar Joshi, Eunsol Choi, Daniel~S Weld, and Luke Zettlemoyer.
\newblock Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension.
\newblock In \emph{Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pages 1601--1611, 2017.

\bibitem[Jung et~al.(2024)Jung, Liu, Huang, Zhou, and Chen]{jung2024familiarityawareevidencecompressionretrievalaugmented}
Dongwon Jung, Qin Liu, Tenghao Huang, Ben Zhou, and Muhao Chen.
\newblock Familiarity-aware evidence compression for retrieval-augmented generation, 2024.
\newblock URL \url{https://arxiv.org/abs/2409.12468}.

\bibitem[Jung and Kim(2024)]{Jung_2024}
Hoyoun Jung and Kyung-Joong Kim.
\newblock Discrete prompt compression with reinforcement learning.
\newblock \emph{IEEE Access}, 12:\penalty0 72578–72587, 2024.
\newblock ISSN 2169-3536.
\newblock \doi{10.1109/access.2024.3403426}.
\newblock URL \url{http://dx.doi.org/10.1109/ACCESS.2024.3403426}.

\bibitem[Kalman(1960)]{kalman1960new}
Rudolph~Emil Kalman.
\newblock A new approach to linear filtering and prediction problems.
\newblock 1960.

\bibitem[Kamradt(2023)]{needleinhaystack}
Greg Kamradt.
\newblock Needle in a haystack - pressure testing llms.
\newblock \url{https://github.com/gkamradt/LLMTest_NeedleInAHaystack}, 2023.

\bibitem[Kang et~al.(2024)Kang, Zhang, Kundu, Jeong, Liu, Krishna, and Zhao]{kang2024gearefficientkvcache}
Hao Kang, Qingru Zhang, Souvik Kundu, Geonhwa Jeong, Zaoxing Liu, Tushar Krishna, and Tuo Zhao.
\newblock Gear: An efficient kv cache compression recipe for near-lossless generative inference of llm, 2024.
\newblock URL \url{https://arxiv.org/abs/2403.05527}.

\bibitem[Kapoor et~al.(2024)Kapoor, Henderson, and Narayanan]{kapoor2024promises}
Sayash Kapoor, Peter Henderson, and Arvind Narayanan.
\newblock Promises and pitfalls of artificial intelligence for legal applications.
\newblock \emph{CoRR}, abs/2402.01656, 2024.
\newblock \doi{10.48550/ARXIV.2402.01656}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2402.01656}.

\bibitem[Karpinska et~al.(2024)Karpinska, Thai, Lo, Goyal, and Iyyer]{nocha-2024-karp-thai-et-al}
Marzena Karpinska, Katherine Thai, Kyle Lo, Tanya Goyal, and Mohit Iyyer.
\newblock One thousand and one pairs: A "novel" challenge for long-context language models, 2024.

\bibitem[Katharopoulos et~al.(2020)Katharopoulos, Vyas, Pappas, and Fleuret]{katharopoulos2020transformersrnnsfastautoregressive}
Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran{\c{c}}ois Fleuret.
\newblock Transformers are rnns: Fast autoregressive transformers with linear attention.
\newblock In \emph{{ICML}}, volume 119 of \emph{Proceedings of Machine Learning Research}, pages 5156--5165. {PMLR}, 2020.

\bibitem[Kazemnejad et~al.(2024)Kazemnejad, Padhi, Natesan~Ramamurthy, Das, and Reddy]{kazemnejad2024impact}
Amirhossein Kazemnejad, Inkit Padhi, Karthikeyan Natesan~Ramamurthy, Payel Das, and Siva Reddy.
\newblock The impact of positional encoding on length generalization in transformers.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[Khandelwal et~al.(2019)Khandelwal, Levy, Jurafsky, Zettlemoyer, and Lewis]{knn-lm}
Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and M.~Lewis.
\newblock Generalization through memorization: Nearest neighbor language models.
\newblock \emph{International Conference on Learning Representations}, 2019.

\bibitem[Kim et~al.(2024)Kim, Ryu, and Lee]{kim2024tccl}
Heehoon Kim, Junyeol Ryu, and Jaejin Lee.
\newblock Tccl: Discovering better communication paths for pcie gpu clusters.
\newblock In \emph{Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3}, pages 999--1015, 2024.

\bibitem[Kim and Bang(2018)]{Bang18}
Youngjoo Kim and Hyochoong Bang.
\newblock Introduction to kalman filter and its applications.
\newblock In Felix Govaers, editor, \emph{Introduction and Implementations of the Kalman Filter}, chapter~2. IntechOpen, Rijeka, 2018.

\bibitem[Ko{\v{c}}isk{\`y} et~al.(2018)Ko{\v{c}}isk{\`y}, Schwarz, Blunsom, Dyer, Hermann, Melis, and Grefenstette]{kovcisky2018narrativeqa}
Tom{\'a}{\v{s}} Ko{\v{c}}isk{\`y}, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl~Moritz Hermann, G{\'a}bor Melis, and Edward Grefenstette.
\newblock The narrativeqa reading comprehension challenge.
\newblock \emph{Transactions of the Association for Computational Linguistics}, 6:\penalty0 317--328, 2018.

\bibitem[Ko{\v{c}}isk{\'y} et~al.(2018)Ko{\v{c}}isk{\'y}, Schwarz, Blunsom, Dyer, Hermann, Melis, and Grefenstette]{narrativeqa}
Tom{\'a}{\v{s}} Ko{\v{c}}isk{\'y}, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl~Moritz Hermann, G{\'a}bor Melis, and Edward Grefenstette.
\newblock The {N}arrative{QA} reading comprehension challenge.
\newblock \emph{Transactions of the Association for Computational Linguistics}, 6:\penalty0 317--328, 2018.
\newblock \doi{10.1162/tacl_a_00023}.
\newblock URL \url{https://aclanthology.org/Q18-1023/}.

\bibitem[K{\"o}ksal et~al.(2023)K{\"o}ksal, Schick, Korhonen, and Sch{\"u}tze]{koksal2023longform}
Abdullatif K{\"o}ksal, Timo Schick, Anna Korhonen, and Hinrich Sch{\"u}tze.
\newblock Longform: Effective instruction tuning with reverse instructions.
\newblock \emph{arXiv preprint arXiv:2304.08460}, 2023.

\bibitem[Korthikanti et~al.(2023)Korthikanti, Casper, Lym, McAfee, Andersch, Shoeybi, and Catanzaro]{korthikanti2023reducing}
Vijay~Anand Korthikanti, Jared Casper, Sangkug Lym, Lawrence McAfee, Michael Andersch, Mohammad Shoeybi, and Bryan Catanzaro.
\newblock Reducing activation recomputation in large transformer models.
\newblock \emph{Proceedings of Machine Learning and Systems}, 5:\penalty0 341--353, 2023.

\bibitem[Krell et~al.(2021)Krell, Kosec, Perez, and Fitzgibbon]{krell2021efficient}
Mario~Michael Krell, Matej Kosec, Sergio~P Perez, and Andrew Fitzgibbon.
\newblock Efficient sequence packing without cross-contamination: Accelerating large language models without impacting performance.
\newblock \emph{arXiv preprint arXiv:2107.02027}, 2021.

\bibitem[Krishna et~al.(2021)Krishna, Roy, and Iyyer]{krishna2021hurdles}
Kalpesh Krishna, Aurko Roy, and Mohit Iyyer.
\newblock Hurdles to progress in long-form question answering.
\newblock \emph{arXiv preprint arXiv:2103.06332}, 2021.

\bibitem[Kulkarni et~al.(2020)Kulkarni, Chammas, Zhu, Sha, and Ie]{kulkarni2020aquamuse}
Sayali Kulkarni, Sheide Chammas, Wan Zhu, Fei Sha, and Eugene Ie.
\newblock Aquamuse: Automatically generating datasets for query-based multi-document summarization.
\newblock \emph{arXiv preprint arXiv:2010.12694}, 2020.

\bibitem[Kumar et~al.(2024)Kumar, Viswanathan, Yerra, Salemi, Rossi, Dernoncourt, Deilamsalehy, Chen, Zhang, Agarwal, et~al.]{kumar2024longlamp}
Ishita Kumar, Snigdha Viswanathan, Sushrita Yerra, Alireza Salemi, Ryan~A Rossi, Franck Dernoncourt, Hanieh Deilamsalehy, Xiang Chen, Ruiyi Zhang, Shubham Agarwal, et~al.
\newblock Longlamp: A benchmark for personalized long-form text generation.
\newblock \emph{arXiv preprint arXiv:2407.11016}, 2024.

\bibitem[Kundu et~al.(2024)Kundu, Lee, Wynter, Ganti, and Mishra]{kundu2024enhancing}
Achintya Kundu, Rhui~Dih Lee, Laura Wynter, Raghu~Kiran Ganti, and Mayank Mishra.
\newblock Enhancing training efficiency using packing with flash attention.
\newblock \emph{arXiv preprint arXiv:2407.09105}, 2024.

\bibitem[Kuratov et~al.(2024)Kuratov, Bulatov, Anokhin, Rodkin, Sorokin, Sorokin, and Burtsev]{kuratov2024babilong}
Yuri Kuratov, Aydar Bulatov, Petr Anokhin, Ivan Rodkin, Dmitry Sorokin, Artyom Sorokin, and Mikhail Burtsev.
\newblock Babilong: Testing the limits of llms with long context reasoning-in-a-haystack.
\newblock \emph{arXiv preprint arXiv:2406.10149}, 2024.

\bibitem[Kwan et~al.(2023)Kwan, Zeng, Wang, Sun, Li, Shang, Liu, and Wong]{kwan2023m4le}
Wai-Chung Kwan, Xingshan Zeng, Yufei Wang, Yusen Sun, Liangyou Li, Lifeng Shang, Qun Liu, and Kam-Fai Wong.
\newblock M4le: A multi-ability multi-range multi-task multi-domain long-context evaluation benchmark for large language models.
\newblock \emph{arXiv preprint arXiv:2310.19240}, 2023.

\bibitem[Kwiatkowski et~al.(2019)Kwiatkowski, Palomaki, Redfield, Collins, Parikh, Alberti, Epstein, Polosukhin, Devlin, Lee, et~al.]{kwiatkowski2019natural}
Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et~al.
\newblock Natural questions: A benchmark for question answering research.
\newblock \emph{Transactions of the Association for Computational Linguistics}, 7:\penalty0 452--466, 2019.

\bibitem[Kwon et~al.(2023)Kwon, Li, Zhuang, Sheng, Zheng, Yu, Gonzalez, Zhang, and Stoica]{kwon2023efficient}
Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody~Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica.
\newblock Efficient memory management for large language model serving with pagedattention.
\newblock In \emph{Proceedings of the 29th Symposium on Operating Systems Principles}, pages 611--626, 2023.

\bibitem[Laban et~al.(2024)Laban, Fabbri, Xiong, and Wu]{laban2024SummHay}
Philippe Laban, Alexander~R Fabbri, Caiming Xiong, and Chien-Sheng Wu.
\newblock Summary of a haystack: A challenge to long-context llms and rag systems.
\newblock \emph{arXiv preprint arXiv:https://arxiv.org/pdf/2407.01370}, 2024.

\bibitem[Labs()]{perplexity_pages}
Perplexity Labs.
\newblock perplexity.
\newblock URL \url{https://www.perplexity.ai}.
\newblock Accessed: 2024-05-30.

\bibitem[Lanchantin et~al.(2023)Lanchantin, Toshniwal, Weston, szlam, and Sukhbaatar]{self-notes}
Jack Lanchantin, Shubham Toshniwal, Jason Weston, arthur szlam, and Sainbayar Sukhbaatar.
\newblock Learning to reason and memorize with self-notes.
\newblock In A.~Oh, T.~Naumann, A.~Globerson, K.~Saenko, M.~Hardt, and S.~Levine, editors, \emph{Advances in Neural Information Processing Systems}, volume~36, pages 11891--11911. Curran Associates, Inc., 2023.
\newblock URL \url{https://proceedings.neurips.cc/paper_files/paper/2023/file/274d0146144643ee2459a602123c60ff-Paper-Conference.pdf}.

\bibitem[Lee et~al.(2023{\natexlab{a}})Lee, Hartmann, Park, Papailiopoulos, and Lee]{lee-etal-2023-prompted}
Gibbeum Lee, Volker Hartmann, Jongho Park, Dimitris Papailiopoulos, and Kangwook Lee.
\newblock Prompted llms as chatbot modules for long open-domain conversation.
\newblock In Anna Rogers, Jordan~L. Boyd{-}Graber, and Naoaki Okazaki, editors, \emph{Findings of the Association for Computational Linguistics: {ACL} 2023, Toronto, Canada, July 9-14, 2023}, pages 4536--4554. Association for Computational Linguistics, 2023{\natexlab{a}}.
\newblock \doi{10.18653/V1/2023.FINDINGS-ACL.277}.
\newblock URL \url{https://doi.org/10.18653/v1/2023.findings-acl.277}.

\bibitem[Lee et~al.(2024{\natexlab{a}})Lee, Chen, Dai, Dua, Sachan, Boratko, Luan, Arnold, Perot, Dalmia, et~al.]{lee2024can}
Jinhyuk Lee, Anthony Chen, Zhuyun Dai, Dheeru Dua, Devendra~Singh Sachan, Michael Boratko, Yi~Luan, S{\'e}bastien~MR Arnold, Vincent Perot, Siddharth Dalmia, et~al.
\newblock Can long-context language models subsume retrieval, rag, sql, and more?
\newblock \emph{arXiv preprint arXiv:2406.13121}, 2024{\natexlab{a}}.

\bibitem[Lee et~al.(2024{\natexlab{b}})Lee, Chen, Furuta, Canny, and Fischer]{readagent}
Kuang{-}Huei Lee, Xinyun Chen, Hiroki Furuta, John~F. Canny, and Ian Fischer.
\newblock A human-inspired reading agent with gist memory of very long contexts.
\newblock In \emph{Forty-first International Conference on Machine Learning, {ICML} 2024, Vienna, Austria, July 21-27, 2024}. OpenReview.net, 2024{\natexlab{b}}.
\newblock URL \url{https://openreview.net/forum?id=OTmcsyEO5G}.

\bibitem[Lee et~al.(2024{\natexlab{c}})Lee, Yoon, Jang, Lee, Song, Kim, and Kang]{lee2024ethic}
Taewhoo Lee, Chanwoong Yoon, Kyochul Jang, Donghyeon Lee, Minju Song, Hyunjae Kim, and Jaewoo Kang.
\newblock Ethic: Evaluating large language models on long-context tasks with high information coverage.
\newblock \emph{arXiv preprint arXiv:2410.16848}, 2024{\natexlab{c}}.

\bibitem[Lee et~al.(2023{\natexlab{b}})Lee, Lee, Park, Hwang, Kim, Lee, and Lee]{lee2023qasa}
Yoonjoo Lee, Kyungjae Lee, Sunghyun Park, Dasol Hwang, Jaehyeon Kim, Hong-in Lee, and Moontae Lee.
\newblock Qasa: advanced question answering on scientific articles.
\newblock In \emph{International Conference on Machine Learning}, pages 19036--19052. PMLR, 2023{\natexlab{b}}.

\bibitem[Lei et~al.(2024)Lei, Liu, Huang, He, Zhao, and Liu]{lei2024s3eval}
Fangyu Lei, Qian Liu, Yiming Huang, Shizhu He, Jun Zhao, and Kang Liu.
\newblock S3eval: A synthetic, scalable, systematic evaluation suite for large language model.
\newblock In \emph{Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)}, pages 1259--1286, 2024.

\bibitem[Leskovec et~al.(2020)Leskovec, Rajaraman, and Ullman]{leskovec2020mining}
Jure Leskovec, Anand Rajaraman, and Jeffrey~David Ullman.
\newblock \emph{Mining of massive data sets}.
\newblock Cambridge university press, 2020.

\bibitem[Lester et~al.(2021)Lester, Al-Rfou, and Constant]{lester-etal-2021-power}
Brian Lester, Rami Al-Rfou, and Noah Constant.
\newblock The power of scale for parameter-efficient prompt tuning.
\newblock In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih, editors, \emph{Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing}, pages 3045--3059, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2021.emnlp-main.243}.
\newblock URL \url{https://aclanthology.org/2021.emnlp-main.243}.

\bibitem[Leviathan et~al.(2023)Leviathan, Kalman, and Matias]{leviathan2023fast}
Yaniv Leviathan, Matan Kalman, and Yossi Matias.
\newblock Fast inference from transformers via speculative decoding.
\newblock In \emph{International Conference on Machine Learning}, pages 19274--19286. PMLR, 2023.

\bibitem[Leviathan et~al.(2024)Leviathan, Kalman, and Matias]{leviathan2024selectiveattentionimprovestransformer}
Yaniv Leviathan, Matan Kalman, and Yossi Matias.
\newblock Selective attention improves transformer, 2024.
\newblock URL \url{https://arxiv.org/abs/2410.02703}.

\bibitem[Levine et~al.(2022)Levine, Wies, Jannai, Navon, Hoshen, and Shashua]{levine2022the}
Yoav Levine, Noam Wies, Daniel Jannai, Dan Navon, Yedid Hoshen, and Amnon Shashua.
\newblock The inductive bias of in-context learning: Rethinking pretraining example design.
\newblock In \emph{International Conference on Learning Representations}, 2022.

\bibitem[Lewis et~al.(2020)Lewis, Perez, Piktus, Petroni, Karpukhin, Goyal, Küttler, Lewis, tau Yih, Rocktäschel, Riedel, and Kiela]{lewis2020retrievalaugmented}
Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela.
\newblock Retrieval-augmented generation for knowledge-intensive nlp tasks.
\newblock \emph{arXiv preprint arXiv: 2005.11401}, 2020.

\bibitem[Li et~al.(2024{\natexlab{a}})Li, Zhang, Guo, Zhang, Li, Zhang, Zhang, Li, Liu, and Li]{li-2024-arxiv-llavaonevision}
Bo~Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li.
\newblock Llava-onevision: Easy visual task transfer.
\newblock \emph{CoRR}, abs/2408.03326, 2024{\natexlab{a}}.
\newblock \doi{10.48550/ARXIV.2408.03326}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2408.03326}.

\bibitem[Li et~al.(2023{\natexlab{a}})Li, Shao, Xie, Sheng, Zheng, Gonzalez, Stoica, Ma, and Zhang]{li2023long}
Dacheng Li, Rulin Shao, Anze Xie, Ying Sheng, Lianmin Zheng, Joseph Gonzalez, Ion Stoica, Xuezhe Ma, and Hao Zhang.
\newblock How long can context length of open-source llms truly promise?
\newblock In \emph{NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following}, 2023{\natexlab{a}}.

\bibitem[Li et~al.(2025{\natexlab{a}})Li, Cao, Griggs, Liu, Mo, Patil, Zaharia, Gonzalez, and Stoica]{li2025llms}
Dacheng Li, Shiyi Cao, Tyler Griggs, Shu Liu, Xiangxi Mo, Shishir~G Patil, Matei Zaharia, Joseph~E Gonzalez, and Ion Stoica.
\newblock Llms can easily learn to reason from demonstrations structure, not content, is what matters!
\newblock \emph{arXiv preprint arXiv:2502.07374}, 2025{\natexlab{a}}.

\bibitem[Li et~al.(2024{\natexlab{b}})Li, Fu, Lin, Ge, Wang, Niu, Jiang, and Cui]{li2024demystifying}
Haoyang Li, Fangcheng Fu, Sheng Lin, Hao Ge, Xuanyu Wang, Jiawen Niu, Jie Jiang, and Bin Cui.
\newblock Demystifying workload imbalances in large transformer model training over variable-length sequences.
\newblock \emph{arXiv preprint arXiv:2412.07894}, 2024{\natexlab{b}}.

\bibitem[Li et~al.(2023{\natexlab{b}})Li, Wang, Zheng, and Zhang]{li2023loogle}
Jiaqi Li, Mengmeng Wang, Zilong Zheng, and Muhan Zhang.
\newblock Loogle: Can long-context language models understand long contexts?
\newblock \emph{arXiv preprint arXiv:2311.04939}, 2023{\natexlab{b}}.

\bibitem[Li(2025)]{flashmla2025}
Jiashi Li.
\newblock Flashmla: Efficient mla decoding kernels.
\newblock \url{https://github.com/deepseek-ai/FlashMLA}, 2025.

\bibitem[Li et~al.(2015)Li, Galley, Brockett, Gao, and Dolan]{li2015diversity}
Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, and Bill Dolan.
\newblock A diversity-promoting objective function for neural conversation models.
\newblock \emph{arXiv preprint arXiv:1510.03055}, 2015.

\bibitem[Li et~al.(2023{\natexlab{c}})Li, Li, Savarese, and Hoi]{li-2023-icml-blip2}
Junnan Li, Dongxu Li, Silvio Savarese, and Steven C.~H. Hoi.
\newblock {BLIP-2:} bootstrapping language-image pre-training with frozen image encoders and large language models.
\newblock In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, \emph{International Conference on Machine Learning, {ICML} 2023, 23-29 July 2023, Honolulu, Hawaii, {USA}}, volume 202 of \emph{Proceedings of Machine Learning Research}, pages 19730--19742. {PMLR}, 2023{\natexlab{c}}.
\newblock URL \url{https://proceedings.mlr.press/v202/li23q.html}.

\bibitem[Li et~al.(2023{\natexlab{d}})Li, Zhang, Li, Chen, Chen, Cheng, Wang, Zhou, and Xiao]{Li2023FromQT}
Ming Li, Yong Zhang, Zhitao Li, Jiuhai Chen, Lichang Chen, Ning Cheng, Jianzong Wang, Tianyi Zhou, and Jing Xiao.
\newblock From quantity to quality: Boosting llm performance with self-guided data selection for instruction tuning.
\newblock \emph{ArXiv}, abs/2308.12032, 2023{\natexlab{d}}.

\bibitem[Li et~al.(2024{\natexlab{c}})Li, Zhang, Liu, and Chen]{li2024needlebench}
Mo~Li, Songyang Zhang, Yunxin Liu, and Kai Chen.
\newblock Needlebench: Can llms do retrieval and reasoning in 1 million context window?
\newblock \emph{arXiv preprint arXiv:2407.11963}, 2024{\natexlab{c}}.

\bibitem[Li et~al.(2023{\natexlab{e}})Li, Zhang, Li, Yao, Zhang, Chu, Sun, Du, and Xie]{li2023fptq}
Qingyuan Li, Yifan Zhang, Liang Li, Peng Yao, Bo~Zhang, Xiangxiang Chu, Yerui Sun, Li~Du, and Yuchen Xie.
\newblock Fptq: Fine-grained post-training quantization for large language models.
\newblock \emph{arXiv preprint arXiv:2308.15987}, 2023{\natexlab{e}}.

\bibitem[Li et~al.(2025{\natexlab{b}})Li, Wang, Zhang, Wang, and Yeung-Levy]{li-2025-arxiv-TPO}
Rui Li, Xiaohan Wang, Yuhui Zhang, Zeyu Wang, and Serena Yeung-Levy.
\newblock Temporal preference optimization for long-form video understanding.
\newblock \emph{CoRR}, abs/2501.13919, 2025{\natexlab{b}}.
\newblock \doi{10.48550/ARXIV.2501.13919}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2501.13919}.

\bibitem[Li et~al.(2023{\natexlab{f}})Li, You, Guruganesh, Ainslie, Ontanon, Zaheer, Sanghai, Yang, Kumar, and Bhojanapalli]{li2023functional}
Shanda Li, Chong You, Guru Guruganesh, Joshua Ainslie, Santiago Ontanon, Manzil Zaheer, Sumit Sanghai, Yiming Yang, Sanjiv Kumar, and Srinadh Bhojanapalli.
\newblock Functional interpolation for relative positions improves long context transformers.
\newblock \emph{arXiv preprint arXiv:2310.04418}, 2023{\natexlab{f}}.

\bibitem[Li et~al.(2024{\natexlab{d}})Li, He, Guo, Bu, Bai, Liu, Liu, Qu, Li, Ouyang, Su, and Zheng]{graphreader}
Shilong Li, Yancheng He, Hangyu Guo, Xingyuan Bu, Ge~Bai, Jie Liu, Jiaheng Liu, Xingwei Qu, Yangguang Li, Wanli Ouyang, Wenbo Su, and Bo~Zheng.
\newblock Graphreader: Building graph-based agent to enhance long-context abilities of large language models.
\newblock \emph{Conference on Empirical Methods in Natural Language Processing}, 2024{\natexlab{d}}.
\newblock \doi{10.48550/arXiv.2406.14550}.

\bibitem[Li et~al.(2024{\natexlab{e}})Li, He, Huang, Bu, Liu, Guo, Wang, Gu, Su, and Zheng]{Li20242DDPOSD}
Shilong Li, Yancheng He, Hui Huang, Xingyuan Bu, Jiaheng Liu, Hangyu Guo, Weixun Wang, Jihao Gu, Wenbo Su, and Bo~Zheng.
\newblock 2d-dpo: Scaling direct preference optimization with 2-dimensional supervision.
\newblock \emph{ArXiv}, abs/2410.19720, 2024{\natexlab{e}}.

\bibitem[Li et~al.(2024{\natexlab{f}})Li, Zhang, Do, Yue, and Chen]{li2024long}
Tianle Li, Ge~Zhang, Quy~Duc Do, Xiang Yue, and Wenhu Chen.
\newblock Long-context llms struggle with long in-context learning.
\newblock \emph{arXiv preprint arXiv:2404.02060}, 2024{\natexlab{f}}.

\bibitem[Li et~al.(2025{\natexlab{c}})Li, Wang, Yu, Zeng, Zhu, Huang, Gao, Li, He, Wang, Qiao, Wang, and Wang]{DBLP:journals/corr/abs-2501-00574}
Xinhao Li, Yi~Wang, Jiashuo Yu, Xiangyu Zeng, Yuhan Zhu, Haian Huang, Jianfei Gao, Kunchang Li, Yinan He, Chenting Wang, Yu~Qiao, Yali Wang, and Limin Wang.
\newblock Videochat-flash: Hierarchical compression for long-context video modeling.
\newblock \emph{CoRR}, abs/2501.00574, 2025{\natexlab{c}}.
\newblock \doi{10.48550/ARXIV.2501.00574}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2501.00574}.

\bibitem[Li et~al.(2024{\natexlab{g}})Li, Liang, Lyu, and Wang]{DBLP:conf/acl/LiLL024}
Yanyang Li, Shuo Liang, Michael~R. Lyu, and Liwei Wang.
\newblock Making long-context language models better multi-hop reasoners.
\newblock In Lun{-}Wei Ku, Andre Martins, and Vivek Srikumar, editors, \emph{Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), {ACL} 2024, Bangkok, Thailand, August 11-16, 2024}, pages 2462--2475. Association for Computational Linguistics, 2024{\natexlab{g}}.
\newblock \doi{10.18653/V1/2024.ACL-LONG.135}.
\newblock URL \url{https://doi.org/10.18653/v1/2024.acl-long.135}.

\bibitem[Li et~al.(2023{\natexlab{g}})Li, Dong, Guerin, and Lin]{li-etal-2023-compressing}
Yucheng Li, Bo~Dong, Frank Guerin, and Chenghua Lin.
\newblock Compressing context to enhance inference efficiency of large language models.
\newblock In Houda Bouamor, Juan Pino, and Kalika Bali, editors, \emph{Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing}, pages 6342--6353, Singapore, December 2023{\natexlab{g}}. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2023.emnlp-main.391}.
\newblock URL \url{https://aclanthology.org/2023.emnlp-main.391}.

\bibitem[Li et~al.(2024{\natexlab{h}})Li, Jiang, Wu, Luo, Ahn, Zhang, Abdi, Li, Gao, Yang, et~al.]{li2024scbench}
Yucheng Li, Huiqiang Jiang, Qianhui Wu, Xufang Luo, Surin Ahn, Chengruidong Zhang, Amir~H Abdi, Dongsheng Li, Jianfeng Gao, Yuqing Yang, et~al.
\newblock Scbench: A kv cache-centric analysis of long-context methods.
\newblock \emph{arXiv preprint arXiv:2412.10319}, 2024{\natexlab{h}}.

\bibitem[Li et~al.(2025{\natexlab{d}})Li, Yue, Xu, Jiang, Niu, Lin, Ramasubramanian, and Poovendran]{li2025small}
Yuetai Li, Xiang Yue, Zhangchen Xu, Fengqing Jiang, Luyao Niu, Bill~Yuchen Lin, Bhaskar Ramasubramanian, and Radha Poovendran.
\newblock Small models struggle to learn from strong reasoners.
\newblock \emph{arXiv preprint arXiv:2502.12143}, 2025{\natexlab{d}}.

\bibitem[Li et~al.(2024{\natexlab{i}})Li, Huang, Yang, Venkitesh, Locatelli, Ye, Cai, Lewis, and Chen]{li2024snapkvllmknowslooking}
Yuhong Li, Yingbing Huang, Bowen Yang, Bharat Venkitesh, Acyr Locatelli, Hanchen Ye, Tianle Cai, Patrick Lewis, and Deming Chen.
\newblock Snapkv: {LLM} knows what you are looking for before generation.
\newblock In \emph{NeurIPS}, 2024{\natexlab{i}}.

\bibitem[Li et~al.(2024{\natexlab{j}})Li, Wei, Zhang, and Zhang]{li2024eagle}
Yuhui Li, Fangyun Wei, Chao Zhang, and Hongyang Zhang.
\newblock Eagle: speculative sampling requires rethinking feature uncertainty.
\newblock In \emph{Proceedings of the 41st International Conference on Machine Learning}, pages 28935--28948, 2024{\natexlab{j}}.

\bibitem[Li et~al.(2024{\natexlab{k}})Li, Li, Zhang, Mei, and Bendersky]{li2024retrieval}
Zhuowan Li, Cheng Li, Mingyang Zhang, Qiaozhu Mei, and Michael Bendersky.
\newblock Retrieval augmented generation or long-context llms? a comprehensive study and hybrid approach.
\newblock \emph{arXiv preprint arXiv:2407.16833}, 2024{\natexlab{k}}.

\bibitem[Li et~al.(2024{\natexlab{l}})Li, Liu, Su, and Collier]{Li2024PromptCF}
Zongqian Li, Yinhong Liu, Yixuan Su, and Nigel Collier.
\newblock Prompt compression for large language models: A survey.
\newblock \emph{ArXiv}, abs/2410.12388, 2024{\natexlab{l}}.
\newblock URL \url{https://api.semanticscholar.org/CorpusID:273375474}.

\bibitem[Li et~al.(2024{\natexlab{m}})Li, Liu, Su, and Collier]{li2024promptcompressionlargelanguage}
Zongqian Li, Yinhong Liu, Yixuan Su, and Nigel Collier.
\newblock Prompt compression for large language models: A survey, 2024{\natexlab{m}}.
\newblock URL \url{https://arxiv.org/abs/2410.12388}.

\bibitem[Li et~al.(2024{\natexlab{n}})Li, Su, and Collier]{li2024500xcompressorgeneralizedpromptcompression}
Zongqian Li, Yixuan Su, and Nigel Collier.
\newblock 500xcompressor: Generalized prompt compression for large language models, 2024{\natexlab{n}}.
\newblock URL \url{https://arxiv.org/abs/2408.03094}.

\bibitem[Liang et~al.(2024)Liang, Li, Bai, Huang, Sun, Wang, He, Cui, Chen, and Zhang]{liang-2024-arxiv-keyvideollm}
Hao Liang, Jiapeng Li, Tianyi Bai, Xijie Huang, Linzhuang Sun, Zhengren Wang, Conghui He, Bin Cui, Chong Chen, and Wentao Zhang.
\newblock Keyvideollm: Towards large-scale video keyframe selection.
\newblock \emph{CoRR}, abs/2407.03104, 2024.
\newblock \doi{10.48550/ARXIV.2407.03104}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2407.03104}.

\bibitem[Liang et~al.(2023)Liang, Tang, Li, and Zhang]{liang2023open}
Xiaobo Liang, Zecheng Tang, Juntao Li, and Min Zhang.
\newblock Open-ended long text generation via masked language modeling.
\newblock In \emph{Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pages 223--241, 2023.

\bibitem[Lieber et~al.(2024)Lieber, Lenz, Bata, Cohen, Osin, Dalmedigos, Safahi, Meirom, Belinkov, Shalev-Shwartz, et~al.]{lieber2024jamba}
Opher Lieber, Barak Lenz, Hofit Bata, Gal Cohen, Jhonathan Osin, Itay Dalmedigos, Erez Safahi, Shaked Meirom, Yonatan Belinkov, Shai Shalev-Shwartz, et~al.
\newblock Jamba: A hybrid transformer-mamba language model.
\newblock \emph{arXiv preprint arXiv:2403.19887}, 2024.

\bibitem[Lin(2004)]{lin2004rouge}
Chin-Yew Lin.
\newblock Rouge: A package for automatic evaluation of summaries.
\newblock In \emph{Text summarization branches out}, pages 74--81, 2004.

\bibitem[Lin et~al.(2024)Lin, Tang, Tang, Yang, Chen, Wang, Xiao, Dang, Gan, and Han]{lin2024awq}
Ji~Lin, Jiaming Tang, Haotian Tang, Shang Yang, Wei-Ming Chen, Wei-Chen Wang, Guangxuan Xiao, Xingyu Dang, Chuang Gan, and Song Han.
\newblock Awq: Activation-aware weight quantization for on-device llm compression and acceleration.
\newblock \emph{Proceedings of Machine Learning and Systems}, 6:\penalty0 87--100, 2024.

\bibitem[Ling et~al.(2025)Ling, Liu, Yan, Yang, Lin, Fan, Shen, Du, and Chen]{ling2025longreason}
Zhan Ling, Kang Liu, Kai Yan, Yifan Yang, Weijian Lin, Ting-Han Fan, Lingfeng Shen, Zhengyin Du, and Jiecao Chen.
\newblock Longreason: A synthetic long-context reasoning benchmark via context expansion.
\newblock \emph{arXiv preprint arXiv:2501.15089}, 2025.

\bibitem[Lingle(2024)]{lingle2024transformervqlineartimetransformersvector}
Lucas~D. Lingle.
\newblock Transformer-vq: Linear-time transformers via vector quantization.
\newblock In \emph{{ICLR}}. OpenReview.net, 2024.

\bibitem[Liskavets et~al.(2024)Liskavets, Ushakov, Roy, Klibanov, Etemad, and Luke]{liskavets2024promptcompressioncontextawaresentence}
Barys Liskavets, Maxim Ushakov, Shuvendu Roy, Mark Klibanov, Ali Etemad, and Shane Luke.
\newblock Prompt compression with context-aware sentence encoding for fast and improved llm inference, 2024.
\newblock URL \url{https://arxiv.org/abs/2409.01227}.

\bibitem[Liu et~al.(2024{\natexlab{a}})Liu, Feng, Wang, Wang, Liu, Zhao, Deng, Ruan, Dai, Guo, et~al.]{liu2024deepseek}
Aixin Liu, Bei Feng, Bin Wang, Bingxuan Wang, Bo~Liu, Chenggang Zhao, Chengqi Deng, Chong Ruan, Damai Dai, Daya Guo, et~al.
\newblock Deepseek-v2: A strong, economical, and efficient mixture-of-experts language model.
\newblock \emph{CoRR}, 2024{\natexlab{a}}.

\bibitem[Liu et~al.(2023{\natexlab{a}})Liu, Zaharia, and Abbeel]{liu2023ring}
Hao Liu, Matei Zaharia, and Pieter Abbeel.
\newblock Ring attention with blockwise transformers for near-infinite context.
\newblock \emph{arXiv preprint arXiv:2310.01889}, 2023{\natexlab{a}}.

\bibitem[Liu et~al.(2024{\natexlab{b}})Liu, Yan, Zaharia, and Abbeel]{liu2024world}
Hao Liu, Wilson Yan, Matei Zaharia, and Pieter Abbeel.
\newblock World model on million-length video and language with ringattention.
\newblock \emph{arXiv e-prints}, pages arXiv--2402, 2024{\natexlab{b}}.

\bibitem[Liu et~al.(2024{\natexlab{c}})Liu, Ni, Que, Sun, Wang, Yang, JiakaiWang, Guo, Peng, Zhang, Tian, Bu, Xu, Rong, Peng, and Zhang]{liu2024roleagent}
Jiaheng Liu, Zehao Ni, Haoran Que, Tao Sun, Noah Wang, Jian Yang, JiakaiWang, Hongcheng Guo, Z.Y. Peng, Ge~Zhang, Jiayi Tian, Xingyuan Bu, Ke~Xu, Wenge Rong, Junran Peng, and Zhaoxiang Zhang.
\newblock Roleagent: Building, interacting, and benchmarking high-quality role-playing agents from scripts.
\newblock In \emph{The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track}, 2024{\natexlab{c}}.
\newblock URL \url{https://openreview.net/forum?id=hORTHzt2cE}.

\bibitem[Liu et~al.(2024{\natexlab{d}})Liu, ZhiqiBai, Zhang, Zhang, YuangZh, Zhang, JiakaiWang, Que, Chen, Su, Ge, Fu, Chen, and Zheng]{liu-etal-2024-e2}
Jiaheng Liu, ZhiqiBai ZhiqiBai, Yuanxing Zhang, Chenchen Zhang, YuangZh YuangZh, Ge~Zhang, JiakaiWang JiakaiWang, Haoran Que, Yukang Chen, Wenbo Su, Tiezheng Ge, Jie Fu, Wenhu Chen, and Bo~Zheng.
\newblock E2-{LLM}: Efficient and extreme length extension of large language models.
\newblock In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, \emph{Findings of the Association for Computational Linguistics: ACL 2024}, pages 4243--4253, Bangkok, Thailand, August 2024{\natexlab{d}}. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2024.findings-acl.252}.
\newblock URL \url{https://aclanthology.org/2024.findings-acl.252/}.

\bibitem[Liu et~al.(2024{\natexlab{e}})Liu, Tian, Daita, Wei, Ding, Wang, Yang, and Zhang]{liu2024repoqa}
Jiawei Liu, Jia~Le Tian, Vijay Daita, Yuxiang Wei, Yifeng Ding, Yuhan~Katherine Wang, Jun Yang, and Lingming Zhang.
\newblock Repoqa: Evaluating long context code understanding.
\newblock \emph{arXiv preprint arXiv:2406.06025}, 2024{\natexlab{e}}.

\bibitem[Liu et~al.(2024{\natexlab{f}})Liu, Song, Lin, Lam, Neubig, Li, and Yue]{DBLP:journals/corr/abs-2404-05955}
Junpeng Liu, Yifan Song, Bill~Yuchen Lin, Wai Lam, Graham Neubig, Yuanzhi Li, and Xiang Yue.
\newblock Visualwebbench: How far have multimodal llms evolved in web page understanding and grounding?
\newblock \emph{CoRR}, abs/2404.05955, 2024{\natexlab{f}}.
\newblock \doi{10.48550/ARXIV.2404.05955}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2404.05955}.

\bibitem[Liu et~al.(2024{\natexlab{g}})Liu, Chen, Liu, Peng, and Lou]{liu2024stall+}
Junwei Liu, Yixuan Chen, Mingwei Liu, Xin Peng, and Yiling Lou.
\newblock Stall+: Boosting llm-based repository-level code completion with static analysis.
\newblock \emph{arXiv preprint arXiv:2406.10018}, 2024{\natexlab{g}}.

\bibitem[Liu et~al.(2023{\natexlab{b}})Liu, Li, Xiang, Wang, and Qian]{liu-etal-2023-tcra}
Junyi Liu, Liangzhi Li, Tong Xiang, Bowen Wang, and Yiming Qian.
\newblock {TCRA}-{LLM}: Token compression retrieval augmented large language model for inference cost reduction.
\newblock In Houda Bouamor, Juan Pino, and Kalika Bali, editors, \emph{Findings of the Association for Computational Linguistics: EMNLP 2023}, pages 9796--9810, Singapore, December 2023{\natexlab{b}}. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2023.findings-emnlp.655}.
\newblock URL \url{https://aclanthology.org/2023.findings-emnlp.655}.

\bibitem[Liu et~al.(2024{\natexlab{h}})Liu, Lin, Hewitt, Paranjape, Bevilacqua, Petroni, and Liang]{liu2024lost}
Nelson~F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang.
\newblock Lost in the middle: How language models use long contexts.
\newblock \emph{Transactions of the Association for Computational Linguistics}, 12:\penalty0 157--173, 2024{\natexlab{h}}.

\bibitem[Liu et~al.(2024{\natexlab{i}})Liu, Gao, Zhao, Ma, Wang, and Wen]{liu2024unlocking}
Peiyu Liu, Ze-Feng Gao, Wayne~Xin Zhao, Yipeng Ma, Tao Wang, and Ji-Rong Wen.
\newblock Unlocking data-free low-bit quantization with matrix decomposition for kv cache compression.
\newblock In \emph{Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pages 2430--2440, 2024{\natexlab{i}}.

\bibitem[Liu et~al.(2024{\natexlab{j}})Liu, Zheng, Muennighoff, Zeng, Dou, Pang, Jiang, and Lin]{liu2024regmix}
Qian Liu, Xiaosen Zheng, Niklas Muennighoff, Guangtao Zeng, Longxu Dou, Tianyu Pang, Jing Jiang, and Min Lin.
\newblock Regmix: Data mixture as regression for language model pre-training.
\newblock \emph{arXiv preprint arXiv:2407.01492}, 2024{\natexlab{j}}.

\bibitem[Liu et~al.(2024{\natexlab{k}})Liu, Chai, Yang, Shi, Zhu, Wang, Jin, Zhang, Zhu, Guo, et~al.]{mdeval}
Shukai Liu, Linzheng Chai, Jian Yang, Jiajun Shi, He~Zhu, Liran Wang, Ke~Jin, Wei Zhang, Hualei Zhu, Shuyue Guo, et~al.
\newblock Mdeval: Massively multilingual code debugging.
\newblock \emph{arXiv preprint arXiv:2411.02310}, 2024{\natexlab{k}}.

\bibitem[Liu et~al.(2024{\natexlab{l}})Liu, Xu, and McAuley]{liu2024repobench}
Tianyang Liu, Canwen Xu, and Julian McAuley.
\newblock Repobench: Benchmarking repository-level code auto-completion systems.
\newblock In \emph{The Twelfth International Conference on Learning Representations}, 2024{\natexlab{l}}.

\bibitem[Liu et~al.(2024{\natexlab{m}})Liu, Shi, Hong, Hu, Yin, and Zhang]{liu-2024-arxiv-mustdrop}
Ting Liu, Liangtao Shi, Richang Hong, Yue Hu, Quanjun Yin, and Linfeng Zhang.
\newblock Multi-stage vision token dropping: Towards efficient multimodal large language model.
\newblock \emph{CoRR}, abs/2411.10803, 2024{\natexlab{m}}.
\newblock \doi{10.48550/ARXIV.2411.10803}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2411.10803}.

\bibitem[Liu et~al.(2024{\natexlab{n}})Liu, Zeng, He, Jiang, and He]{liu2024what}
Wei Liu, Weihao Zeng, Keqing He, Yong Jiang, and Junxian He.
\newblock What makes good data for alignment? a comprehensive study of automatic data selection in instruction tuning.
\newblock In \emph{The Twelfth International Conference on Learning Representations}, 2024{\natexlab{n}}.

\bibitem[Liu et~al.(2024{\natexlab{o}})Liu, Dong, Hu, and Chu]{liu2024longgenbench}
Xiang Liu, Peijie Dong, Xuming Hu, and Xiaowen Chu.
\newblock Longgenbench: Long-context generation benchmark.
\newblock \emph{arXiv preprint arXiv:2410.04199}, 2024{\natexlab{o}}.

\bibitem[Liu et~al.(2023{\natexlab{c}})Liu, Yan, Zhang, An, Qiu, and Lin]{liu2023scaling}
Xiaoran Liu, Hang Yan, Shuo Zhang, Chenxin An, Xipeng Qiu, and Dahua Lin.
\newblock Scaling laws of rope-based extrapolation.
\newblock \emph{arXiv preprint arXiv:2310.05209}, 2023{\natexlab{c}}.

\bibitem[Liu et~al.(2024{\natexlab{p}})Liu, Lv, Guo, Yan, He, Qiu, and Lin]{liu-etal-2024-longwanjuan}
Xiaoran Liu, Kai Lv, Qipeng Guo, Hang Yan, Conghui He, Xipeng Qiu, and Dahua Lin.
\newblock {L}ong{W}anjuan: Towards systematic measurement for long text quality.
\newblock In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen, editors, \emph{Findings of the Association for Computational Linguistics: EMNLP 2024}, pages 5709--5725, Miami, Florida, USA, November 2024{\natexlab{p}}. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2024.findings-emnlp.327}.
\newblock URL \url{https://aclanthology.org/2024.findings-emnlp.327}.

\bibitem[Liu et~al.(2025)Liu, Li, Huang, Liu, Song, Guo, He, Wang, Li, Liu, Zhou, Huang, and Qiu]{Liu2025ThusSL}
Xiaoran Liu, Ruixiao Li, Mianqiu Huang, Zhigeng Liu, Yuerong Song, Qipeng Guo, Siyang He, Qiqi Wang, Linlin Li, Qun Liu, Yaqian Zhou, Xuanjing Huang, and Xipeng Qiu.
\newblock Thus spake long-context large language model.
\newblock 2025.
\newblock URL \url{https://api.semanticscholar.org/CorpusID:276575712}.

\bibitem[Liu et~al.(2022)Liu, Liu, Chen, Lu, Feng, Feng, Sun, Tian, Wu, and Wang]{ERNIE-SPARSE}
Yang Liu, Jiaxiang Liu, Li~Chen, Yuxiang Lu, Shikun Feng, Zhida Feng, Yu~Sun, Hao Tian, Hua Wu, and Haifeng Wang.
\newblock {ERNIE-SPARSE:} learning hierarchical efficient transformer through regularized self-attention.
\newblock \emph{CoRR}, abs/2203.12276, 2022.

\bibitem[Liu et~al.(2024{\natexlab{q}})Liu, He, Han, Zhang, Liu, Tian, Zhang, Wang, Gao, Zhong, et~al.]{liu2024understanding}
Yiheng Liu, Hao He, Tianle Han, Xu~Zhang, Mengyuan Liu, Jiaming Tian, Yutong Zhang, Jiaqi Wang, Xiaohui Gao, Tianyang Zhong, et~al.
\newblock Understanding llms: A comprehensive overview from training to inference.
\newblock \emph{arXiv preprint arXiv:2401.02038}, 2024{\natexlab{q}}.

\bibitem[Liu et~al.(2024{\natexlab{r}})Liu, Li, Cheng, Ray, Huang, Zhang, Du, Yao, Lu, Ananthanarayanan, et~al.]{liu2024cachegen}
Yuhan Liu, Hanchen Li, Yihua Cheng, Siddhant Ray, Yuyang Huang, Qizheng Zhang, Kuntai Du, Jiayi Yao, Shan Lu, Ganesh Ananthanarayanan, et~al.
\newblock Cachegen: Kv cache compression and streaming for fast large language model serving.
\newblock In \emph{Proceedings of the ACM SIGCOMM 2024 Conference}, pages 38--56, 2024{\natexlab{r}}.

\bibitem[Liu et~al.(2023{\natexlab{d}})Liu, Desai, Liao, Wang, Xie, Xu, Kyrillidis, and Shrivastava]{NEURIPS2023_a452a7c6}
Zichang Liu, Aditya Desai, Fangshuo Liao, Weitao Wang, Victor Xie, Zhaozhuo Xu, Anastasios Kyrillidis, and Anshumali Shrivastava.
\newblock Scissorhands: Exploiting the persistence of importance hypothesis for llm kv cache compression at test time.
\newblock In A.~Oh, T.~Naumann, A.~Globerson, K.~Saenko, M.~Hardt, and S.~Levine, editors, \emph{Advances in Neural Information Processing Systems}, volume~36, pages 52342--52364. Curran Associates, Inc., 2023{\natexlab{d}}.
\newblock URL \url{https://proceedings.neurips.cc/paper_files/paper/2023/file/a452a7c6c463e4ae8fbdc614c6e983e6-Paper-Conference.pdf}.

\bibitem[Liu et~al.(2024{\natexlab{s}})Liu, Yuan, Jin, Zhong, Xu, Braverman, Chen, and Hu]{pmlr-v235-liu24bz}
Zirui Liu, Jiayi Yuan, Hongye Jin, Shaochen Zhong, Zhaozhuo Xu, Vladimir Braverman, Beidi Chen, and Xia Hu.
\newblock {KIVI}: A tuning-free asymmetric 2bit quantization for {KV} cache.
\newblock In \emph{Proceedings of the 41st International Conference on Machine Learning}, pages 32332--32344, 2024{\natexlab{s}}.

\bibitem[Long et~al.(2023)Long, Chowdhury, and Roy]{segmentrecurrent}
Yinghan Long, Sayeed~Shafayet Chowdhury, and Kaushik Roy.
\newblock Segmented recurrent transformer: An efficient sequence-to-sequence model.
\newblock In \emph{{EMNLP} (Findings)}, pages 8325--8337. Association for Computational Linguistics, 2023.

\bibitem[Lozhkov et~al.(2024)Lozhkov, Li, Allal, Cassano, Lamy{-}Poirier, Tazi, Tang, Pykhtar, Liu, Wei, Liu, Tian, Kocetkov, Zucker, Belkada, Wang, Liu, Abulkhanov, Paul, Li, Li, Risdal, Li, Zhu, Zhuo, Zheltonozhskii, Dade, Yu, Krau{\ss}, Jain, Su, He, Dey, Abati, Chai, Muennighoff, Tang, Oblokulov, Akiki, Marone, Mou, Mishra, Gu, Hui, Dao, Zebaze, Dehaene, Patry, Xu, McAuley, Hu, Scholak, Paquet, Robinson, Anderson, Chapados, and et~al.]{lozhkov2024starcoder2stackv2}
Anton Lozhkov, Raymond Li, Loubna~Ben Allal, Federico Cassano, Joel Lamy{-}Poirier, Nouamane Tazi, Ao~Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei, Tianyang Liu, Max Tian, Denis Kocetkov, Arthur Zucker, Younes Belkada, Zijian Wang, Qian Liu, Dmitry Abulkhanov, Indraneil Paul, Zhuang Li, Wen{-}Ding Li, Megan Risdal, Jia Li, Jian Zhu, Terry~Yue Zhuo, Evgenii Zheltonozhskii, Nii Osae~Osae Dade, Wenhao Yu, Lucas Krau{\ss}, Naman Jain, Yixuan Su, Xuanli He, Manan Dey, Edoardo Abati, Yekun Chai, Niklas Muennighoff, Xiangru Tang, Muhtasham Oblokulov, Christopher Akiki, Marc Marone, Chenghao Mou, Mayank Mishra, Alex Gu, Binyuan Hui, Tri Dao, Armel Zebaze, Olivier Dehaene, Nicolas Patry, Canwen Xu, Julian~J. McAuley, Han Hu, Torsten Scholak, S{\'{e}}bastien Paquet, Jennifer Robinson, Carolyn~Jane Anderson, Nicolas Chapados, and et~al.
\newblock Starcoder 2 and the stack v2: The next generation.
\newblock \emph{CoRR}, abs/2402.19173, 2024.
\newblock \doi{10.48550/ARXIV.2402.19173}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2402.19173}.

\bibitem[Lu et~al.(2025{\natexlab{a}})Lu, Jiang, Liu, Du, Jiang, Hong, Liu, He, Yuan, Wang, Huang, Yuan, Xu, Xu, Lai, Chen, Zheng, Yan, Su, Wu, Zhang, Yang, Zhou, Zhang, and Qiu]{lu2025mobamixtureblockattention}
Enzhe Lu, Zhejun Jiang, Jingyuan Liu, Yulun Du, Tao Jiang, Chao Hong, Shaowei Liu, Weiran He, Enming Yuan, Yuzhi Wang, Zhiqi Huang, Huan Yuan, Suting Xu, Xinran Xu, Guokun Lai, Yanru Chen, Huabin Zheng, Junjie Yan, Jianlin Su, Yuxin Wu, Neo~Y. Zhang, Zhilin Yang, Xinyu Zhou, Mingxing Zhang, and Jiezhong Qiu.
\newblock Moba: Mixture of block attention for long-context llms, 2025{\natexlab{a}}.
\newblock URL \url{https://arxiv.org/abs/2502.13189}.

\bibitem[Lu et~al.(2025{\natexlab{b}})Lu, Jiang, Liu, Du, Jiang, Hong, Liu, He, Yuan, Wang, et~al.]{lu2025moba}
Enzhe Lu, Zhejun Jiang, Jingyuan Liu, Yulun Du, Tao Jiang, Chao Hong, Shaowei Liu, Weiran He, Enming Yuan, Yuzhi Wang, et~al.
\newblock Moba: Mixture of block attention for long-context llms.
\newblock \emph{arXiv preprint arXiv:2502.13189}, 2025{\natexlab{b}}.

\bibitem[Lu et~al.(2024{\natexlab{a}})Lu, Nie, Liang, Pan, Zhang, Zhao, Chen, Zhou, Dong, Cui, et~al.]{lu2024datasculpt}
Keer Lu, Xiaonan Nie, Zheng Liang, Da~Pan, Shusen Zhang, Keshi Zhao, Weipeng Chen, Zenan Zhou, Guosheng Dong, Bin Cui, et~al.
\newblock Datasculpt: Crafting data landscapes for long-context llms through multi-objective partitioning.
\newblock \emph{arXiv preprint arXiv:2409.00997}, 2024{\natexlab{a}}.

\bibitem[Lu et~al.(2024{\natexlab{b}})Lu, Yan, Yang, Chiu, Ren, Yuan, Zhao, Wu, and Rush]{lu2024controlled}
Yi~Lu, Jing~Nathan Yan, Songlin Yang, Justin~T Chiu, Siyu Ren, Fei Yuan, Wenting Zhao, Zhiyong Wu, and Alexander~M Rush.
\newblock A controlled study on long context extension and generalization in llms.
\newblock \emph{arXiv preprint arXiv:2409.12181}, 2024{\natexlab{b}}.

\bibitem[Lu et~al.(2024{\natexlab{c}})Lu, Zhou, He, Zhao, Ji, Gui, Zhang, and Huang]{lu2024longheadsmultiheadattentionsecretly4}
Yi~Lu, Xin Zhou, Wei He, Jun Zhao, Tao Ji, Tao Gui, Qi~Zhang, and Xuanjing Huang.
\newblock Longheads: Multi-head attention is secretly a long context processor.
\newblock In \emph{{EMNLP} (Findings)}, pages 7136--7148. Association for Computational Linguistics, 2024{\natexlab{c}}.

\bibitem[Lucas et~al.(2024)Lucas, Kangas, and Havens]{lucas2024extraglobalattentiondesignation}
Evan Lucas, Dylan Kangas, and Timothy~C Havens.
\newblock Extra global attention designation using keyword detection in sparse transformer architectures, 2024.
\newblock URL \url{https://arxiv.org/abs/2410.08971}.

\bibitem[Luo et~al.(2024)Luo, Ye, Liang, Zhang, Qin, Lu, Wu, Cong, Lin, Zhang, et~al.]{luo2024repoagent}
Qinyu Luo, Yining Ye, Shihao Liang, Zhong Zhang, Yujia Qin, Yaxi Lu, Yesai Wu, Xin Cong, Yankai Lin, Yingli Zhang, et~al.
\newblock Repoagent: An llm-powered open-source framework for repository-level code documentation generation.
\newblock \emph{arXiv preprint arXiv:2402.16667}, 2024.

\bibitem[Lyu et~al.(2024)Lyu, Du, Xu, Duan, Wu, Lynn, Aji, Wong, and Wang]{lyu2023paradigm}
Chenyang Lyu, Zefeng Du, Jitao Xu, Yitao Duan, Minghao Wu, Teresa Lynn, Alham~Fikri Aji, Derek~F. Wong, and Longyue Wang.
\newblock A paradigm shift: The future of machine translation lies with large language models.
\newblock In Nicoletta Calzolari, Min{-}Yen Kan, V{\'{e}}ronique Hoste, Alessandro Lenci, Sakriani Sakti, and Nianwen Xue, editors, \emph{Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation, {LREC/COLING} 2024, 20-25 May, 2024, Torino, Italy}, pages 1339--1352. {ELRA} and {ICCL}, 2024.
\newblock URL \url{https://aclanthology.org/2024.lrec-main.120}.

\bibitem[Ma et~al.(2024{\natexlab{a}})Ma, Xue, Wang, Zhou, Rao, Yan, Zhang, Wu, Shou, and Sun]{ma-2024-arxiv-vlora}
Feipeng Ma, Hongwei Xue, Guangting Wang, Yizhou Zhou, Fengyun Rao, Shilin Yan, Yueyi Zhang, Siying Wu, Mike~Zheng Shou, and Xiaoyan Sun.
\newblock Visual perception by large language model's weights.
\newblock \emph{CoRR}, abs/2405.20339, 2024{\natexlab{a}}.
\newblock \doi{10.48550/ARXIV.2405.20339}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2405.20339}.

\bibitem[Ma et~al.(2023)Ma, Gong, He, hai zhao, and Duan]{ma2023query}
Xinbei Ma, Yeyun Gong, Pengcheng He, hai zhao, and Nan Duan.
\newblock Query rewriting in retrieval-augmented large language models.
\newblock In \emph{The 2023 Conference on Empirical Methods in Natural Language Processing}, 2023.
\newblock URL \url{https://openreview.net/forum?id=gXq1cwkUZc}.

\bibitem[Ma et~al.(2025)Ma, Wan, Yu, Fang, and Wang]{ma2025cot}
Xinyin Ma, Guangnian Wan, Runpeng Yu, Gongfan Fang, and Xinchao Wang.
\newblock Cot-valve: Length-compressible chain-of-thought tuning.
\newblock \emph{arXiv preprint arXiv:2502.09601}, 2025.

\bibitem[Ma et~al.(2024{\natexlab{b}})Ma, Wang, Yang, Wei, and Lin]{ma2024fine}
Xueguang Ma, Liang Wang, Nan Yang, Furu Wei, and Jimmy Lin.
\newblock Fine-tuning llama for multi-stage text retrieval.
\newblock In \emph{Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval}, pages 2421--2425, 2024{\natexlab{b}}.

\bibitem[Ma et~al.(2024{\natexlab{c}})Ma, Zang, Chen, Chen, Jiao, Li, Lu, Liu, Ma, Dong, Zhang, Pan, Jiang, Wang, Cao, and Sun]{ma-2024-arxiv-mmlongbenchdoc}
Yubo Ma, Yuhang Zang, Liangyu Chen, Meiqi Chen, Yizhu Jiao, Xinze Li, Xinyuan Lu, Ziyu Liu, Yan Ma, Xiaoyi Dong, Pan Zhang, Liangming Pan, Yu{-}Gang Jiang, Jiaqi Wang, Yixin Cao, and Aixin Sun.
\newblock Mmlongbench-doc: Benchmarking long-context document understanding with visualizations.
\newblock \emph{CoRR}, abs/2407.01523, 2024{\natexlab{c}}.
\newblock \doi{10.48550/ARXIV.2407.01523}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2407.01523}.

\bibitem[Madaan et~al.(2022)Madaan, Tandon, Clark, and Yang]{memprompt}
Aman Madaan, Niket Tandon, Peter Clark, and Yiming Yang.
\newblock Memory-assisted prompt editing to improve gpt-3 after deployment.
\newblock \emph{arXiv preprint arXiv: 2201.06009}, 2022.

\bibitem[Maekawa\textsuperscript{*} et~al.(2024)Maekawa\textsuperscript{*}, Iso\textsuperscript{*}, and Bhutani]{maekawa2025holistic}
Seiji Maekawa\textsuperscript{*}, Hayate Iso\textsuperscript{*}, and Nikita Bhutani.
\newblock Holistic reasoning with long-context lms: A benchmark for database operations on massive textual data, 2024.
\newblock URL \url{https://arxiv.org/abs/2410.11996}.
\newblock \textsuperscript{*}These authors contributed equally to this work.

\bibitem[Maharana et~al.(2023)Maharana, Yadav, and Bansal]{D2}
Adyasha Maharana, Prateek Yadav, and Mohit Bansal.
\newblock {D2} pruning: Message passing for balancing diversity and difficulty in data pruning.
\newblock \emph{CoRR}, abs/2310.07931, 2023.
\newblock \doi{10.48550/ARXIV.2310.07931}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2310.07931}.

\bibitem[Malaviya et~al.(2023)Malaviya, Lee, Chen, Sieber, Yatskar, and Roth]{malaviya2023expertqa}
Chaitanya Malaviya, Subin Lee, Sihao Chen, Elizabeth Sieber, Mark Yatskar, and Dan Roth.
\newblock Expertqa: Expert-curated questions and attributed answers.
\newblock \emph{arXiv preprint arXiv:2309.07852}, 2023.

\bibitem[Malaviya et~al.(2024)Malaviya, Agrawal, Ganchev, Srinivasan, Huot, Berant, Yatskar, Das, Lapata, and Alberti]{malaviya2024dolomites}
Chaitanya Malaviya, Priyanka Agrawal, Kuzman Ganchev, Pranesh Srinivasan, Fantine Huot, Jonathan Berant, Mark Yatskar, Dipanjan Das, Mirella Lapata, and Chris Alberti.
\newblock Dolomites: Domain-specific long-form methodical tasks.
\newblock \emph{arXiv preprint arXiv:2405.05938}, 2024.

\bibitem[Masry and Hajian(2024{\natexlab{a}})]{DBLP:journals/corr/abs-2401-15050}
Ahmed Masry and Amir Hajian.
\newblock Longfin: {A} multimodal document understanding model for long financial domain documents.
\newblock \emph{CoRR}, abs/2401.15050, 2024{\natexlab{a}}.
\newblock \doi{10.48550/ARXIV.2401.15050}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2401.15050}.

\bibitem[Masry and Hajian(2024{\natexlab{b}})]{masry2024longfin}
Ahmed Masry and Amir Hajian.
\newblock Longfin: A multimodal document understanding model for long financial domain documents.
\newblock \emph{arXiv preprint arXiv:2401.15050}, 2024{\natexlab{b}}.

\bibitem[Mehta et~al.(2022)Mehta, Gupta, Tay, Dehghani, Tran, Rao, Najork, Strubell, and Metzler]{dsiplusplus}
Sanket~Vaibhav Mehta, Jai Gupta, Yi~Tay, Mostafa Dehghani, Vinh~Q. Tran, J.~Rao, Marc Najork, Emma Strubell, and Donald Metzler.
\newblock Dsi++: Updating transformer memory with new documents.
\newblock \emph{Conference on Empirical Methods in Natural Language Processing}, 2022.
\newblock \doi{10.48550/arXiv.2212.09744}.

\bibitem[Men et~al.(2024)Men, Xu, Wang, Zhang, Lin, Han, and Chen]{men2024baseropeboundscontext}
Xin Men, Mingyu Xu, Bingning Wang, Qingyu Zhang, Hongyu Lin, Xianpei Han, and Weipeng Chen.
\newblock Base of rope bounds context length, 2024.
\newblock URL \url{https://arxiv.org/abs/2405.14591}.

\bibitem[Meng et~al.(2022)Meng, Bau, Andonian, and Belinkov]{meng2022locating}
Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov.
\newblock Locating and editing factual associations in gpt.
\newblock In S.~Koyejo, S.~Mohamed, A.~Agarwal, D.~Belgrave, K.~Cho, and A.~Oh, editors, \emph{Advances in Neural Information Processing Systems}, volume~35, pages 17359--17372. Curran Associates, Inc., 2022.
\newblock URL \url{https://proceedings.neurips.cc/paper_files/paper/2022/file/6f1d43d5a82a37e89b0665b33bf3a182-Paper-Conference.pdf}.

\bibitem[Meng et~al.(2024)Meng, Xia, and Chen]{Meng2024SimPOSP}
Yu~Meng, Mengzhou Xia, and Danqi Chen.
\newblock Simpo: Simple preference optimization with a reference-free reward.
\newblock \emph{ArXiv}, abs/2405.14734, 2024.

\bibitem[Micikevicius et~al.(2017)Micikevicius, Narang, Alben, Diamos, Elsen, Garcia, Ginsburg, Houston, Kuchaiev, Venkatesh, et~al.]{micikevicius2017mixed}
Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, et~al.
\newblock Mixed precision training.
\newblock \emph{arXiv preprint arXiv:1710.03740}, 2017.

\bibitem[Microsoft(2023)]{bing_copilot}
Microsoft.
\newblock Microsoft copilot, 2023.
\newblock URL \url{https://copilot.microsoft.com/}.

\bibitem[Miculicich et~al.(2018)Miculicich, Ram, Pappas, and Henderson]{miculicich2018document}
Lesly Miculicich, Dhananjay Ram, Nikolaos Pappas, and James Henderson.
\newblock Document-level neural machine translation with hierarchical attention networks.
\newblock In Ellen Riloff, David Chiang, Julia Hockenmaier, and Jun'ichi Tsujii, editors, \emph{Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, October 31 - November 4, 2018}, pages 2947--2954. Association for Computational Linguistics, 2018.
\newblock \doi{10.18653/V1/D18-1325}.
\newblock URL \url{https://doi.org/10.18653/v1/d18-1325}.

\bibitem[Mikolov et~al.(2010)Mikolov, Karafi{\'a}t, Burget, Cernock{\`y}, and Khudanpur]{mikolov2010recurrent}
Tomas Mikolov, Martin Karafi{\'a}t, Lukas Burget, Jan Cernock{\`y}, and Sanjeev Khudanpur.
\newblock Recurrent neural network based language model.
\newblock In \emph{Interspeech}, volume~2, pages 1045--1048. Makuhari, 2010.

\bibitem[Miller(2023)]{attn_off_by_one}
Evan Miller.
\newblock Attention is off by one, 7 2023.
\newblock URL \url{https://www.evanmiller.org/attention-is-off-by-one.html}.

\bibitem[Min et~al.(2020)Min, Michael, Hajishirzi, and Zettlemoyer]{min2020ambigqa}
Sewon Min, Julian Michael, Hannaneh Hajishirzi, and Luke Zettlemoyer.
\newblock Ambigqa: Answering ambiguous open-domain questions.
\newblock \emph{arXiv preprint arXiv:2004.10645}, 2020.

\bibitem[Min et~al.(2023)Min, Krishna, Lyu, Lewis, Yih, Koh, Iyyer, Zettlemoyer, and Hajishirzi]{min2023factscore}
Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang~Wei Koh, Mohit Iyyer, Luke Zettlemoyer, and Hannaneh Hajishirzi.
\newblock Factscore: Fine-grained atomic evaluation of factual precision in long form text generation.
\newblock \emph{arXiv preprint arXiv:2305.14251}, 2023.

\bibitem[MiniMax et~al.(2025)MiniMax, Li, Gong, Yang, Shan, Liu, Zhu, Zhang, Guo, Chen, Li, Jiao, Li, Zhang, Sun, Dong, Zhu, Zhuang, Song, Zhu, Han, Li, Xie, Xu, Yan, Zhang, Xiao, Kang, Han, Wang, Yu, Feng, Zheng, Chai, Xing, Ju, Chi, Zhang, Huang, Niu, Li, Zhao, Yang, Xu, Wang, Wang, Li, Leng, Shi, Yu, Li, Zhu, Huang, Liang, Sun, Sun, Cheng, Li, Song, Su, Han, Zhang, Hou, Min, Zou, Shen, Gong, Zhu, Zhou, Zhong, Hu, Fan, Yu, Yang, Li, Huang, Li, Huang, Xu, Mao, Li, Li, Tao, Ying, Cong, Qin, Fan, Yu, Jiang, and Wu]{minimax2025minimax01scalingfoundationmodels}
MiniMax, Aonian Li, Bangwei Gong, Bo~Yang, Boji Shan, Chang Liu, Cheng Zhu, Chunhao Zhang, Congchao Guo, Da~Chen, Dong Li, Enwei Jiao, Gengxin Li, Guojun Zhang, Haohai Sun, Houze Dong, Jiadai Zhu, Jiaqi Zhuang, Jiayuan Song, Jin Zhu, Jingtao Han, Jingyang Li, Junbin Xie, Junhao Xu, Junjie Yan, Kaishun Zhang, Kecheng Xiao, Kexi Kang, Le~Han, Leyang Wang, Lianfei Yu, Liheng Feng, Lin Zheng, Linbo Chai, Long Xing, Meizhi Ju, Mingyuan Chi, Mozhi Zhang, Peikai Huang, Pengcheng Niu, Pengfei Li, Pengyu Zhao, Qi~Yang, Qidi Xu, Qiexiang Wang, Qin Wang, Qiuhui Li, Ruitao Leng, Shengmin Shi, Shuqi Yu, Sichen Li, Songquan Zhu, Tao Huang, Tianrun Liang, Weigao Sun, Weixuan Sun, Weiyu Cheng, Wenkai Li, Xiangjun Song, Xiao Su, Xiaodong Han, Xinjie Zhang, Xinzhu Hou, Xu~Min, Xun Zou, Xuyang Shen, Yan Gong, Yingjie Zhu, Yipeng Zhou, Yiran Zhong, Yongyi Hu, Yuanxiang Fan, Yue Yu, Yufeng Yang, Yuhao Li, Yunan Huang, Yunji Li, Yunpeng Huang, Yunzhi Xu, Yuxin Mao, Zehan Li, Zekang Li, Zewei Tao, Zewen Ying, Zhaoyang Cong, Zhen
  Qin, Zhenhua Fan, Zhihang Yu, Zhuo Jiang, and Zijia Wu.
\newblock Minimax-01: Scaling foundation models with lightning attention, 2025.
\newblock URL \url{https://arxiv.org/abs/2501.08313}.

\bibitem[Mishra et~al.(2024)Mishra, Stallone, Zhang, Shen, Prasad, Soria, Merler, Selvam, Surendran, Singh, Sethi, Dang, Li, Wu, Zawad, Coleman, White, Lewis, Pavuluri, Koyfman, Lublinsky, de~Bayser, Abdelaziz, Basu, Agarwal, Zhou, Johnson, Goyal, Patel, Shah, Zerfos, Ludwig, Munawar, Crouse, Kapanipathi, Salaria, Calio, Wen, Seelam, Belgodere, Fonseca, Singhee, Desai, Cox, Puri, and Panda]{DBLP:journals/corr/abs-2405-04324}
Mayank Mishra, Matt Stallone, Gaoyuan Zhang, Yikang Shen, Aditya Prasad, Adriana~Meza Soria, Michele Merler, Parameswaran Selvam, Saptha Surendran, Shivdeep Singh, Manish Sethi, Xuan{-}Hong Dang, Pengyuan Li, Kun{-}Lung Wu, Syed Zawad, Andrew Coleman, Matthew White, Mark Lewis, Raju Pavuluri, Yan Koyfman, Boris Lublinsky, Maximilien de~Bayser, Ibrahim Abdelaziz, Kinjal Basu, Mayank Agarwal, Yi~Zhou, Chris Johnson, Aanchal Goyal, Hima Patel, S.~Yousaf Shah, Petros Zerfos, Heiko Ludwig, Asim Munawar, Maxwell Crouse, Pavan Kapanipathi, Shweta Salaria, Bob Calio, Sophia Wen, Seetharami Seelam, Brian Belgodere, Carlos~A. Fonseca, Amith Singhee, Nirmit Desai, David~D. Cox, Ruchir Puri, and Rameswar Panda.
\newblock Granite code models: {A} family of open foundation models for code intelligence.
\newblock \emph{CoRR}, abs/2405.04324, 2024.
\newblock \doi{10.48550/ARXIV.2405.04324}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2405.04324}.

\bibitem[Mo et~al.(2024)Mo, Mao, Zhao, Qian, Chen, Cheng, Li, Zhu, Dou, and Nie]{mo2024survey}
Fengran Mo, Kelong Mao, Ziliang Zhao, Hongjin Qian, Haonan Chen, Yiruo Cheng, Xiaoxi Li, Yutao Zhu, Zhicheng Dou, and Jian-Yun Nie.
\newblock A survey of conversational search.
\newblock \emph{arXiv preprint arXiv: 2410.15576}, 2024.

\bibitem[Modarressi et~al.(2025)Modarressi, Deilamsalehy, Dernoncourt, Bui, Rossi, Yoon, and Sch{\"u}tze]{modarressi2025nolima}
Ali Modarressi, Hanieh Deilamsalehy, Franck Dernoncourt, Trung Bui, Ryan~A Rossi, Seunghyun Yoon, and Hinrich Sch{\"u}tze.
\newblock Nolima: Long-context evaluation beyond literal matching.
\newblock \emph{arXiv preprint arXiv:2502.05167}, 2025.

\bibitem[Mohtashami and Jaggi(2023)]{mohtashami2023landmark}
Amirkeivan Mohtashami and Martin Jaggi.
\newblock Landmark attention: Random-access infinite context length for transformers.
\newblock \emph{arXiv preprint arXiv:2305.16300}, 2023.

\bibitem[Mu et~al.(2023)Mu, Li, and Goodman]{NEURIPS2023_3d77c6dc}
Jesse Mu, Xiang Li, and Noah Goodman.
\newblock Learning to compress prompts with gist tokens.
\newblock In A.~Oh, T.~Naumann, A.~Globerson, K.~Saenko, M.~Hardt, and S.~Levine, editors, \emph{Advances in Neural Information Processing Systems}, volume~36, pages 19327--19352. Curran Associates, Inc., 2023.
\newblock URL \url{https://proceedings.neurips.cc/paper_files/paper/2023/file/3d77c6dcc7f143aa2154e7f4d5e22d68-Paper-Conference.pdf}.

\bibitem[Muennighoff et~al.(2024)Muennighoff, Su, Wang, Yang, Wei, Yu, Singh, and Kiela]{gritlm}
Niklas Muennighoff, Hongjin Su, Liang Wang, Nan Yang, Furu Wei, Tao Yu, Amanpreet Singh, and Douwe Kiela.
\newblock Generative representational instruction tuning.
\newblock \emph{arXiv preprint arXiv: 2402.09906}, 2024.

\bibitem[Muennighoff et~al.(2025)Muennighoff, Yang, Shi, Li, Fei-Fei, Hajishirzi, Zettlemoyer, Liang, Cand{\`e}s, and Hashimoto]{muennighoff2025s1}
Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang~Lisa Li, Li~Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Cand{\`e}s, and Tatsunori Hashimoto.
\newblock s1: Simple test-time scaling.
\newblock \emph{arXiv preprint arXiv:2501.19393}, 2025.

\bibitem[Munkhdalai et~al.(2024{\natexlab{a}})Munkhdalai, Faruqui, and Gopal]{infinitransformer}
Tsendsuren Munkhdalai, Manaal Faruqui, and Siddharth Gopal.
\newblock Leave no context behind: Efficient infinite context transformers with infini-attention.
\newblock \emph{CoRR}, abs/2404.07143, 2024{\natexlab{a}}.

\bibitem[Munkhdalai et~al.(2024{\natexlab{b}})Munkhdalai, Faruqui, and Gopal]{munkhdalai2024leavecontextbehindefficient}
Tsendsuren Munkhdalai, Manaal Faruqui, and Siddharth Gopal.
\newblock Leave no context behind: Efficient infinite context transformers with infini-attention, 2024{\natexlab{b}}.
\newblock URL \url{https://arxiv.org/abs/2404.07143}.

\bibitem[Nagrani et~al.(2024)Nagrani, Zhang, Mehran, Hornung, Gundavarapu, Jha, Myers, Zhou, Gong, Schmid, Sirotenko, Zhu, and Weyand]{Nagrani-2024-arxiv-Neptune}
Arsha Nagrani, Mingda Zhang, Ramin Mehran, Rachel Hornung, Nitesh~Bharadwaj Gundavarapu, Nilpa Jha, Austin Myers, Xingyi Zhou, Boqing Gong, Cordelia Schmid, Mikhail Sirotenko, Yukun Zhu, and Tobias Weyand.
\newblock Neptune: The long orbit to benchmarking long video understanding.
\newblock \emph{CoRR}, abs/2412.09582, 2024.
\newblock \doi{10.48550/ARXIV.2412.09582}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2412.09582}.

\bibitem[Nallapati et~al.(2016)Nallapati, Zhou, Gulcehre, Xiang, et~al.]{nallapati2016abstractive}
Ramesh Nallapati, Bowen Zhou, Caglar Gulcehre, Bing Xiang, et~al.
\newblock Abstractive text summarization using sequence-to-sequence rnns and beyond.
\newblock \emph{arXiv preprint arXiv:1602.06023}, 2016.

\bibitem[Nanda(2023)]{nanda2023mechanistic}
Neel Nanda.
\newblock Mechanistic interpretability quickstart guide, 2023.
\newblock URL \url{https://www.neelnanda.io/mechanistic-interpretability/quickstart}.
\newblock Accessed: 2025-01-30.

\bibitem[Nguyen et~al.(2024)Nguyen, Nguyen, Pham, Zhang, Deilamsalehy, Mathur, Rossi, Bui, Lai, Dernoncourt, and Nguyen]{vannguyen2024taipanefficientexpressivestate}
Chien~Van Nguyen, Huy~Huu Nguyen, Thang~M. Pham, Ruiyi Zhang, Hanieh Deilamsalehy, Puneet Mathur, Ryan~A. Rossi, Trung Bui, Viet~Dac Lai, Franck Dernoncourt, and Thien~Huu Nguyen.
\newblock Taipan: Efficient and expressive state space language models with selective attention, 2024.
\newblock URL \url{https://arxiv.org/abs/2410.18572}.

\bibitem[Nguyen et~al.(2016)Nguyen, Rosenberg, Song, Gao, Tiwary, Majumder, and Deng]{nguyen2016ms}
Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, and Li~Deng.
\newblock Ms marco: A human-generated machine reading comprehension dataset.
\newblock 2016.

\bibitem[Nie et~al.(2024)Nie, Kong, Dong, Mulvey, Poor, Wen, and Zohren]{nie2024survey}
Yuqi Nie, Yaxuan Kong, Xiaowen Dong, John~M. Mulvey, H.~Vincent Poor, Qingsong Wen, and Stefan Zohren.
\newblock A survey of large language models for financial applications: Progress, prospects and challenges.
\newblock \emph{CoRR}, abs/2406.11903, 2024.
\newblock \doi{10.48550/ARXIV.2406.11903}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2406.11903}.

\bibitem[{NVIDIA Corporation}(2023)]{nvidia_h200}
{NVIDIA Corporation}.
\newblock Nvidia h200, 2023.
\newblock URL \url{https://www.nvidia.com/en-us/data-center/h200/}.

\bibitem[Olsson et~al.(2022)Olsson, Elhage, Nanda, Joseph, DasSarma, Henighan, Mann, Askell, Bai, Chen, Conerly, Drain, Ganguli, Hatfield-Dodds, Hernandez, Johnston, Jones, Kernion, Lovitt, Ndousse, Amodei, Brown, Clark, Kaplan, McCandlish, and Olah]{anthropic_induction_heads}
Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Scott Johnston, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah.
\newblock In-context learning and induction heads.
\newblock \emph{Transformer Circuits Thread}, 2022.
\newblock https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html.

\bibitem[OpenAI(2024{\natexlab{a}})]{openai2024embedding}
OpenAI.
\newblock New embedding models and api updates, 2024{\natexlab{a}}.
\newblock URL \url{https://openai.com/index/new-embedding-models-and-api-updates/}.
\newblock Accessed: 2024-01-25.

\bibitem[OpenAI(2024{\natexlab{b}})]{openai2024memory}
OpenAI.
\newblock Memory and new controls for chatgpt, 2024{\natexlab{b}}.
\newblock URL \url{https://openai.com/index/memory-and-new-controls-for-chatgpt}.
\newblock Accessed: 2024-02-13.

\bibitem[OpenAI(2024{\natexlab{c}})]{openai_o1_2024}
OpenAI.
\newblock Learning to reason with large language models.
\newblock \url{https://openai.com/index/learning-to-reason-with-llms/}, 2024{\natexlab{c}}.

\bibitem[Ouyang et~al.(2022)Ouyang, Wu, Jiang, Almeida, Wainwright, Mishkin, Zhang, Agarwal, Slama, Ray, Schulman, Hilton, Kelton, Miller, Simens, Askell, Welinder, Christiano, Leike, and Lowe]{Ouyang2022TrainingLM}
Long Ouyang, Jeff Wu, Xu~Jiang, Diogo Almeida, Carroll~L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke~E. Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul~Francis Christiano, Jan Leike, and Ryan~J. Lowe.
\newblock Training language models to follow instructions with human feedback.
\newblock \emph{ArXiv}, abs/2203.02155, 2022.
\newblock URL \url{https://api.semanticscholar.org/CorpusID:246426909}.

\bibitem[Pagliardini et~al.(2023)Pagliardini, Paliotta, Jaggi, and Fleuret]{pagliardini2023fast}
Matteo Pagliardini, Daniele Paliotta, Martin Jaggi, and Fran{\c{c}}ois Fleuret.
\newblock Fast attention over long sequences with dynamic sparse flash attention.
\newblock \emph{Advances in Neural Information Processing Systems}, 36:\penalty0 59808--59831, 2023.

\bibitem[Pal et~al.(2023)Pal, Karkhanis, Roberts, Dooley, Sundararajan, and Naidu]{pal2023giraffe}
Arka Pal, Deep Karkhanis, Manley Roberts, Samuel Dooley, Arvind Sundararajan, and Siddartha Naidu.
\newblock Giraffe: Adventures in expanding context lengths in llms.
\newblock \emph{arXiv preprint arXiv:2308.10882}, 2023.

\bibitem[Pan et~al.(2024)Pan, Wu, Jiang, Xia, Luo, Zhang, Lin, R{\"u}hle, Yang, Lin, Zhao, Qiu, and Zhang]{pan-etal-2024-llmlingua}
Zhuoshi Pan, Qianhui Wu, Huiqiang Jiang, Menglin Xia, Xufang Luo, Jue Zhang, Qingwei Lin, Victor R{\"u}hle, Yuqing Yang, Chin-Yew Lin, H.~Vicky Zhao, Lili Qiu, and Dongmei Zhang.
\newblock {LLML}ingua-2: Data distillation for efficient and faithful task-agnostic prompt compression.
\newblock In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, \emph{Findings of the Association for Computational Linguistics: ACL 2024}, pages 963--981, Bangkok, Thailand, August 2024. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2024.findings-acl.57}.
\newblock URL \url{https://aclanthology.org/2024.findings-acl.57/}.

\bibitem[Pang et~al.(2022)Pang, Parrish, Joshi, Nangia, Phang, Chen, Padmakumar, Ma, Thompson, He, et~al.]{pang2022quality}
Richard~Yuanzhe Pang, Alicia Parrish, Nitish Joshi, Nikita Nangia, Jason Phang, Angelica Chen, Vishakh Padmakumar, Johnny Ma, Jana Thompson, He~He, et~al.
\newblock Quality: Question answering with long input texts, yes!
\newblock In \emph{Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies}, pages 5336--5358, 2022.

\bibitem[Papineni et~al.(2002)Papineni, Roukos, Ward, and Zhu]{papineni2002bleu}
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
\newblock Bleu: a method for automatic evaluation of machine translation.
\newblock In \emph{Proceedings of the 40th annual meeting of the Association for Computational Linguistics}, pages 311--318, 2002.

\bibitem[Park and Egger(2024)]{park2024improving}
Daon Park and Bernhard Egger.
\newblock Improving throughput-oriented llm inference with cpu computations.
\newblock In \emph{Proceedings of the 2024 International Conference on Parallel Architectures and Compilation Techniques}, pages 233--245, 2024.

\bibitem[Park et~al.(2023)Park, O'Brien, Cai, Morris, Liang, and Bernstein]{generative-agents}
J.~Park, Joseph~C. O'Brien, Carrie~J. Cai, M.~Morris, Percy Liang, and Michael~S. Bernstein.
\newblock Generative agents: Interactive simulacra of human behavior.
\newblock \emph{ACM Symposium on User Interface Software and Technology}, 2023.
\newblock \doi{10.1145/3586183.3606763}.

\bibitem[Park et~al.(2024)Park, Park, Xiong, Lee, Cho, Oymak, Lee, and Papailiopoulos]{parkcan}
Jongho Park, Jaeseung Park, Zheyang Xiong, Nayoung Lee, Jaewoong Cho, Samet Oymak, Kangwook Lee, and Dimitris Papailiopoulos.
\newblock Can mamba learn how to learn? {A} comparative study on in-context learning tasks.
\newblock In \emph{{ICML}}. OpenReview.net, 2024.

\bibitem[Patel et~al.(2024)Patel, Choukse, Zhang, Shah, Goiri, Maleki, and Bianchini]{patel2024splitwise}
Pratyush Patel, Esha Choukse, Chaojie Zhang, Aashaka Shah, {\'I}{\~n}igo Goiri, Saeed Maleki, and Ricardo Bianchini.
\newblock Splitwise: Efficient generative llm inference using phase splitting.
\newblock In \emph{2024 ACM/IEEE 51st Annual International Symposium on Computer Architecture (ISCA)}, pages 118--132. IEEE, 2024.

\bibitem[Pawar et~al.(2024)Pawar, Tonmoy, Zaman, Jain, Chadha, and Das]{DBLP:journals/corr/abs-2401-07872}
Saurav Pawar, S.~M. Towhidul~Islam Tonmoy, S.~M.~Mehedi Zaman, Vinija Jain, Aman Chadha, and Amitava Das.
\newblock The what, why, and how of context length extension techniques in large language models - {A} detailed survey.
\newblock \emph{CoRR}, abs/2401.07872, 2024.
\newblock \doi{10.48550/ARXIV.2401.07872}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2401.07872}.

\bibitem[Peng et~al.(2023{\natexlab{a}})Peng, Alcaide, Anthony, Albalak, Arcadinho, Biderman, Cao, Cheng, Chung, Derczynski, Du, Grella, GV, He, Hou, Kazienko, Kocon, Kong, Koptyra, Lau, Lin, Mantri, Mom, Saito, Song, Tang, Wind, Wozniak, Zhang, Zhou, Zhu, and Zhu]{peng2023rwkvreinventingrnnstransformer}
Bo~Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Stella Biderman, Huanqi Cao, Xin Cheng, Michael Chung, Leon Derczynski, Xingjian Du, Matteo Grella, Kranthi~Kiran GV, Xuzheng He, Haowen Hou, Przemyslaw Kazienko, Jan Kocon, Jiaming Kong, Bartlomiej Koptyra, Hayden Lau, Jiaju Lin, Krishna Sri~Ipsit Mantri, Ferdinand Mom, Atsushi Saito, Guangyu Song, Xiangru Tang, Johan~S. Wind, Stanislaw Wozniak, Zhenyuan Zhang, Qinghua Zhou, Jian Zhu, and Rui{-}Jie Zhu.
\newblock {RWKV:} reinventing rnns for the transformer era.
\newblock In \emph{{EMNLP} (Findings)}, pages 14048--14077. Association for Computational Linguistics, 2023{\natexlab{a}}.

\bibitem[Peng et~al.(2024)Peng, Goldstein, Anthony, Albalak, Alcaide, Biderman, Cheah, Du, Ferdinan, Hou, Kazienko, GV, Kocoń, Koptyra, Krishna, Jr., Lin, Muennighoff, Obeid, Saito, Song, Tu, Wirawan, Woźniak, Zhang, Zhao, Zhao, Zhou, Zhu, and Zhu]{peng2024eaglefinchrwkvmatrixvalued}
Bo~Peng, Daniel Goldstein, Quentin Anthony, Alon Albalak, Eric Alcaide, Stella Biderman, Eugene Cheah, Xingjian Du, Teddy Ferdinan, Haowen Hou, Przemysław Kazienko, Kranthi~Kiran GV, Jan Kocoń, Bartłomiej Koptyra, Satyapriya Krishna, Ronald~McClelland Jr., Jiaju Lin, Niklas Muennighoff, Fares Obeid, Atsushi Saito, Guangyu Song, Haoqin Tu, Cahya Wirawan, Stanisław Woźniak, Ruichong Zhang, Bingchen Zhao, Qihang Zhao, Peng Zhou, Jian Zhu, and Rui-Jie Zhu.
\newblock Eagle and finch: Rwkv with matrix-valued states and dynamic recurrence, 2024.
\newblock URL \url{https://arxiv.org/abs/2404.05892}.

\bibitem[Peng and Quesnelle(2023)]{ntk}
Bowen Peng and Jeffrey Quesnelle.
\newblock Ntk-aware scaled rope allows llama models to have extended (8k+) context size without any fine-tuning and minimal perplexity degradation.
\newblock \url{https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_scaled_rope_allows_llama_models_to_have}, 2023.

\bibitem[Peng et~al.(2023{\natexlab{b}})Peng, Quesnelle, Fan, and Shippole]{peng2023yarn}
Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole.
\newblock Yarn: Efficient context window extension of large language models.
\newblock \emph{arXiv preprint arXiv:2309.00071}, 2023{\natexlab{b}}.

\bibitem[Peng et~al.(2023{\natexlab{c}})Peng, Wu, Wei, Zhao, Yang, Liu, Xiong, Yang, Ni, Hu, et~al.]{peng2023fp8}
Houwen Peng, Kan Wu, Yixuan Wei, Guoshuai Zhao, Yuxiang Yang, Ze~Liu, Yifan Xiong, Ziyue Yang, Bolin Ni, Jingcheng Hu, et~al.
\newblock Fp8-lm: Training fp8 large language models.
\newblock \emph{arXiv preprint arXiv:2310.18313}, 2023{\natexlab{c}}.

\bibitem[Pham et~al.(2024)Pham, Sun, and Iyyer]{pham2024suri}
Chau~Minh Pham, Simeng Sun, and Mohit Iyyer.
\newblock Suri: Multi-constraint instruction following for long-form text generation.
\newblock \emph{arXiv preprint arXiv:2406.19371}, 2024.

\bibitem[Phan et~al.(2024)Phan, Acharya, Meyur, Chaturvedi, Sharma, Parker, Nally, Jannesari, Pazdernik, Halappanavar, et~al.]{phan2024examining}
Hung Phan, Anurag Acharya, Rounak Meyur, Sarthak Chaturvedi, Shivam Sharma, Mike Parker, Dan Nally, Ali Jannesari, Karl Pazdernik, Mahantesh Halappanavar, et~al.
\newblock Examining long-context large language models for environmental review document comprehension.
\newblock \emph{arXiv preprint arXiv:2407.07321}, 2024.

\bibitem[Ping et~al.(2025)Ping, Zeng, Meng, Wang, Zhou, and Zhang]{ping2025longdpounlockbetterlongform}
Bowen Ping, Jiali Zeng, Fandong Meng, Shuo Wang, Jie Zhou, and Shanghang Zhang.
\newblock Longdpo: Unlock better long-form generation abilities for llms via critique-augmented stepwise information, 2025.
\newblock URL \url{https://arxiv.org/abs/2502.02095}.

\bibitem[Pouransari et~al.(2024)Pouransari, Li, Chang, Vasu, Koc, Shankar, and Tuzel]{pouransari2024dataset}
Hadi Pouransari, Chun-Liang Li, Jen-Hao~Rick Chang, Pavan Kumar~Anasosalu Vasu, Cem Koc, Vaishaal Shankar, and Oncel Tuzel.
\newblock Dataset decomposition: Faster llm training with variable sequence length curriculum.
\newblock \emph{arXiv preprint arXiv:2405.13226}, 2024.

\bibitem[Pramanick et~al.(2024)Pramanick, Chellappa, and Venugopalan]{pramanick-2024-arxiv-spiqa}
Shraman Pramanick, Rama Chellappa, and Subhashini Venugopalan.
\newblock {SPIQA:} {A} dataset for multimodal question answering on scientific papers.
\newblock \emph{CoRR}, abs/2407.09413, 2024.
\newblock \doi{10.48550/ARXIV.2407.09413}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2407.09413}.

\bibitem[Press et~al.(2021)Press, Smith, and Lewis]{press2021train}
Ofir Press, Noah~A Smith, and Mike Lewis.
\newblock Train short, test long: Attention with linear biases enables input length extrapolation.
\newblock \emph{arXiv preprint arXiv:2108.12409}, 2021.

\bibitem[Qi et~al.(2023)Qi, Wan, Huang, and Lin]{qi2023zero}
Penghui Qi, Xinyi Wan, Guangxing Huang, and Min Lin.
\newblock Zero bubble pipeline parallelism.
\newblock \emph{arXiv preprint arXiv:2401.10241}, 2023.

\bibitem[Qi et~al.(2024)Qi, Xu, Guo, Wang, Zhang, and Xu]{qi2024long2rag}
Zehan Qi, Rongwu Xu, Zhijiang Guo, Cunxiang Wang, Hao Zhang, and Wei Xu.
\newblock Long2rag: Evaluating long-context \& long-form retrieval-augmented generation with key point recall.
\newblock In \emph{Findings of the Association for Computational Linguistics: EMNLP 2024}, pages 4852--4872, 2024.

\bibitem[Qin et~al.(2024{\natexlab{a}})Qin, Li, He, Zhang, Wu, Zheng, and Xu]{qin2024mooncake}
Ruoyu Qin, Zheming Li, Weiran He, Mingxing Zhang, Yongwei Wu, Weimin Zheng, and Xinran Xu.
\newblock Mooncake: A kvcache-centric disaggregated architecture for llm serving.
\newblock \emph{arXiv preprint arXiv:2407.00079}, 2024{\natexlab{a}}.

\bibitem[Qin et~al.(2024{\natexlab{b}})Qin, Li, Zou, Liu, Xia, Huang, Ye, Yuan, Liu, Li, et~al.]{qin2024o1}
Yiwei Qin, Xuefeng Li, Haoyang Zou, Yixiu Liu, Shijie Xia, Zhen Huang, Yixin Ye, Weizhe Yuan, Hector Liu, Yuanzhi Li, et~al.
\newblock O1 replication journey: A strategic progress report--part 1.
\newblock \emph{arXiv preprint arXiv:2410.18982}, 2024{\natexlab{b}}.

\bibitem[Qin et~al.(2022)Qin, Sun, Deng, Li, Wei, Lv, Yan, Kong, and Zhong]{qin2022cosformerrethinkingsoftmaxattention}
Zhen Qin, Weixuan Sun, Hui Deng, Dongxu Li, Yunshen Wei, Baohong Lv, Junjie Yan, Lingpeng Kong, and Yiran Zhong.
\newblock cosformer: Rethinking softmax in attention.
\newblock In \emph{{ICLR}}. OpenReview.net, 2022.

\bibitem[Qin et~al.(2024{\natexlab{c}})Qin, Sun, Li, Shen, Sun, and Zhong]{qin2024lightningattention2freelunch}
Zhen Qin, Weigao Sun, Dong Li, Xuyang Shen, Weixuan Sun, and Yiran Zhong.
\newblock Lightning attention-2: A free lunch for handling unlimited sequence lengths in large language models, 2024{\natexlab{c}}.
\newblock URL \url{https://arxiv.org/abs/2401.04658}.

\bibitem[Qin et~al.(2024{\natexlab{d}})Qin, Sun, Li, Shen, Sun, and Zhong]{qin2024variouslengthsconstantspeed}
Zhen Qin, Weigao Sun, Dong Li, Xuyang Shen, Weixuan Sun, and Yiran Zhong.
\newblock Various lengths, constant speed: Efficient language modeling with lightning attention.
\newblock In \emph{{ICML}}. OpenReview.net, 2024{\natexlab{d}}.

\bibitem[Qiu et~al.(2024)Qiu, Li, Huang, Jiao, Zhong, and King]{qiu2024clongeval}
Zexuan Qiu, Jingjing Li, Shijue Huang, Xiaoqi Jiao, Wanjun Zhong, and Irwin King.
\newblock Clongeval: A chinese benchmark for evaluating long-context large language models.
\newblock \emph{arXiv preprint arXiv:2403.03514}, 2024.

\bibitem[Quan et~al.(2024)Quan, Tang, Yu, Yang, Liu, Gao, Tu, Zhang, Zhou, and Lin]{quan2024language}
Shanghaoran Quan, Tianyi Tang, Bowen Yu, An~Yang, Dayiheng Liu, Bofei Gao, Jianhong Tu, Yichang Zhang, Jingren Zhou, and Junyang Lin.
\newblock Language models can self-lengthen to generate long texts.
\newblock \emph{arXiv preprint arXiv:2410.23933}, 2024.

\bibitem[Que et~al.(2024)Que, Duan, He, Mou, Zhou, Liu, Rong, Wang, Yang, Zhang, et~al.]{que2024hellobench}
Haoran Que, Feiyu Duan, Liqun He, Yutao Mou, Wangchunshu Zhou, Jiaheng Liu, Wenge Rong, Zekun~Moore Wang, Jian Yang, Ge~Zhang, et~al.
\newblock Hellobench: Evaluating long text generation capabilities of large language models.
\newblock \emph{arXiv preprint arXiv:2409.16191}, 2024.

\bibitem[Radford(2018)]{radford2018improving}
Alec Radford.
\newblock Improving language understanding by generative pre-training.
\newblock 2018.

\bibitem[Rae et~al.(2020)Rae, Potapenko, Jayakumar, Hillier, and Lillicrap]{CompressiveTransformers}
Jack~W. Rae, Anna Potapenko, Siddhant~M. Jayakumar, Chloe Hillier, and Timothy~P. Lillicrap.
\newblock Compressive transformers for long-range sequence modelling.
\newblock In \emph{{ICLR}}. OpenReview.net, 2020.

\bibitem[Rae et~al.(2021)Rae, Borgeaud, Cai, Millican, Hoffmann, Song, Aslanides, Henderson, Ring, Young, Rutherford, Hennigan, Menick, Cassirer, Powell, van~den Driessche, Hendricks, Rauh, Huang, Glaese, Welbl, Dathathri, Huang, Uesato, Mellor, Higgins, Creswell, McAleese, Wu, Elsen, Jayakumar, Buchatskaya, Budden, Sutherland, Simonyan, Paganini, Sifre, Martens, Li, Kuncoro, Nematzadeh, Gribovskaya, Donato, Lazaridou, Mensch, Lespiau, Tsimpoukelli, Grigorev, Fritz, Sottiaux, Pajarskas, Pohlen, Gong, Toyama, de~Masson~d'Autume, Li, Terzi, Mikulik, Babuschkin, Clark, de~Las~Casas, Guy, Jones, Bradbury, Johnson, Hechtman, Weidinger, Gabriel, Isaac, Lockhart, Osindero, Rimell, Dyer, Vinyals, Ayoub, Stanway, Bennett, Hassabis, Kavukcuoglu, and Irving]{gopher}
Jack~W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, H.~Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hennigan, Jacob Menick, Albin Cassirer, Richard Powell, George van~den Driessche, Lisa~Anne Hendricks, Maribeth Rauh, Po{-}Sen Huang, Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron Huang, Jonathan Uesato, John Mellor, Irina Higgins, Antonia Creswell, Nat McAleese, Amy Wu, Erich Elsen, Siddhant~M. Jayakumar, Elena Buchatskaya, David Budden, Esme Sutherland, Karen Simonyan, Michela Paganini, Laurent Sifre, Lena Martens, Xiang~Lorraine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya, Domenic Donato, Angeliki Lazaridou, Arthur Mensch, Jean{-}Baptiste Lespiau, Maria Tsimpoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sottiaux, Mantas Pajarskas, Toby Pohlen, Zhitao Gong, Daniel Toyama, Cyprien de~Masson~d'Autume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark, Diego de~Las~Casas, Aurelia Guy,
  Chris Jones, James Bradbury, Matthew~J. Johnson, Blake~A. Hechtman, Laura Weidinger, Iason Gabriel, William Isaac, Edward Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol Vinyals, Kareem Ayoub, Jeff Stanway, Lorrayne Bennett, Demis Hassabis, Koray Kavukcuoglu, and Geoffrey Irving.
\newblock Scaling language models: Methods, analysis {\&} insights from training gopher.
\newblock \emph{CoRR}, abs/2112.11446, 2021.
\newblock URL \url{https://arxiv.org/abs/2112.11446}.

\bibitem[Rafailov et~al.(2023)Rafailov, Sharma, Mitchell, Ermon, Manning, and Finn]{Rafailov2023DirectPO}
Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher~D. Manning, and Chelsea Finn.
\newblock Direct preference optimization: Your language model is secretly a reward model.
\newblock \emph{ArXiv}, abs/2305.18290, 2023.
\newblock URL \url{https://api.semanticscholar.org/CorpusID:258959321}.

\bibitem[Raffel et~al.(2020)Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li, and Liu]{raffel2020exploring}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter~J Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text transformer.
\newblock \emph{Journal of machine learning research}, 21\penalty0 (140):\penalty0 1--67, 2020.

\bibitem[Rasley et~al.(2020)Rasley, Rajbhandari, Ruwase, and He]{rasley2020deepspeed}
Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He.
\newblock Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters.
\newblock In \emph{Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining}, pages 3505--3506, 2020.

\bibitem[Reddy et~al.(2024)Reddy, Koncel{-}Kedziorski, Lai, Krumdick, Lovering, and Tanner]{reddy2024docfinqa}
Varshini Reddy, Rik Koncel{-}Kedziorski, Viet~Dac Lai, Michael Krumdick, Charles Lovering, and Chris Tanner.
\newblock Docfinqa: {A} long-context financial reasoning dataset.
\newblock In Lun{-}Wei Ku, Andre Martins, and Vivek Srikumar, editors, \emph{Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics, {ACL} 2024 - Short Papers, Bangkok, Thailand, August 11-16, 2024}, pages 445--458. Association for Computational Linguistics, 2024.
\newblock URL \url{https://aclanthology.org/2024.acl-short.42}.

\bibitem[Rehg(2024)]{rehg2024kv}
Isaac Rehg.
\newblock Kv-compress: Paged kv-cache compression with variable compression rates per attention head.
\newblock \emph{arXiv preprint arXiv:2410.00161}, 2024.

\bibitem[Ren et~al.(2024{\natexlab{a}})Ren, Liu, Lu, Shen, Liang, and Chen]{ren2024samba}
Liliang Ren, Yang Liu, Yadong Lu, Yelong Shen, Chen Liang, and Weizhu Chen.
\newblock Samba: Simple hybrid state space models for efficient unlimited context language modeling.
\newblock \emph{arXiv preprint arXiv:2406.07522}, 2024{\natexlab{a}}.

\bibitem[Ren et~al.(2024{\natexlab{b}})Ren, Liu, Lu, Shen, Liang, and Chen]{ren2024sambasimplehybridstate}
Liliang Ren, Yang Liu, Yadong Lu, Yelong Shen, Chen Liang, and Weizhu Chen.
\newblock Samba: Simple hybrid state space models for efficient unlimited context language modeling, 2024{\natexlab{b}}.
\newblock URL \url{https://arxiv.org/abs/2406.07522}.

\bibitem[Roberts et~al.(2024)Roberts, Han, and Albanie]{roberts2024needle}
Jonathan Roberts, Kai Han, and Samuel Albanie.
\newblock Needle threading: Can llms follow threads through near-million-scale haystacks?
\newblock \emph{arXiv preprint arXiv:2411.05000}, 2024.

\bibitem[Rodkin et~al.(2024)Rodkin, Kuratov, Bulatov, and Burtsev]{armt}
Ivan Rodkin, Yuri Kuratov, Aydar Bulatov, and Mikhail Burtsev.
\newblock Associative recurrent memory transformer.
\newblock \emph{CoRR}, abs/2407.04841, 2024.

\bibitem[Rosenthal et~al.(2024)Rosenthal, Sil, Florian, and Roukos]{rosenthal2024clapnq}
Sara Rosenthal, Avirup Sil, Radu Florian, and Salim Roukos.
\newblock Clapnq: Cohesive long-form answers from passages in natural questions for rag systems.
\newblock \emph{arXiv preprint arXiv:2404.02103}, 2024.

\bibitem[Ruan et~al.(2024)Ruan, Wang, and Wan]{ruan2024defining}
Jie Ruan, Wenqing Wang, and Xiaojun Wan.
\newblock Defining and detecting vulnerability in human evaluation guidelines: A preliminary study towards reliable nlg evaluation.
\newblock \emph{arXiv preprint arXiv:2406.07935}, 2024.

\bibitem[Ruoss et~al.(2023)Ruoss, Del{\'e}tang, Genewein, Grau-Moya, Csord{\'a}s, Bennani, Legg, and Veness]{ruoss2023randomized}
Anian Ruoss, Gr{\'e}goire Del{\'e}tang, Tim Genewein, Jordi Grau-Moya, R{\'o}bert Csord{\'a}s, Mehdi Bennani, Shane Legg, and Joel Veness.
\newblock Randomized positional encodings boost length generalization of transformers.
\newblock \emph{arXiv preprint arXiv:2305.16843}, 2023.

\bibitem[Ruoss et~al.(2024)Ruoss, Pardo, Chan, Li, Mnih, and Genewein]{Ruoss-2024-arxiv-lmact}
Anian Ruoss, Fabio Pardo, Harris Chan, Bonnie Li, Volodymyr Mnih, and Tim Genewein.
\newblock Lmact: {A} benchmark for in-context imitation learning with long multimodal demonstrations.
\newblock \emph{CoRR}, abs/2412.01441, 2024.
\newblock \doi{10.48550/ARXIV.2412.01441}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2412.01441}.

\bibitem[Rupesh~Bansal(2023)]{deepsearch}
Shiwangi~Shah Rupesh~Bansal.
\newblock Deepsearch: Semantic search on multimedia sources like audio, video and images.
\newblock \url{https://github.com/deepsearch-ai/deepsearch}, 2023.
\newblock URL \url{https://github.com/deepsearch-ai/deepsearch}.

\bibitem[Ryoo et~al.(2023)Ryoo, Gopalakrishnan, Kahatapitiya, Xiao, Rao, Stone, Lu, Ibarz, and Arnab]{token-turing-machine}
Michael~S. Ryoo, Keerthana Gopalakrishnan, Kumara Kahatapitiya, Ted Xiao, Kanishka Rao, Austin Stone, Yao Lu, Julian Ibarz, and Anurag Arnab.
\newblock Token turing machines.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, pages 19070--19081, June 2023.

\bibitem[Saad{-}Falcon et~al.(2024)Saad{-}Falcon, Fu, Arora, Guha, and R{\'{e}}]{saad2024benchmarking}
Jon Saad{-}Falcon, Daniel~Y. Fu, Simran Arora, Neel Guha, and Christopher R{\'{e}}.
\newblock Benchmarking and building long-context retrieval models with loco and {M2-BERT}.
\newblock In \emph{Forty-first International Conference on Machine Learning, {ICML} 2024, Vienna, Austria, July 21-27, 2024}. OpenReview.net, 2024.
\newblock URL \url{https://openreview.net/forum?id=HkCRgoGtt6}.

\bibitem[Saeidi et~al.(2024)Saeidi, Verma, RRV, and Baral]{saeidi2024triple}
Amir Saeidi, Shivanshu Verma, Aswin RRV, and Chitta Baral.
\newblock Triple preference optimization: Achieving better alignment with less data in a single step optimization, 2024.

\bibitem[Scao et~al.(2022)Scao, Fan, Akiki, Pavlick, Ili'c, Hesslow, Castagn'e, Luccioni, Yvon, Gall{\'e}, Tow, Rush, Biderman, Webson, Ammanamanchi, Wang, Sagot, Muennighoff, del Moral, Ruwase, Bawden, Bekman, McMillan-Major, Beltagy, Nguyen, Saulnier, Tan, Suarez, Sanh, Laurenccon, Jernite, Launay, Mitchell, Raffel, Gokaslan, Simhi, Etxabe, Aji, Alfassy, Rogers, Nitzav, Xu, Mou, Emezue, Klamm, Leong, van Strien, Adelani, Radev, Ponferrada, Levkovizh, Kim, Natan, Toni, Dupont, Kruszewski, Pistilli, ElSahar, Benyamina, Tran, Yu, Abdulmumin, Johnson, Gonzalez-Dios, de~la Rosa, Chim, Dodge, Zhu, Chang, Frohberg, Tobing, Bhattacharjee, Almubarak, Chen, Lo, von Werra, Weber, Phan, Allal, Tanguy, Dey, Mu{\~n}oz, Masoud, Grandury, vSavsko, Huang, Coavoux, Singh, Jiang, Vu, mad A.~Jauhar, Ghaleb, Subramani, Kassner, Khamis, Nguyen, Espejel, de~Gibert, Villegas, Henderson, Colombo, Amuok, Lhoest, Harliman, Bommasani, L'opez, Ribeiro, Osei, Pyysalo, Nagel, Bose, Muhammad, Sharma, Longpre, Nikpoor, Silberberg, Pai,
  Zink, Torrent, Schick, Thrush, Danchev, Nikoulina, Laippala, Lepercq, Prabhu, Alyafeai, Talat, Raja, Heinzerling, Si, Salesky, Mielke, Lee, Sharma, Santilli, Chaffin, Stiegler, Datta, Szczechla, Chhablani, Wang, Pandey, Strobelt, Fries, Rozen, Gao, Sutawika, Bari, Al-Shaibani, Manica, Nayak, Teehan, Albanie, Shen, Ben-David, Bach, Kim, Bers, F{\'e}vry, Neeraj, Thakker, Raunak, Tang, Yong, Sun, Brody, Uri, Tojarieh, Roberts, Chung, Tae, Phang, Press, Li, Narayanan, Bourfoune, Casper, Rasley, Ryabinin, Mishra, Zhang, Shoeybi, Peyrounette, Patry, Tazi, Sanseviero, von Platen, Cornette, Lavall'ee, Lacroix, Rajbhandari, Gandhi, Smith, Requena, Patil, Dettmers, Baruwa, Singh, Cheveleva, Ligozat, Subramonian, N'ev'eol, Lovering, Garrette, Tunuguntla, Reiter, Taktasheva, Voloshina, Bogdanov, Winata, Schoelkopf, Kalo, Novikova, Forde, Tang, Kasai, Kawamura, Hazan, Carpuat, Clinciu, Kim, Cheng, Serikov, Antverg, van~der Wal, Zhang, Zhang, Gehrmann, Mirkin, Pais, Shavrina, Scialom, Yun, Limisiewicz, Rieser, Protasov,
  Mikhailov, Pruksachatkun, Belinkov, Bamberger, Kasner, Kasner, Pestana, Feizpour, Khan, Faranak, Santos, Hevia, Unldreaj, Aghagol, Abdollahi, Tammour, HajiHosseini, Behroozi, Ajibade, Saxena, Ferrandis, Contractor, Lansky, David, Kiela, Nguyen, Tan, Baylor, Ozoani, Mirza, Ononiwu, Rezanejad, Jones, Bhattacharya, Solaiman, Sedenko, Nejadgholi, Passmore, Seltzer, Sanz, Fort, Dutra, Samagaio, Elbadri, Mieskes, Gerchick, Akinlolu, McKenna, Qiu, Ghauri, Burynok, Abrar, Rajani, Elkott, Fahmy, Samuel, An, Kromann, Hao, Alizadeh, Shubber, Wang, Roy, Viguier, Le, Oyebade, Le, Yang, Nguyen, Kashyap, Palasciano, Callahan, Shukla, Miranda-Escalada, Singh, Beilharz, Wang, de~Brito, Zhou, Jain, Xu, Fourrier, Perin'an, Molano, Yu, Manjavacas, Barth, Fuhrimann, Altay, Bayrak, Burns, Vrabec, Bello, Dash, Kang, Giorgi, Golde, Posada, Sivaraman, Bulchandani, Liu, Shinzato, de~Bykhovetz, Takeuchi, P{\`a}mies, Castillo, Nezhurina, Sanger, Samwald, Cullan, Weinberg, Wolf, Mihaljcic, Liu, Freidank, Kang, Seelam, Dahlberg, Broad,
  Muellner, Fung, Haller, Haller, Eisenberg, Martin, Canalli, Su, Su, Cahyawijaya, Garda, Deshmukh, Mishra, Kiblawi, Ott, Sang-aroonsiri, Kumar, Schweter, Bharati, Laud, Gigant, Kainuma, Kusa, Labrak, Bajaj, Venkatraman, Xu, Xu, Xu, Tan, Xie, Ye, Bras, Belkada, and Wolf]{Scao2022BLOOMA1}
Teven~Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili'c, Daniel Hesslow, Roman Castagn'e, Alexandra~Sasha Luccioni, François Yvon, Matthias Gall{\'e}, Jonathan Tow, Alexander~M. Rush, Stella Biderman, Albert Webson, Pawan~Sasanka Ammanamanchi, Thomas Wang, Beno{\^i}t Sagot, Niklas Muennighoff, Albert~Villanova del Moral, Olatunji Ruwase, Rachel Bawden, Stas Bekman, Angelina McMillan-Major, Iz~Beltagy, Huu Nguyen, Lucile Saulnier, Samson Tan, Pedro~Ortiz Suarez, Victor Sanh, Hugo Laurenccon, Yacine Jernite, Julien Launay, Margaret Mitchell, Colin Raffel, Aaron Gokaslan, Adi Simhi, Aitor~Soroa Etxabe, Alham~Fikri Aji, Amit Alfassy, Anna Rogers, Ariel~Kreisberg Nitzav, Canwen Xu, Chenghao Mou, Chris~C. Emezue, Christopher Klamm, Colin Leong, Daniel~Alexander van Strien, David~Ifeoluwa Adelani, Dragomir~R. Radev, Eduardo~Gonz'alez Ponferrada, Efrat Levkovizh, Ethan Kim, Eyal Natan, Francesco~De Toni, G{\'e}rard Dupont, Germ{\'a}n Kruszewski, Giada Pistilli, Hady ElSahar, Hamza Benyamina,
  Hieu~Trung Tran, Ian Yu, Idris Abdulmumin, Isaac Johnson, Itziar Gonzalez-Dios, Javier de~la Rosa, Jenny Chim, Jesse Dodge, Jian Zhu, Jonathan Chang, Jorg Frohberg, Josephine Tobing, Joydeep Bhattacharjee, Khalid Almubarak, Kimbo Chen, Kyle Lo, Leandro von Werra, Leon Weber, Long Phan, Loubna~Ben Allal, Ludovic Tanguy, Manan Dey, Manuel~Romero Mu{\~n}oz, Maraim Masoud, Mar{\'i}a Grandury, Mario vSavsko, Max Huang, Maximin Coavoux, Mayank Singh, Mike Tian-Jian Jiang, Minh~Chien Vu, Moham mad A.~Jauhar, Mustafa Ghaleb, Nishant Subramani, Nora Kassner, Nurulaqilla Khamis, Olivier Nguyen, Omar Espejel, Ona de~Gibert, Paulo Villegas, Peter Henderson, Pierre Colombo, Priscilla Amuok, Quentin Lhoest, Rheza Harliman, Rishi Bommasani, Roberto L'opez, Rui Ribeiro, Salomey Osei, Sampo Pyysalo, Sebastian Nagel, Shamik Bose, Shamsuddeen~Hassan Muhammad, Shanya~Sharma Sharma, S.~Longpre, Somaieh Nikpoor, S.~Silberberg, Suhas Pai, Sydney Zink, Tiago~Timponi Torrent, Timo Schick, Tristan Thrush, Valentin Danchev, Vassilina
  Nikoulina, Veronika Laippala, Violette Lepercq, Vrinda Prabhu, Zaid Alyafeai, Zeerak Talat, Arun Raja, Benjamin Heinzerling, Chenglei Si, Elizabeth Salesky, Sabrina~J. Mielke, Wilson~Y. Lee, Abheesht Sharma, Andrea Santilli, Antoine Chaffin, Arnaud Stiegler, Debajyoti Datta, Eliza Szczechla, Gunjan Chhablani, Han Wang, Harshit Pandey, Hendrik Strobelt, Jason~Alan Fries, Jos Rozen, Leo Gao, Lintang Sutawika, M~Saiful Bari, Maged~S. Al-Shaibani, Matteo Manica, Nihal~V. Nayak, Ryan Teehan, Samuel Albanie, Sheng Shen, Srulik Ben-David, Stephen~H. Bach, Taewoon Kim, Tali Bers, Thibault F{\'e}vry, Trishala Neeraj, Urmish Thakker, Vikas Raunak, Xiang Tang, Zheng-Xin Yong, Zhiqing Sun, Shaked Brody, Y~Uri, Hadar Tojarieh, Adam Roberts, Hyung~Won Chung, Jaesung Tae, Jason Phang, Ofir Press, Conglong Li, Deepak Narayanan, Hatim Bourfoune, Jared Casper, Jeff Rasley, Max Ryabinin, Mayank Mishra, Minjia Zhang, Mohammad Shoeybi, Myriam Peyrounette, Nicolas Patry, Nouamane Tazi, Omar Sanseviero, Patrick von Platen, Pierre
  Cornette, Pierre~Franccois Lavall'ee, R{\'e}mi Lacroix, Samyam Rajbhandari, Sanchit Gandhi, Shaden Smith, St{\'e}phane Requena, Suraj Patil, Tim Dettmers, Ahmed Baruwa, Amanpreet Singh, Anastasia Cheveleva, Anne-Laure Ligozat, Arjun Subramonian, Aur'elie N'ev'eol, Charles Lovering, Daniel~H Garrette, Deepak~R. Tunuguntla, Ehud Reiter, Ekaterina Taktasheva, Ekaterina Voloshina, Eli Bogdanov, Genta~Indra Winata, Hailey Schoelkopf, Jan-Christoph Kalo, Jekaterina Novikova, Jessica~Zosa Forde, Xiangru Tang, Jungo Kasai, Ken Kawamura, Liam Hazan, Marine Carpuat, Miruna Clinciu, Najoung Kim, Newton Cheng, Oleg Serikov, Omer Antverg, Oskar van~der Wal, Rui Zhang, Ruochen Zhang, Sebastian Gehrmann, Shachar Mirkin, S.~Osher Pais, Tatiana Shavrina, Thomas Scialom, Tian Yun, Tomasz Limisiewicz, Verena Rieser, Vitaly Protasov, Vladislav Mikhailov, Yada Pruksachatkun, Yonatan Belinkov, Zachary Bamberger, Zdenˇek Kasner, Zdeněk Kasner, Amanda Pestana, Amir Feizpour, Ammar Khan, Amy Faranak, Ananda Santa~Rosa Santos,
  Anthony Hevia, Antigona Unldreaj, Arash Aghagol, Arezoo Abdollahi, Aycha Tammour, Azadeh HajiHosseini, Bahareh Behroozi, Benjamin~Ayoade Ajibade, Bharat~Kumar Saxena, Carlos~Mu{\~n}oz Ferrandis, Danish Contractor, David~M. Lansky, Davis David, Douwe Kiela, Duong~Anh Nguyen, Edward Tan, Emi Baylor, Ezinwanne Ozoani, Fatim~Tahirah Mirza, Frankline Ononiwu, Habib Rezanejad, H.A. Jones, Indrani Bhattacharya, Irene Solaiman, Irina Sedenko, Isar Nejadgholi, Jan Passmore, Joshua Seltzer, Julio~Bonis Sanz, Karen Fort, L{\'i}via Dutra, Mairon Samagaio, Maraim Elbadri, Margot Mieskes, Marissa Gerchick, Martha Akinlolu, Michael McKenna, Mike Qiu, Muhammed Ghauri, Mykola Burynok, Nafis Abrar, Nazneen Rajani, Nour Elkott, Nourhan Fahmy, Olanrewaju Samuel, Ran An, R.~P. Kromann, Ryan Hao, Samira Alizadeh, Sarmad Shubber, Silas~L. Wang, Sourav Roy, Sylvain Viguier, Thanh-Cong Le, Tobi Oyebade, Trieu Nguyen~Hai Le, Yoyo Yang, Zach Nguyen, Abhinav~Ramesh Kashyap, Alfredo Palasciano, Alison Callahan, Anima Shukla, Antonio
  Miranda-Escalada, Ayush~Kumar Singh, Benjamin Beilharz, Bo~Wang, Caio Matheus~Fonseca de~Brito, Chenxi Zhou, Chirag Jain, Chuxin Xu, Cl{\'e}mentine Fourrier, Daniel~Le'on Perin'an, Daniel Molano, Dian Yu, Enrique Manjavacas, Fabio Barth, Florian Fuhrimann, Gabriel Altay, Giyaseddin Bayrak, Gully Burns, Helena~U. Vrabec, Iman~I.B. Bello, Isha Dash, Ji~Soo Kang, John Giorgi, Jonas Golde, Jos{\'e}~D. Posada, Karthi Sivaraman, Lokesh Bulchandani, Lu~Liu, Luisa Shinzato, Madeleine~Hahn de~Bykhovetz, Maiko Takeuchi, Marc P{\`a}mies, Mar{\'i}a~Andrea Castillo, Marianna Nezhurina, Mario Sanger, Matthias Samwald, Michael Cullan, Michael Weinberg, M~Wolf, Mina Mihaljcic, Minna Liu, Moritz Freidank, Myungsun Kang, Natasha Seelam, Nathan Dahlberg, Nicholas~Michio Broad, Nikolaus Muellner, Pascale Fung, Patricia Haller, Patrick Haller, Renata Eisenberg, Robert Martin, Rodrigo Canalli, Rosaline Su, Ruisi Su, Samuel Cahyawijaya, Samuele Garda, Shlok~S Deshmukh, Shubhanshu Mishra, Sid Kiblawi, Simon Ott, Sinee
  Sang-aroonsiri, Srishti Kumar, Stefan Schweter, Sushil~Pratap Bharati, Tanmay Laud, Th{\'e}o Gigant, Tomoya Kainuma, Wojciech Kusa, Yanis Labrak, Yashasvi Bajaj, Y.~Venkatraman, Yifan Xu, Ying Xu, Yu~Xu, Zhee~Xao Tan, Zhongli Xie, Zifan Ye, Mathilde Bras, Younes Belkada, and Thomas Wolf.
\newblock Bloom: A 176b-parameter open-access multilingual language model.
\newblock \emph{ArXiv}, abs/2211.05100, 2022.

\bibitem[Schick et~al.(2023)Schick, Dwivedi-Yu, Dessì, Raileanu, Lomeli, Zettlemoyer, Cancedda, and Scialom]{toolformer}
Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, M.~Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom.
\newblock Toolformer: Language models can teach themselves to use tools.
\newblock \emph{Neural Information Processing Systems}, 2023.
\newblock \doi{10.48550/arXiv.2302.04761}.

\bibitem[Schuurmans(2023)]{schuurmans2023memory}
Dale Schuurmans.
\newblock Memory augmented large language models are computationally universal.
\newblock \emph{arXiv preprint arXiv: 2301.04589}, 2023.
\newblock URL \url{https://arxiv.org/abs/2301.04589v1}.

\bibitem[Shah et~al.(2025)Shah, Bikshandi, Zhang, Thakkar, Ramani, and Dao]{shah2025flashattention}
Jay Shah, Ganesh Bikshandi, Ying Zhang, Vijay Thakkar, Pradeep Ramani, and Tri Dao.
\newblock Flashattention-3: Fast and accurate attention with asynchrony and low-precision.
\newblock \emph{Advances in Neural Information Processing Systems}, 37:\penalty0 68658--68685, 2025.

\bibitem[Shaham et~al.(2023)Shaham, Ivgi, Efrat, Berant, and Levy]{shaham2023zeroscrolls}
Uri Shaham, Maor Ivgi, Avia Efrat, Jonathan Berant, and Omer Levy.
\newblock Zeroscrolls: A zero-shot benchmark for long text understanding.
\newblock \emph{arXiv preprint arXiv:2305.14196}, 2023.

\bibitem[Shandilya et~al.(2024)Shandilya, Xia, Ghosh, Jiang, Zhang, Wu, and Rühle]{shandilya2024tacorltaskawareprompt}
Shivam Shandilya, Menglin Xia, Supriyo Ghosh, Huiqiang Jiang, Jue Zhang, Qianhui Wu, and Victor Rühle.
\newblock Taco-rl: Task aware prompt compression optimization with reinforcement learning, 2024.
\newblock URL \url{https://arxiv.org/abs/2409.13035}.

\bibitem[Shannon(1948)]{shannon1948mathematical}
Claude~Elwood Shannon.
\newblock A mathematical theory of communication.
\newblock \emph{The Bell system technical journal}, 27\penalty0 (3):\penalty0 379--423, 1948.

\bibitem[Shao and Yan(2024)]{shao2024long}
Bin Shao and Jiawei Yan.
\newblock A long-context language model for deciphering and generating bacteriophage genomes.
\newblock \emph{Nature Communications}, 15\penalty0 (1):\penalty0 9392, 2024.

\bibitem[Shao et~al.(2024)Shao, Xiao, Liu, and Zhang]{extensibleembedding}
Ninglu Shao, Shitao Xiao, Zheng Liu, and Peitian Zhang.
\newblock Extensible embedding: {A} flexible multipler for llm's context length.
\newblock \emph{CoRR}, abs/2402.11577, 2024.

\bibitem[Shao et~al.(2019)Shao, Huang, Wen, Xu, and Zhu]{shao2019long}
Zhihong Shao, Minlie Huang, Jiangtao Wen, Wenfei Xu, and Xiaoyan Zhu.
\newblock Long and diverse text generation with planning-based hierarchical variational model.
\newblock \emph{arXiv preprint arXiv:1908.06605}, 2019.

\bibitem[Sharma et~al.(2024)Sharma, Saxon, and Wang]{Sharma-2024-arxiv-LOCOVQA}
Aditya Sharma, Michael Saxon, and William~Yang Wang.
\newblock Losing visual needles in image haystacks: Vision language models are easily distracted in short and long contexts.
\newblock In Yaser Al{-}Onaizan, Mohit Bansal, and Yun{-}Nung Chen, editors, \emph{Findings of the Association for Computational Linguistics: {EMNLP} 2024, Miami, Florida, USA, November 12-16, 2024}, pages 5429--5451. Association for Computational Linguistics, 2024.
\newblock URL \url{https://aclanthology.org/2024.findings-emnlp.312}.

\bibitem[Sharma et~al.(2019)Sharma, Li, and Wang]{sharma2019bigpatent}
Eva Sharma, Chen Li, and Lu~Wang.
\newblock Bigpatent: A large-scale dataset for abstractive and coherent summarization.
\newblock In \emph{Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics}, pages 2204--2213, 2019.

\bibitem[Shaw et~al.(2018)Shaw, Uszkoreit, and Vaswani]{shaw2018self}
Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani.
\newblock Self-attention with relative position representations.
\newblock \emph{arXiv preprint arXiv:1803.02155}, 2018.

\bibitem[Shen et~al.(2022)Shen, Lo, Yu, Dahlberg, Schlanger, and Downey]{shen2022multi}
Zejiang Shen, Kyle Lo, Lauren Yu, Nathan Dahlberg, Margo Schlanger, and Doug Downey.
\newblock Multi-lexsum: Real-world summaries of civil rights lawsuits at multiple granularities.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 13158--13173, 2022.

\bibitem[Shen et~al.(2021)Shen, Zhang, Zhao, Yi, and Li]{shen2021efficient}
Zhuoran Shen, Mingyuan Zhang, Haiyu Zhao, Shuai Yi, and Hongsheng Li.
\newblock Efficient attention: Attention with linear complexities.
\newblock In \emph{Proceedings of the IEEE/CVF winter conference on applications of computer vision}, pages 3531--3539, 2021.

\bibitem[Sheng et~al.(2023)Sheng, Zheng, Yuan, Li, Ryabinin, Chen, Liang, R{\'e}, Stoica, and Zhang]{sheng2023flexgen}
Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max Ryabinin, Beidi Chen, Percy Liang, Christopher R{\'e}, Ion Stoica, and Ce~Zhang.
\newblock Flexgen: High-throughput generative inference of large language models with a single gpu.
\newblock In \emph{International Conference on Machine Learning}, pages 31094--31116. PMLR, 2023.

\bibitem[Shi et~al.(2024{\natexlab{a}})Shi, Sun, Li, and Xu]{DBLP:journals/corr/abs-2405-03085}
Kaize Shi, Xueyao Sun, Qing Li, and Guandong Xu.
\newblock Compressing long context for enhancing {RAG} with amr-based concept distillation.
\newblock \emph{CoRR}, abs/2405.03085, 2024{\natexlab{a}}.
\newblock \doi{10.48550/ARXIV.2405.03085}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2405.03085}.

\bibitem[Shi et~al.(2023)Shi, Min, Lomeli, Zhou, Li, Lin, Smith, Zettlemoyer, Yih, and Lewis]{Shi2023InContextPL}
Weijia Shi, Sewon Min, Maria Lomeli, Chunting Zhou, Margaret Li, Victoria Lin, Noah~A. Smith, Luke~S. Zettlemoyer, Scott Yih, and Mike Lewis.
\newblock In-context pretraining: Language modeling beyond document boundaries.
\newblock \emph{ArXiv}, abs/2310.10638, 2023.

\bibitem[Shi et~al.(2024{\natexlab{b}})Shi, Min, Yasunaga, Seo, James, Lewis, Zettlemoyer, and Yih]{replug}
Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Richard James, Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih.
\newblock {REPLUG}: Retrieval-augmented black-box language models.
\newblock In Kevin Duh, Helena Gomez, and Steven Bethard, editors, \emph{Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)}, pages 8371--8384, Mexico City, Mexico, jun 2024{\natexlab{b}}. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2024.naacl-long.463}.
\newblock URL \url{https://aclanthology.org/2024.naacl-long.463/}.

\bibitem[Shinn et~al.(2023)Shinn, Cassano, Berman, Gopinath, Narasimhan, and Yao]{reflexion}
Noah Shinn, Federico Cassano, Edward Berman, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao.
\newblock Reflexion: Language agents with verbal reinforcement learning.
\newblock \emph{arXiv preprint arXiv: 2303.11366}, 2023.

\bibitem[Shoeybi et~al.(2019)Shoeybi, Patwary, Puri, LeGresley, Casper, and Catanzaro]{shoeybi2019megatron}
Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro.
\newblock Megatron-lm: Training multi-billion parameter language models using model parallelism.
\newblock \emph{arXiv preprint arXiv:1909.08053}, 2019.

\bibitem[Shrivastava et~al.(2023{\natexlab{a}})Shrivastava, Kocetkov, de~Vries, Bahdanau, and Scholak]{shrivastava2023repofusion}
Disha Shrivastava, Denis Kocetkov, Harm de~Vries, Dzmitry Bahdanau, and Torsten Scholak.
\newblock Repofusion: Training code models to understand your repository.
\newblock \emph{CoRR}, abs/2306.10998, 2023{\natexlab{a}}.
\newblock \doi{10.48550/ARXIV.2306.10998}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2306.10998}.

\bibitem[Shrivastava et~al.(2023{\natexlab{b}})Shrivastava, Larochelle, and Tarlow]{Shrivastava2022RepositoryLevelPG}
Disha Shrivastava, Hugo Larochelle, and Daniel Tarlow.
\newblock Repository-level prompt generation for large language models of code.
\newblock In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, \emph{International Conference on Machine Learning, {ICML} 2023, 23-29 July 2023, Honolulu, Hawaii, {USA}}, volume 202 of \emph{Proceedings of Machine Learning Research}, pages 31693--31715. {PMLR}, 2023{\natexlab{b}}.
\newblock URL \url{https://proceedings.mlr.press/v202/shrivastava23a.html}.

\bibitem[Si et~al.(2024)Si, Zhao, Chen, Li, Luo, Lv, An, Qi, Chang, and Sun]{geteau}
Shuzheng Si, Haozhe Zhao, Gang Chen, Yunshui Li, Kangyang Luo, Chuancheng Lv, Kaikai An, Fanchao Qi, Baobao Chang, and Maosong Sun.
\newblock Gateau: Selecting influential sample for long context alignment.
\newblock 2024.

\bibitem[Singhania et~al.(2024)Singhania, Singh, He, Feizi, and Bhatele]{NEURIPS2024_1e027da6}
Prajwal Singhania, Siddharth Singh, Shwai He, Soheil Feizi, and Abhinav Bhatele.
\newblock Loki: Low-rank keys for efficient sparse attention.
\newblock In A.~Globerson, L.~Mackey, D.~Belgrave, A.~Fan, U.~Paquet, J.~Tomczak, and C.~Zhang, editors, \emph{Advances in Neural Information Processing Systems}, volume~37, pages 16692--16723. Curran Associates, Inc., 2024.
\newblock URL \url{https://proceedings.neurips.cc/paper_files/paper/2024/file/1e027da6bec9ceb2ec37951ceeccae93-Paper-Conference.pdf}.

\bibitem[Soboleva et~al.(2023)Soboleva, Al-Khateeb, Myers, Steeves, Hestness, and Dey]{slimpajama}
Daria Soboleva, Faisal Al-Khateeb, Robert Myers, Jacob~R Steeves, Joel Hestness, and Nolan Dey.
\newblock {SlimPajama: A 627B token cleaned and deduplicated version of RedPajama}, June 2023.
\newblock URL \url{https://huggingface.co/datasets/cerebras/SlimPajama-627B}.

\bibitem[Song et~al.(2024{\natexlab{a}})Song, Chen, Chen, Yu, Wan, and Wang]{song-2024-arxiv-milebench}
Dingjie Song, Shunian Chen, Guiming~Hardy Chen, Fei Yu, Xiang Wan, and Benyou Wang.
\newblock Milebench: Benchmarking mllms in long context.
\newblock \emph{CoRR}, abs/2404.18532, 2024{\natexlab{a}}.
\newblock \doi{10.48550/ARXIV.2404.18532}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2404.18532}.

\bibitem[Song et~al.(2023)Song, Wang, Cho, Pan, and Yu]{song2023zebraextendingcontextwindow}
Kaiqiang Song, Xiaoyang Wang, Sangwoo Cho, Xiaoman Pan, and Dong Yu.
\newblock Zebra: Extending context window with layerwise grouped local-global attention, 2023.
\newblock URL \url{https://arxiv.org/abs/2312.08618}.

\bibitem[Song et~al.(2024{\natexlab{b}})Song, Zheng, and Luo]{song2024counting}
Mingyang Song, Mao Zheng, and Xuan Luo.
\newblock Counting-stars: A multi-evidence, position-aware, and scalable benchmark for evaluating long-context large language models.
\newblock \emph{Preprint}, 2024{\natexlab{b}}.

\bibitem[Sourouri et~al.(2014)Sourouri, Gillberg, Baden, and Cai]{sourouri2014effective}
Mohammed Sourouri, Tor Gillberg, Scott~B Baden, and Xing Cai.
\newblock Effective multi-gpu communication using multiple cuda streams and threads.
\newblock In \emph{2014 20th IEEE International Conference on Parallel and Distributed Systems (ICPADS)}, pages 981--986. IEEE, 2014.

\bibitem[spicychat.ai(2024)]{spicychat}
spicychat.ai.
\newblock Spicychat, 2024.
\newblock URL \url{https://spicychat.ai/}.

\bibitem[Staniszewski et~al.(2023{\natexlab{a}})Staniszewski, Tworkowski, Jaszczur, Zhao, Michalewski, Kuci{\'n}ski, and Mi{\l}o{\'s}]{staniszewski2023structured}
Konrad Staniszewski, Szymon Tworkowski, Sebastian Jaszczur, Yu~Zhao, Henryk Michalewski, {\L}ukasz Kuci{\'n}ski, and Piotr Mi{\l}o{\'s}.
\newblock Structured packing in llm training improves long context utilization.
\newblock \emph{arXiv preprint arXiv:2312.17296}, 2023{\natexlab{a}}.

\bibitem[Staniszewski et~al.(2023{\natexlab{b}})Staniszewski, Tworkowski, Zhao, Jaszczur, Michalewski, Kuci'nski, and Milo's]{spacking}
Konrad Staniszewski, Szymon Tworkowski, Yu~Zhao, Sebastian Jaszczur, Henryk Michalewski, Lukasz Kuci'nski, and Piotr Milo's.
\newblock Structured packing in llm training improves long context utilization.
\newblock \emph{ArXiv}, abs/2312.17296, 2023{\natexlab{b}}.

\bibitem[Stelmakh et~al.(2022)Stelmakh, Luan, Dhingra, and Chang]{stelmakh2022asqa}
Ivan Stelmakh, Yi~Luan, Bhuwan Dhingra, and Ming-Wei Chang.
\newblock Asqa: Factoid questions meet long-form answers.
\newblock \emph{arXiv preprint arXiv:2204.06092}, 2022.

\bibitem[Su(2023{\natexlab{a}})]{jianlin_rope_beta_base}
Jianlin Su.
\newblock Transformer 10, Aug 2023{\natexlab{a}}.
\newblock URL \url{https://spaces.ac.cn/archives/9675}.

\bibitem[Su(2023{\natexlab{b}})]{kexuefm-9708}
Jianlin Su.
\newblock Transformer 12, Aug 2023{\natexlab{b}}.
\newblock URL \url{https://kexue.fm/archives/9708}.

\bibitem[Su(2024)]{su-kexuefm-2024-ropetie}
Jianlin Su.
\newblock Transformer 17.
\newblock \emph{Online Resource}, Mar 2024.
\newblock URL \url{https://spaces.ac.cn/archives/10040}.

\bibitem[Su et~al.(2024)Su, Ahmed, Lu, Pan, Bo, and Liu]{su2024roformer}
Jianlin Su, Murtadha Ahmed, Yu~Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu.
\newblock Roformer: Enhanced transformer with rotary position embedding.
\newblock \emph{Neurocomputing}, 568:\penalty0 127063, 2024.

\bibitem[Sun et~al.()Sun, Chen, Yang, Tian, and Chen]{suntriforce}
Hanshi Sun, Zhuoming Chen, Xinyu Yang, Yuandong Tian, and Beidi Chen.
\newblock Triforce: Lossless acceleration of long sequence generation with hierarchical speculative decoding.
\newblock In \emph{First Conference on Language Modeling}.

\bibitem[Sun et~al.(2023{\natexlab{a}})Sun, Zhuang, Kong, Dai, and Zhang]{adaplanner}
Haotian Sun, Yuchen Zhuang, Lingkai Kong, Bo~Dai, and Chao Zhang.
\newblock Adaplanner: Adaptive planning from feedback with language models.
\newblock \emph{arXiv preprint arXiv: 2305.16653}, 2023{\natexlab{a}}.

\bibitem[Sun et~al.(2021)Sun, Krishna, Mattarella-Micke, and Iyyer]{sun2021long}
Simeng Sun, Kalpesh Krishna, Andrew Mattarella-Micke, and Mohit Iyyer.
\newblock Do long-range language models actually use long-range context?
\newblock In \emph{Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing}, pages 807--822, 2021.

\bibitem[Sun et~al.(2023{\natexlab{b}})Sun, Liu, Wang, Zhu, and Iyyer]{pearl}
Simeng Sun, Y.~Liu, Shuo Wang, Chenguang Zhu, and Mohit Iyyer.
\newblock Pearl: Prompting large language models to plan and execute actions over long documents.
\newblock \emph{Conference of the European Chapter of the Association for Computational Linguistics}, 2023{\natexlab{b}}.
\newblock \doi{10.48550/arXiv.2305.14564}.

\bibitem[Sun et~al.(2023{\natexlab{c}})Sun, Yan, Ma, Wang, Ren, Chen, Yin, and Ren]{DBLP:conf/emnlp/0001YMWRCYR23}
Weiwei Sun, Lingyong Yan, Xinyu Ma, Shuaiqiang Wang, Pengjie Ren, Zhumin Chen, Dawei Yin, and Zhaochun Ren.
\newblock Is chatgpt good at search? investigating large language models as re-ranking agents.
\newblock In Houda Bouamor, Juan Pino, and Kalika Bali, editors, \emph{Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, {EMNLP} 2023, Singapore, December 6-10, 2023}, pages 14918--14937. Association for Computational Linguistics, 2023{\natexlab{c}}.
\newblock \doi{10.18653/V1/2023.EMNLP-MAIN.923}.
\newblock URL \url{https://doi.org/10.18653/v1/2023.emnlp-main.923}.

\bibitem[Sun et~al.(2022)Sun, Dong, Patra, Ma, Huang, Benhaim, Chaudhary, Song, and Wei]{sun2022length}
Yutao Sun, Li~Dong, Barun Patra, Shuming Ma, Shaohan Huang, Alon Benhaim, Vishrav Chaudhary, Xia Song, and Furu Wei.
\newblock A length-extrapolatable transformer.
\newblock \emph{arXiv preprint arXiv:2212.10554}, 2022.

\bibitem[Sun et~al.(2023{\natexlab{d}})Sun, Dong, Huang, Ma, Xia, Xue, Wang, and Wei]{sun2023retentivenetworksuccessortransformer}
Yutao Sun, Li~Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, and Furu Wei.
\newblock Retentive network: A successor to transformer for large language models, 2023{\natexlab{d}}.
\newblock URL \url{https://arxiv.org/abs/2307.08621}.

\bibitem[Sun et~al.(2025)Sun, Dong, Zhu, Huang, Wang, Ma, Zhang, Wang, and Wei]{sun2025you}
Yutao Sun, Li~Dong, Yi~Zhu, Shaohan Huang, Wenhui Wang, Shuming Ma, Quanlu Zhang, Jianyong Wang, and Furu Wei.
\newblock You only cache once: Decoder-decoder architectures for language models.
\newblock \emph{Advances in Neural Information Processing Systems}, 37:\penalty0 7339--7361, 2025.

\bibitem[Talmor et~al.(2018)Talmor, Herzig, Lourie, and Berant]{talmor2018commonsenseqa}
Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant.
\newblock Commonsenseqa: A question answering challenge targeting commonsense knowledge.
\newblock \emph{arXiv preprint arXiv:1811.00937}, 2018.

\bibitem[Tan et~al.(2024{\natexlab{a}})Tan, Guo, Shi, Xu, Liu, Feng, Li, Wang, Shang, Liu, et~al.]{tan2024proxyqa}
Haochen Tan, Zhijiang Guo, Zhan Shi, Lu~Xu, Zhili Liu, Yunlong Feng, Xiaoguang Li, Yasheng Wang, Lifeng Shang, Qun Liu, et~al.
\newblock Proxyqa: An alternative framework for evaluating long-form text generation with large language models.
\newblock \emph{arXiv preprint arXiv:2401.15042}, 2024{\natexlab{a}}.

\bibitem[Tan et~al.(2024{\natexlab{b}})Tan, Ding, Zhang, Li, Zhou, Yue, Xia, Jiang, Zheng, Xu, Bi, Gu, Wang, Karlsson, An, and Lu]{DBLP:journals/corr/abs-2403-03186}
Weihao Tan, Ziluo Ding, Wentao Zhang, Boyu Li, Bohan Zhou, Junpeng Yue, Haochong Xia, Jiechuan Jiang, Longtao Zheng, Xinrun Xu, Yifei Bi, Pengjie Gu, Xinrun Wang, B{\"{o}}rje~F. Karlsson, Bo~An, and Zongqing Lu.
\newblock Towards general computer control: {A} multimodal agent for red dead redemption {II} as a case study.
\newblock \emph{CoRR}, abs/2403.03186, 2024{\natexlab{b}}.
\newblock \doi{10.48550/ARXIV.2403.03186}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2403.03186}.

\bibitem[Tancik et~al.(2020)Tancik, Srinivasan, Mildenhall, Fridovich-Keil, Raghavan, Singhal, Ramamoorthi, Barron, and Ng]{nips_fourier_high_freq}
Matthew Tancik, Pratul Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin Raghavan, Utkarsh Singhal, Ravi Ramamoorthi, Jonathan Barron, and Ren Ng.
\newblock Fourier features let networks learn high frequency functions in low dimensional domains.
\newblock In H.~Larochelle, M.~Ranzato, R.~Hadsell, M.F. Balcan, and H.~Lin, editors, \emph{Advances in Neural Information Processing Systems}, volume~33, pages 7537--7547. Curran Associates, Inc., 2020.
\newblock URL \url{https://proceedings.neurips.cc/paper_files/paper/2020/file/55053683268957697aa39fba6f231c68-Paper.pdf}.

\bibitem[Tang et~al.(2024{\natexlab{a}})Tang, Lin, Lin, Han, Hong, Yao, and Wang]{tang2024razorattention}
Hanlin Tang, Yang Lin, Jing Lin, Qingsen Han, Shikuan Hong, Yiwu Yao, and Gongyi Wang.
\newblock Razorattention: Efficient kv cache compression through retrieval heads.
\newblock \emph{arXiv preprint arXiv:2407.15891}, 2024{\natexlab{a}}.

\bibitem[Tang et~al.(2025)Tang, Lin, Lin, Han, Ke, Hong, Yao, and Wang]{tang2025razorattention}
Hanlin Tang, Yang Lin, Jing Lin, Qingsen Han, Danning Ke, Shikuan Hong, Yiwu Yao, and Gongyi Wang.
\newblock Razorattention: Efficient {KV} cache compression through retrieval heads.
\newblock In \emph{The Thirteenth International Conference on Learning Representations}, 2025.
\newblock URL \url{https://openreview.net/forum?id=tkiZQlL04w}.

\bibitem[Tang et~al.(2024{\natexlab{b}})Tang, Zhao, Zhu, Xiao, Kasikci, and Han]{pmlr-v235-tang24l}
Jiaming Tang, Yilong Zhao, Kan Zhu, Guangxuan Xiao, Baris Kasikci, and Song Han.
\newblock {QUEST}: Query-aware sparsity for efficient long-context {LLM} inference.
\newblock In Ruslan Salakhutdinov, Zico Kolter, Katherine Heller, Adrian Weller, Nuria Oliver, Jonathan Scarlett, and Felix Berkenkamp, editors, \emph{Proceedings of the 41st International Conference on Machine Learning}, volume 235 of \emph{Proceedings of Machine Learning Research}, pages 47901--47911. PMLR, 21--27 Jul 2024{\natexlab{b}}.
\newblock URL \url{https://proceedings.mlr.press/v235/tang24l.html}.

\bibitem[Tang et~al.(2024{\natexlab{c}})Tang, Sun, Li, Zhu, and Zhang]{Tang2024LOGOL}
Zecheng Tang, Zechen Sun, Juntao Li, Qiaoming Zhu, and Min Zhang.
\newblock Logo - long context alignment via efficient preference optimization.
\newblock \emph{ArXiv}, abs/2410.18533, 2024{\natexlab{c}}.

\bibitem[Tang et~al.(2024{\natexlab{d}})Tang, Sun, Li, Zhu, and Zhang]{tang2024logo}
Zecheng Tang, Zechen Sun, Juntao Li, Qiaoming Zhu, and Min Zhang.
\newblock Logo--long context alignment via efficient preference optimization.
\newblock \emph{arXiv preprint arXiv:2410.18533}, 2024{\natexlab{d}}.

\bibitem[Tang et~al.(2024{\natexlab{e}})Tang, Zhou, Li, Ji, Hou, and Zhang]{tang2024lciteeval}
Zecheng Tang, Keyan Zhou, Juntao Li, Baibei Ji, Jianye Hou, and Min Zhang.
\newblock L-citeeval: Do long-context models truly leverage context for responding?, 2024{\natexlab{e}}.

\bibitem[Tay et~al.(2020)Tay, Dehghani, Abnar, Shen, Bahri, Pham, Rao, Yang, Ruder, and Metzler]{tay2020longrangearenabenchmark}
Yi~Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler.
\newblock Long range arena: A benchmark for efficient transformers, 2020.
\newblock URL \url{https://arxiv.org/abs/2011.04006}.

\bibitem[Tay et~al.(2022)Tay, Tran, Dehghani, Ni, Bahri, Mehta, Qin, Hui, Zhao, Gupta, Schuster, Cohen, and Metzler]{dsi}
Yi~Tay, Vinh~Q. Tran, Mostafa Dehghani, Jianmo Ni, Dara Bahri, Harsh Mehta, Zhen Qin, Kai Hui, Zhe Zhao, Jai Gupta, Tal Schuster, William~W. Cohen, and Donald Metzler.
\newblock Transformer memory as a differentiable search index.
\newblock \emph{arXiv preprint arXiv: 2202.06991}, 2022.

\bibitem[Team(2025{\natexlab{a}})]{coze}
Coze Team.
\newblock Coze.
\newblock \url{https://www.coze.cn/}, 2025{\natexlab{a}}.

\bibitem[Team(2025{\natexlab{b}})]{dify}
Dify Team.
\newblock The innovation engine for genai applications.
\newblock \url{https://dify.ai/}, 2025{\natexlab{b}}.

\bibitem[Team et~al.(2024{\natexlab{a}})Team, Georgiev, Lei, Burnell, Bai, Gulati, Tanzer, Vincent, Pan, Wang, et~al.]{team2024gemini}
Gemini Team, Petko Georgiev, Ving~Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et~al.
\newblock Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context.
\newblock \emph{arXiv preprint arXiv:2403.05530}, 2024{\natexlab{a}}.

\bibitem[Team et~al.(2024{\natexlab{b}})Team, Riviere, Pathak, Sessa, Hardin, Bhupatiraju, Hussenot, Mesnard, Shahriari, Ram{\'e}, et~al.]{team2024gemma}
Gemma Team, Morgane Riviere, Shreya Pathak, Pier~Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, L{\'e}onard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ram{\'e}, et~al.
\newblock Gemma 2: Improving open language models at a practical size.
\newblock \emph{arXiv preprint arXiv:2408.00118}, 2024{\natexlab{b}}.

\bibitem[Team et~al.(2024{\natexlab{c}})Team, Lenz, Arazi, Bergman, Manevich, Peleg, Aviram, Almagor, Fridman, Padnos, et~al.]{team2024jamba}
Jamba Team, Barak Lenz, Alan Arazi, Amir Bergman, Avshalom Manevich, Barak Peleg, Ben Aviram, Chen Almagor, Clara Fridman, Dan Padnos, et~al.
\newblock Jamba-1.5: Hybrid transformer-mamba models at scale.
\newblock \emph{arXiv preprint arXiv:2408.12570}, 2024{\natexlab{c}}.

\bibitem[Team et~al.(2025{\natexlab{a}})Team, Du, Gao, Xing, Jiang, Chen, Li, Xiao, Du, Liao, et~al.]{team2025kimi}
Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, et~al.
\newblock Kimi k1. 5: Scaling reinforcement learning with llms.
\newblock \emph{arXiv preprint arXiv:2501.12599}, 2025{\natexlab{a}}.

\bibitem[Team et~al.(2025{\natexlab{b}})Team, Du, Yao, Ma, Wang, Zheng, Zhu, Liu, Liang, Jin, Wei, Zheng, Deng, Guo, Jia, Jiang, Liao, Li, Li, Li, Li, Li, Ma, Ni, Que, Wang, Wen, Wu, Xing, Xu, Yang, Wang, Zhou, Bai, Bu, Cai, Chen, Chen, Cheng, Cheng, Ding, Huang, Huang, Li, Li, Li, Liang, Lin, Lin, Ma, Peng, Peng, Qi, Qiu, Qu, Tan, Wang, Wang, Wang, Wang, Wang, Xu, Yang, Yuan, hao Yue, Zhan, Zhang, Zhang, Zhang, Zhang, Zhang, Zhao, Zheng, Zhong, Gao, Li, Liu, Liu, Liu, Ni, Peng, Qin, Su, Wang, Wang, Yang, Yang, Cao, Yue, Zhang, Zhou, Liu, Lin, Huang, and Zhang]{supergpqa}
M-A-P Team, Xinrun Du, Yifan Yao, Kaijing Ma, Bingli Wang, Tianyu Zheng, Kang Zhu, Minghao Liu, Yiming Liang, Xiaolong Jin, Zhen-Nan Wei, Chujie Zheng, Kaixin Deng, Shuyue Guo, Shian Jia, Sichao Jiang, Yiyan Liao, Rui Li, Qinrui Li, Sirun Li, Yizhi Li, Yunwen Li, Dehua Ma, Yuansheng Ni, Haoran Que, Qiyao Wang, Zhoufutu Wen, Si-Xuan Wu, Tianshun Xing, Ming Xu, Zhenzhu Yang, Ze~Wang, Junting Zhou, Yu~Bai, Xingyuan Bu, Chenglin Cai, Liang Chen, Yifan Chen, Chengtuo Cheng, Tianhao Cheng, Keyi Ding, Siming Huang, Yun-Jing Huang, Yaoru Li, Yizhe Li, Zhaoqun Li, Tianhao Liang, Chengdong Lin, Hongquan Lin, Yi-Hui Ma, Zhongyuan Peng, Zifan Peng, Qige Qi, Shi Qiu, Xingwei Qu, Yizhou Tan, Zili Wang, Chenqing Wang, Hao Wang, Yiya Wang, Yubo Wang, Jiajun Xu, Kexin Yang, Ru-Qing Yuan, Yuan hao Yue, Tianyang Zhan, Chun Zhang, Jing-Yun Zhang, Xiyue Zhang, Xing Zhang, Yue Zhang, Yongchi Zhao, Xiangyu Zheng, Chenghua Zhong, Yang Gao, Zhoujun Li, Dayiheng Liu, Qian Liu, Tianyu Liu, Shiwen Ni, Junran Peng, Yujia Qin, Wenbo Su,
  Guoyin Wang, Shi Wang, Jian Yang, Min Yang, Meng Cao, Xiang Yue, Zhaoxiang Zhang, Wangchunshu Zhou, Jiaheng Liu, Qunshu Lin, Wenhao Huang, and Ge~Zhang.
\newblock Supergpqa: Scaling llm evaluation across 285 graduate disciplines.
\newblock 2025{\natexlab{b}}.

\bibitem[Team(2025{\natexlab{c}})]{manus}
Manus Team.
\newblock Leave it to manus.
\newblock \url{https://manus.im/}, 2025{\natexlab{c}}.

\bibitem[Team(2025{\natexlab{d}})]{oai2025computeruse}
OpenAI Team.
\newblock Computer-using agent.
\newblock \url{https://openai.com/index/computer-using-agent/}, 2025{\natexlab{d}}.

\bibitem[Team(2025{\natexlab{e}})]{oai2025deepresearch}
OpenAI Team.
\newblock Introducing deep research.
\newblock \url{https://openai.com/index/introducing-deep-research/}, 2025{\natexlab{e}}.

\bibitem[Team(2025{\natexlab{f}})]{Qwen2.5-VL}
Qwen Team.
\newblock Qwen2.5-vl, January 2025{\natexlab{f}}.
\newblock URL \url{https://qwenlm.github.io/blog/qwen2.5-vl/}.

\bibitem[Tian et~al.(2024)Tian, Li, Fu, Deng, Luo, Qian, Wang, Cong, Zhang, Wu, et~al.]{tian2024distance}
Runchu Tian, Yanghao Li, Yuepeng Fu, Siyang Deng, Qinyu Luo, Cheng Qian, Shuo Wang, Xin Cong, Zhong Zhang, Yesai Wu, et~al.
\newblock Distance between relevant information pieces causes bias in long-context llms.
\newblock \emph{arXiv preprint arXiv:2410.14641}, 2024.

\bibitem[Tirumala et~al.(2023)Tirumala, Simig, Aghajanyan, and Morcos]{D4}
Kushal Tirumala, Daniel Simig, Armen Aghajanyan, and Ari~S. Morcos.
\newblock {D4:} improving {LLM} pretraining via document de-duplication and diversification.
\newblock \emph{CoRR}, abs/2308.12284, 2023.
\newblock \doi{10.48550/ARXIV.2308.12284}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2308.12284}.

\bibitem[Touvron et~al.(2023)Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar, et~al.]{touvron2023llama}
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric Hambro, Faisal Azhar, et~al.
\newblock Llama: Open and efficient foundation language models.
\newblock \emph{arXiv preprint arXiv:2302.13971}, 2023.

\bibitem[Vasu et~al.(2024)Vasu, Faghri, Li, Koc, True, Antony, Santhanam, Gabriel, Grasch, and Tuzel]{vasu-2024-arxiv-fastvlm}
Pavan Kumar~Anasosalu Vasu, Fartash Faghri, Chun-Liang Li, Cem Koc, Nate True, Albert Antony, Gokul Santhanam, James Gabriel, Peter Grasch, and Oncel Tuzel.
\newblock Fastvlm: Efficient vision encoding for vision language models.
\newblock \emph{CoRR}, abs/2412.13303, 2024.
\newblock \doi{10.48550/ARXIV.2412.13303}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2412.13303}.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan~N. Gomez, Lukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock In \emph{{NIPS}}, pages 5998--6008, 2017.

\bibitem[Venkatraman et~al.(2024)Venkatraman, Tripto, and Lee]{venkatraman2024collabstory}
Saranya Venkatraman, Nafis~Irtiza Tripto, and Dongwon Lee.
\newblock Collabstory: Multi-llm collaborative story generation and authorship analysis.
\newblock \emph{arXiv preprint arXiv:2406.12665}, 2024.

\bibitem[Vodrahalli et~al.(2024)Vodrahalli, Ontanon, Tripuraneni, Xu, Jain, Shivanna, Hui, Dikkala, Kazemi, Fatemi, et~al.]{vodrahalli2024michelangelo}
Kiran Vodrahalli, Santiago Ontanon, Nilesh Tripuraneni, Kelvin Xu, Sanil Jain, Rakesh Shivanna, Jeffrey Hui, Nishanth Dikkala, Mehran Kazemi, Bahare Fatemi, et~al.
\newblock Michelangelo: Long context evaluations beyond haystacks via latent structure queries.
\newblock \emph{arXiv preprint arXiv:2409.12640}, 2024.

\bibitem[Voelker and Eliasmith(2018)]{voelker2018improving}
Aaron~R. Voelker and Chris Eliasmith.
\newblock Improving spiking dynamical networks: Accurate delays, higher-order synapses, and time cells.
\newblock \emph{Neural Comput.}, 30\penalty0 (3), 2018.

\bibitem[Voita et~al.(2024)Voita, Ferrando, and Nalmpantis]{voita-etal-2024-neurons_func}
Elena Voita, Javier Ferrando, and Christoforos Nalmpantis.
\newblock Neurons in large language models: Dead, n-gram, positional.
\newblock In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, \emph{Findings of the Association for Computational Linguistics: ACL 2024}, pages 1288--1301, Bangkok, Thailand, August 2024. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2024.findings-acl.75}.
\newblock URL \url{https://aclanthology.org/2024.findings-acl.75/}.

\bibitem[Waleffe et~al.(2024)Waleffe, Byeon, Riach, Norick, Korthikanti, Dao, Gu, Hatamizadeh, Singh, Narayanan, et~al.]{waleffe2024empirical}
Roger Waleffe, Wonmin Byeon, Duncan Riach, Brandon Norick, Vijay Korthikanti, Tri Dao, Albert Gu, Ali Hatamizadeh, Sudhakar Singh, Deepak Narayanan, et~al.
\newblock An empirical study of mamba-based language models.
\newblock \emph{arXiv preprint arXiv:2406.07887}, 2024.

\bibitem[Wang et~al.(2022)Wang, Pang, Chen, Phang, and Bowman]{wang2022squality}
Alex Wang, Richard~Yuanzhe Pang, Angelica Chen, Jason Phang, and Samuel Bowman.
\newblock Squality: Building a long-document summarization dataset the hard way.
\newblock In \emph{Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing}, pages 1139--1156, 2022.

\bibitem[Wang et~al.(2019)Wang, Zhao, Lioma, Li, Zhang, and Simonsen]{wang2019encoding}
Benyou Wang, Donghao Zhao, Christina Lioma, Qiuchi Li, Peng Zhang, and Jakob~Grue Simonsen.
\newblock Encoding word order in complex embeddings.
\newblock \emph{arXiv preprint arXiv:1912.12333}, 2019.

\bibitem[Wang et~al.(2024{\natexlab{a}})Wang, Zhang, Feng, Li, Sun, Liu, and Peng]{wang2024teaching}
Chong Wang, Jian Zhang, Yebo Feng, Tianlin Li, Weisong Sun, Yang Liu, and Xin Peng.
\newblock Teaching code llms to use autocompletion tools in repository-level code generation.
\newblock \emph{arXiv preprint arXiv:2401.06391}, 2024{\natexlab{a}}.

\bibitem[Wang et~al.(2024{\natexlab{b}})Wang, Duan, Zhang, Lin, and Chen]{wang2024adaleval}
Chonghua Wang, Haodong Duan, Songyang Zhang, Dahua Lin, and Kai Chen.
\newblock Ada-leval: Evaluating long-context llms with length-adaptable benchmarks, 2024{\natexlab{b}}.

\bibitem[Wang et~al.(2023{\natexlab{a}})Wang, Xie, Jiang, Mandlekar, Xiao, Zhu, Fan, and Anandkumar]{voyager}
Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar.
\newblock Voyager: An open-ended embodied agent with large language models.
\newblock \emph{arXiv preprint arXiv: 2305.16291}, 2023{\natexlab{a}}.

\bibitem[Wang et~al.(2024{\natexlab{c}})Wang, Liu, Du, Zhu, Du, Kawaguchi, and Pang]{wang2024precision}
Haonan Wang, Qian Liu, Chao Du, Tongyao Zhu, Cunxiao Du, Kenji Kawaguchi, and Tianyu Pang.
\newblock When precision meets position: Bfloat16 breaks down rope in long-context training.
\newblock \emph{arXiv preprint arXiv:2411.13476}, 2024{\natexlab{c}}.

\bibitem[Wang et~al.(2024{\natexlab{d}})Wang, Shi, Tan, Qin, Wang, Zhang, Nambi, Ganu, and Wang]{wang-2024-arxiv-MMNeedle}
Hengyi Wang, Haizhou Shi, Shiwei Tan, Weiyi Qin, Wenyuan Wang, Tunyu Zhang, Akshay Nambi, Tanuja Ganu, and Hao Wang.
\newblock Multimodal needle in a haystack: Benchmarking long-context capability of multimodal large language models.
\newblock \emph{CoRR}, abs/2406.11230, 2024{\natexlab{d}}.
\newblock \doi{10.48550/ARXIV.2406.11230}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2406.11230}.

\bibitem[Wang et~al.(2024{\natexlab{e}})Wang, Meng, Liang, and Zhou]{wang2024drt}
Jiaan Wang, Fandong Meng, Yunlong Liang, and Jie Zhou.
\newblock Drt-o1: Optimized deep reasoning translation via long chain-of-thought.
\newblock \emph{arXiv preprint arXiv:2412.17498}, 2024{\natexlab{e}}.

\bibitem[Wang et~al.(2023{\natexlab{b}})Wang, Variengien, Conmy, Shlegeris, and Steinhardt]{wang_interpretability_wild_2023}
Kevin~Ro Wang, Alexandre Variengien, Arthur Conmy, Buck Shlegeris, and Jacob Steinhardt.
\newblock Interpretability in the {Wild}: a {Circuit} for {Indirect} {Object} {Identification} in {GPT}-2 {Small}.
\newblock In \emph{The {Eleventh} {International} {Conference} on {Learning} {Representations}}, 2023{\natexlab{b}}.
\newblock URL \url{https://openreview.net/forum?id=NpsVSN6o4ul}.

\bibitem[Wang et~al.(2023{\natexlab{c}})Wang, Ma, Feng, Zhang, ran Yang, Zhang, Chen, Tang, Chen, Lin, Zhao, Wei, and rong Wen]{agent-survey-1}
Lei Wang, Chengbang Ma, Xueyang Feng, Zeyu Zhang, Hao ran Yang, Jingsen Zhang, Zhi-Yang Chen, Jiakai Tang, Xu~Chen, Yankai Lin, Wayne~Xin Zhao, Zhewei Wei, and Ji~rong Wen.
\newblock A survey on large language model based autonomous agents.
\newblock \emph{Frontiers Comput. Sci.}, 2023{\natexlab{c}}.
\newblock \doi{10.1007/s11704-024-40231-1}.

\bibitem[Wang et~al.(2024{\natexlab{f}})Wang, Dong, Xu, Dong, Wang, Saha, Lim, Xiong, and Sahoo]{wang2024mathhay}
Lei Wang, Shan Dong, Yuhui Xu, Hanze Dong, Yalu Wang, Amrita Saha, Ee-Peng Lim, Caiming Xiong, and Doyen Sahoo.
\newblock Mathhay: An automated benchmark for long-context mathematical reasoning in llms.
\newblock \emph{arXiv preprint arXiv:2410.04698}, 2024{\natexlab{f}}.

\bibitem[Wang et~al.(2023{\natexlab{d}})Wang, Yang, and Wei]{wang2023query2doc0}
Liang Wang, Nan Yang, and Furu Wei.
\newblock Query2doc: Query expansion with large language models.
\newblock \emph{Conference on Empirical Methods in Natural Language Processing}, 2023{\natexlab{d}}.
\newblock \doi{10.48550/arXiv.2303.07678}.

\bibitem[Wang et~al.(2024{\natexlab{g}})Wang, Yang, Huang, Yang, Majumder, and Wei]{DBLP:conf/acl/WangYHYMW24}
Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, and Furu Wei.
\newblock Improving text embeddings with large language models.
\newblock In Lun{-}Wei Ku, Andre Martins, and Vivek Srikumar, editors, \emph{Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), {ACL} 2024, Bangkok, Thailand, August 11-16, 2024}, pages 11897--11916. Association for Computational Linguistics, 2024{\natexlab{g}}.
\newblock \doi{10.18653/V1/2024.ACL-LONG.642}.
\newblock URL \url{https://doi.org/10.18653/v1/2024.acl-long.642}.

\bibitem[Wang et~al.(2024{\natexlab{h}})Wang, Yang, Huang, Yang, Majumder, and Wei]{wang2024large}
Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, and Furu Wei.
\newblock Large search model: Redefining search stack in the era of llms.
\newblock In \emph{ACM SIGIR Forum}, volume~57, pages 1--16. ACM New York, NY, USA, 2024{\natexlab{h}}.

\bibitem[Wang et~al.(2023{\natexlab{e}})Wang, Lyu, Ji, Zhang, Yu, Shi, and Tu]{DBLP:conf/emnlp/WangLJZY0T23}
Longyue Wang, Chenyang Lyu, Tianbo Ji, Zhirui Zhang, Dian Yu, Shuming Shi, and Zhaopeng Tu.
\newblock Document-level machine translation with large language models.
\newblock In Houda Bouamor, Juan Pino, and Kalika Bali, editors, \emph{Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, {EMNLP} 2023, Singapore, December 6-10, 2023}, pages 16646--16661. Association for Computational Linguistics, 2023{\natexlab{e}}.
\newblock \doi{10.18653/V1/2023.EMNLP-MAIN.1036}.
\newblock URL \url{https://doi.org/10.18653/v1/2023.emnlp-main.1036}.

\bibitem[Wang et~al.(2024{\natexlab{i}})Wang, Du, Jiao, Lyu, Pang, Cui, Song, Wong, Shi, and Tu]{wang2024benchmarking}
Longyue Wang, Zefeng Du, Wenxiang Jiao, Chenyang Lyu, Jianhui Pang, Leyang Cui, Kaiqiang Song, Derek~F. Wong, Shuming Shi, and Zhaopeng Tu.
\newblock Benchmarking and improving long-text translation with large language models.
\newblock In Lun{-}Wei Ku, Andre Martins, and Vivek Srikumar, editors, \emph{Findings of the Association for Computational Linguistics, {ACL} 2024, Bangkok, Thailand and virtual meeting, August 11-16, 2024}, pages 7175--7187. Association for Computational Linguistics, 2024{\natexlab{i}}.
\newblock \doi{10.18653/V1/2024.FINDINGS-ACL.428}.
\newblock URL \url{https://doi.org/10.18653/v1/2024.findings-acl.428}.

\bibitem[Wang et~al.(2024{\natexlab{j}})Wang, Chen, Cheng, Liao, Zhang, Wu, Yu, Xu, Zhang, Luo, et~al.]{wang2024leave}
Minzheng Wang, Longze Chen, Fu~Cheng, Shengyi Liao, Xinghua Zhang, Bingli Wu, Haiyang Yu, Nan Xu, Lei Zhang, Run Luo, et~al.
\newblock Leave no document behind: Benchmarking long-context llms with extended multi-doc qa.
\newblock In \emph{Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing}, pages 5627--5646, 2024{\natexlab{j}}.

\bibitem[Wang et~al.(2024{\natexlab{k}})Wang, Bai, Tan, Wang, Fan, Bai, Chen, Liu, Wang, Ge, Fan, Dang, Du, Ren, Men, Liu, Zhou, Zhou, and Lin]{Qwen2VL}
Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin.
\newblock Qwen2-vl: Enhancing vision-language model's perception of the world at any resolution.
\newblock \emph{arXiv preprint arXiv:2409.12191}, 2024{\natexlab{k}}.

\bibitem[Wang et~al.(2020)Wang, Li, Khabsa, Fang, and Ma]{wang2020linformerselfattentionlinearcomplexity}
Sinong Wang, Belinda~Z. Li, Madian Khabsa, Han Fang, and Hao Ma.
\newblock Linformer: Self-attention with linear complexity, 2020.
\newblock URL \url{https://arxiv.org/abs/2006.04768}.

\bibitem[Wang et~al.(2024{\natexlab{l}})Wang, Kobyzev, Lu, Rezagholizadeh, and Liu]{wang2024resonance}
Suyuchen Wang, Ivan Kobyzev, Peng Lu, Mehdi Rezagholizadeh, and Bang Liu.
\newblock Resonance rope: Improving context length generalization of large language models.
\newblock \emph{arXiv preprint arXiv:2403.00071}, 2024{\natexlab{l}}.

\bibitem[Wang et~al.(2024{\natexlab{m}})Wang, Chen, Jia, Wang, Fang, Wang, Gao, Xie, Xu, Dai, Liu, Wu, Ding, Li, Huang, Deng, Yu, Ma, Xiao, Chen, Xiang, Wang, Zhu, Xiao, Wang, Wang, Ding, Huang, Xu, Tayier, Hu, Gao, Zheng, Ye, Li, Wan, Jiang, Wang, Cheng, Song, Tang, Xu, Zhang, Chen, Jiang, and Zhou]{wang2024weaver}
Tiannan Wang, Jiamin Chen, Qingrui Jia, Shuai Wang, Ruoyu Fang, Huilin Wang, Zhaowei Gao, Chunzhao Xie, Chuou Xu, Jihong Dai, Yibin Liu, Jialong Wu, Shengwei Ding, Long Li, Zhiwei Huang, Xinle Deng, Teng Yu, Gangan Ma, Han Xiao, Zixin Chen, Danjun Xiang, Yunxia Wang, Yuanyuan Zhu, Yi~Xiao, Jing Wang, Yiru Wang, Siran Ding, Jiayang Huang, Jiayi Xu, Yilihamu Tayier, Zhenyu Hu, Yuan Gao, Chengfeng Zheng, Yueshu Ye, Yihang Li, Lei Wan, Xinyue Jiang, Yujie Wang, Siyu Cheng, Zhule Song, Xiangru Tang, Xiaohua Xu, Ningyu Zhang, Huajun Chen, Yuchen~Eleanor Jiang, and Wangchunshu Zhou.
\newblock Weaver: Foundation models for creative writing, 2024{\natexlab{m}}.
\newblock URL \url{https://arxiv.org/abs/2401.17268}.

\bibitem[Wang et~al.(2024{\natexlab{n}})Wang, Yang, Zou, Liang, Xiang, Yang, Wang, and Li]{wang2024study}
Ting Wang, Chuan Yang, Maoyang Zou, Jiaying Liang, Dong Xiang, Wenjie Yang, Hongyang Wang, and Jia Li.
\newblock A study of extractive summarization of long documents incorporating local topic and hierarchical information.
\newblock \emph{Scientific Reports}, 14\penalty0 (1):\penalty0 10140, 2024{\natexlab{n}}.

\bibitem[Wang et~al.(2023{\natexlab{f}})Wang, Dong, Cheng, Liu, Yan, Gao, and Wei]{longmem}
Weizhi Wang, Li~Dong, Hao Cheng, Xiaodong Liu, Xifeng Yan, Jianfeng Gao, and Furu Wei.
\newblock Augmenting language models with long-term memory.
\newblock In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, editors, \emph{Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023}, 2023{\natexlab{f}}.
\newblock URL \url{http://papers.nips.cc/paper\_files/paper/2023/hash/ebd82705f44793b6f9ade5a669d0f0bf-Abstract-Conference.html}.

\bibitem[Wang et~al.(2024{\natexlab{o}})Wang, Song, Chen, Zhang, and Wang]{wang-2024-arxiv-LongLLaVA}
Xidong Wang, Dingjie Song, Shunian Chen, Chen Zhang, and Benyou Wang.
\newblock Longllava: Scaling multi-modal llms to 1000 images efficiently via hybrid architecture.
\newblock \emph{CoRR}, abs/2409.02889, 2024{\natexlab{o}}.
\newblock \doi{10.48550/ARXIV.2409.02889}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2409.02889}.

\bibitem[Wang et~al.(2024{\natexlab{p}})Wang, Salmani, Omidi, Ren, Rezagholizadeh, and Eshaghi]{wang2024limitssurveytechniquesextend}
Xindi Wang, Mahsa Salmani, Parsa Omidi, Xiangyu Ren, Mehdi Rezagholizadeh, and Armaghan Eshaghi.
\newblock Beyond the limits: {A} survey of techniques to extend the context length in large language models.
\newblock In \emph{Proceedings of the Thirty-Third International Joint Conference on Artificial Intelligence, {IJCAI} 2024, Jeju, South Korea, August 3-9, 2024}, pages 8299--8307. ijcai.org, 2024{\natexlab{p}}.
\newblock URL \url{https://www.ijcai.org/proceedings/2024/917}.

\bibitem[Wang et~al.(2024{\natexlab{q}})Wang, Li, Song, Xu, Tang, Zhuge, Pan, Song, Li, Singh, et~al.]{wang2024openhands_}
Xingyao Wang, Boxuan Li, Yufan Song, Frank~F Xu, Xiangru Tang, Mingchen Zhuge, Jiayi Pan, Yueqi Song, Bowen Li, Jaskirat Singh, et~al.
\newblock Openhands: An open platform for ai software developers as generalist agents.
\newblock In \emph{The Thirteenth International Conference on Learning Representations}, 2024{\natexlab{q}}.

\bibitem[Wang et~al.(2024{\natexlab{r}})Wang, Wang, Wang, Guo, Chen, Grundy, Liu, Ma, Mao, Zhang, et~al.]{wang2024repotransbench}
Yanli Wang, Yanlin Wang, Suiquan Wang, Daya Guo, Jiachi Chen, John Grundy, Xilin Liu, Yuchi Ma, Mingzhi Mao, Hongyu Zhang, et~al.
\newblock Repotransbench: A real-world benchmark for repository-level code translation.
\newblock \emph{arXiv preprint arXiv:2412.17744}, 2024{\natexlab{r}}.

\bibitem[Wang et~al.(2024{\natexlab{s}})Wang, Gao, Chen, Jiang, Li, Yang, Yin, Li, Li, Yin, Shang, and McAuley]{memoryllm}
Yu~Wang, Yifan Gao, Xiusi Chen, Haoming Jiang, Shiyang Li, Jingfeng Yang, Qingyu Yin, Zheng Li, Xian Li, Bing Yin, Jingbo Shang, and Julian~J. McAuley.
\newblock {MEMORYLLM:} towards self-updatable large language models.
\newblock In \emph{Forty-first International Conference on Machine Learning, {ICML} 2024, Vienna, Austria, July 21-27, 2024}. OpenReview.net, 2024{\natexlab{s}}.
\newblock URL \url{https://openreview.net/forum?id=p0lKWzdikQ}.

\bibitem[Wang et~al.(2023{\natexlab{g}})Wang, Peng, Que, Liu, Zhou, Wu, Guo, Gan, Ni, Zhang, Zhang, Ouyang, Xu, Chen, Fu, and Peng]{wang2023rolellm}
Zekun~Moore Wang, Zhongyuan Peng, Haoran Que, Jiaheng Liu, Wangchunshu Zhou, Yuhan Wu, Hongcheng Guo, Ruitong Gan, Zehao Ni, Man Zhang, Zhaoxiang Zhang, Wanli Ouyang, Ke~Xu, Wenhu Chen, Jie Fu, and Junran Peng.
\newblock Rolellm: Benchmarking, eliciting, and enhancing role-playing abilities of large language models.
\newblock \emph{arXiv preprint arXiv: 2310.00746}, 2023{\natexlab{g}}.

\bibitem[Wang et~al.(2024{\natexlab{t}})Wang, Yu, Stengel{-}Eskin, Yoon, Cheng, Bertasius, and Bansal]{wang-2024-arxiv-videotree}
Ziyang Wang, Shoubin Yu, Elias Stengel{-}Eskin, Jaehong Yoon, Feng Cheng, Gedas Bertasius, and Mohit Bansal.
\newblock Videotree: Adaptive tree-based video representation for {LLM} reasoning on long videos.
\newblock \emph{CoRR}, abs/2405.19209, 2024{\natexlab{t}}.
\newblock \doi{10.48550/ARXIV.2405.19209}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2405.19209}.

\bibitem[Warner et~al.(2024)Warner, Chaffin, Clavié, Weller, Hallström, Taghadouini, Gallagher, Biswas, Ladhak, Aarsen, Cooper, Adams, Howard, and Poli]{warner2024smarter0}
Benjamin Warner, Antoine Chaffin, Benjamin Clavié, Orion Weller, Oskar Hallström, Said Taghadouini, Alexis Gallagher, Raja Biswas, Faisal Ladhak, Tom Aarsen, Nathan Cooper, Griffin Adams, Jeremy Howard, and Iacopo Poli.
\newblock Smarter, better, faster, longer: A modern bidirectional encoder for fast, memory efficient, and long context finetuning and inference.
\newblock \emph{arXiv preprint arXiv: 2412.13663}, 2024.
\newblock URL \url{https://arxiv.org/abs/2412.13663}.

\bibitem[Wei et~al.(2024)Wei, Yang, Song, Lu, Hu, Tran, Peng, Liu, Huang, Du, et~al.]{wei2024long}
Jerry Wei, Chengrun Yang, Xinying Song, Yifeng Lu, Nathan Hu, Dustin Tran, Daiyi Peng, Ruibo Liu, Da~Huang, Cosmo Du, et~al.
\newblock Long-form factuality in large language models.
\newblock \emph{arXiv preprint arXiv:2403.18802}, 2024.

\bibitem[Weng et~al.(2024)Weng, Han, He, Chang, and Zhuang]{weng2024longvlm}
Yuetian Weng, Mingfei Han, Haoyu He, Xiaojun Chang, and Bohan Zhuang.
\newblock Longvlm: Efficient long video understanding via large language models.
\newblock In \emph{European Conference on Computer Vision}, pages 453--470. Springer, 2024.

\bibitem[Weston et~al.(2015)Weston, Bordes, Chopra, Rush, Van~Merri{\"e}nboer, Joulin, and Mikolov]{weston2015towards}
Jason Weston, Antoine Bordes, Sumit Chopra, Alexander~M Rush, Bart Van~Merri{\"e}nboer, Armand Joulin, and Tomas Mikolov.
\newblock Towards ai-complete question answering: A set of prerequisite toy tasks.
\newblock \emph{arXiv preprint arXiv:1502.05698}, 2015.

\bibitem[wiki(2023)]{print_press}
wiki.
\newblock {Printing press}, 2023.
\newblock URL \url{https://en.wikipedia.org/wiki/Printing_press}.

\bibitem[wiki(2024{\natexlab{a}})]{alex_library}
wiki.
\newblock {Library of Alexandria}, 2024{\natexlab{a}}.
\newblock URL \url{https://en.wikipedia.org/wiki/Library_of_Alexandria}.

\bibitem[wiki(2024{\natexlab{b}})]{tang_astronomer}
wiki.
\newblock {Chinese astronomy}, 2024{\natexlab{b}}.
\newblock URL \url{https://en.wikipedia.org/wiki/Chinese_astronomy}.

\bibitem[Wingate et~al.(2022)Wingate, Shoeybi, and Sorensen]{wingate-etal-2022-prompt}
David Wingate, Mohammad Shoeybi, and Taylor Sorensen.
\newblock Prompt compression and contrastive conditioning for controllability and toxicity reduction in language models.
\newblock In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, \emph{Findings of the Association for Computational Linguistics: EMNLP 2022}, pages 5621--5634, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2022.findings-emnlp.412}.
\newblock URL \url{https://aclanthology.org/2022.findings-emnlp.412/}.

\bibitem[Wiseman et~al.(2017)Wiseman, Shieber, and Rush]{wiseman2017challenges}
Sam Wiseman, Stuart~M Shieber, and Alexander~M Rush.
\newblock Challenges in data-to-document generation.
\newblock \emph{arXiv preprint arXiv:1707.08052}, 2017.

\bibitem[Wu et~al.(2021)Wu, Wu, Qi, and Huang]{hi-transformer}
Chuhan Wu, Fangzhao Wu, Tao Qi, and Yongfeng Huang.
\newblock Hi-transformer: Hierarchical interactive transformer for efficient and effective long document modeling.
\newblock In \emph{{ACL/IJCNLP} {(2)}}, pages 848--853. Association for Computational Linguistics, 2021.

\bibitem[Wu et~al.(2024{\natexlab{a}})Wu, Wang, Yu, Zhang, Chang, and Yu]{wu2024longmemeval}
Di~Wu, Hongwei Wang, Wenhao Yu, Yuwei Zhang, Kai-Wei Chang, and Dong Yu.
\newblock Longmemeval: Benchmarking chat assistants on long-term interactive memory.
\newblock \emph{arXiv preprint arXiv:2410.10813}, 2024{\natexlab{a}}.
\newblock URL \url{https://arxiv.org/abs/2410.10813}.

\bibitem[Wu et~al.(2023)Wu, Zhan, Tan, Hou, Liang, and Song]{wu2023vcsum}
Han Wu, Mingjie Zhan, Haochen Tan, Zhaohui Hou, Ding Liang, and Linqi Song.
\newblock Vcsum: A versatile chinese meeting summarization dataset.
\newblock In \emph{Findings of the Association for Computational Linguistics: ACL 2023}, pages 6065--6079, 2023.

\bibitem[Wu et~al.(2024{\natexlab{b}})Wu, Li, Chen, and Li]{wu-2024-arxiv-longvideobench}
Haoning Wu, Dongxu Li, Bei Chen, and Junnan Li.
\newblock Longvideobench: {A} benchmark for long-context interleaved video-language understanding.
\newblock \emph{CoRR}, abs/2407.15754, 2024{\natexlab{b}}.
\newblock \doi{10.48550/ARXIV.2407.15754}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2407.15754}.

\bibitem[Wu et~al.(2025{\natexlab{a}})Wu, Zhu, Zhao, Yu, Ran, Wong, Sun, and Li]{wu2025longattn}
Longyun Wu, Dawei Zhu, Guangxiang Zhao, Zhuocheng Yu, Junfeng Ran, Xiangyu Wong, Lin Sun, and Sujian Li.
\newblock Longattn: Selecting long-context training data via token-level attention.
\newblock \emph{arXiv preprint arXiv:2502.16860}, 2025{\natexlab{a}}.

\bibitem[Wu et~al.(2020)Wu, Lan, Gu, and Yu]{Memformer}
Qingyang Wu, Zhenzhong Lan, Jing Gu, and Zhou Yu.
\newblock Memformer: The memory-augmented transformer.
\newblock \emph{CoRR}, abs/2010.06891, 2020.
\newblock URL \url{https://arxiv.org/abs/2010.06891}.

\bibitem[Wu et~al.()Wu, Zhao, and Zheng]{wuefficient}
Tong Wu, Yanpeng Zhao, and Zilong Zheng.
\newblock An efficient recipe for long context extension via middle-focused positional encoding.
\newblock In \emph{The Thirty-eighth Annual Conference on Neural Information Processing Systems}.

\bibitem[Wu et~al.(2024{\natexlab{c}})Wu, Wang, Fu, Yue, Zhu, and Li]{wu2024long}
Wenhao Wu, Yizhong Wang, Yao Fu, Xiang Yue, Dawei Zhu, and Sujian Li.
\newblock Long context alignment with short instructions and synthesized positions.
\newblock \emph{arXiv preprint arXiv:2405.03939}, 2024{\natexlab{c}}.

\bibitem[Wu et~al.(2025{\natexlab{b}})Wu, Wang, Xiao, Peng, and Fu]{wu2025retrieval_head}
Wenhao Wu, Yizhong Wang, Guangxuan Xiao, Hao Peng, and Yao Fu.
\newblock Retrieval head mechanistically explains long-context factuality.
\newblock In \emph{The Thirteenth International Conference on Learning Representations}, 2025{\natexlab{b}}.
\newblock URL \url{https://openreview.net/forum?id=EytBpUGB1Z}.

\bibitem[Wu et~al.(2024{\natexlab{d}})Wu, Yang, Chai, Zhang, Liu, Du, Liang, Shu, Cheng, Sun, et~al.]{tablebench}
Xianjie Wu, Jian Yang, Linzheng Chai, Ge~Zhang, Jiaheng Liu, Xinrun Du, Di~Liang, Daixin Shu, Xianfu Cheng, Tianzhen Sun, et~al.
\newblock Tablebench: A comprehensive and complex benchmark for table question answering.
\newblock \emph{arXiv preprint arXiv:2408.09174}, 2024{\natexlab{d}}.

\bibitem[Wu et~al.(2024{\natexlab{e}})Wu, Wang, Liu, Shi, Yan, Lu, Zhu, and Zhang]{wu2024lifbench}
Xiaodong Wu, Minhao Wang, Yichen Liu, Xiaoming Shi, He~Yan, Xiangju Lu, Junmin Zhu, and Wei Zhang.
\newblock Lifbench: Evaluating the instruction following performance and stability of large language models in long-context scenarios.
\newblock \emph{arXiv preprint arXiv:2411.07037}, 2024{\natexlab{e}}.

\bibitem[Wu et~al.(2024{\natexlab{f}})Wu, Hee, Hu, and Lee]{wu2024longgenbench}
Yuhao Wu, Ming~Shan Hee, Zhiqing Hu, and Roy Ka-Wei Lee.
\newblock Longgenbench: Benchmarking long-form generation in long context llms.
\newblock \emph{arXiv preprint arXiv:2409.02076}, 2024{\natexlab{f}}.

\bibitem[Wu et~al.(2022)Wu, Rabe, Hutchins, and Szegedy]{memorizingtransformer}
Yuhuai Wu, Markus~Norman Rabe, DeLesley Hutchins, and Christian Szegedy.
\newblock Memorizing transformers.
\newblock In \emph{{ICLR}}. OpenReview.net, 2022.

\bibitem[Wu et~al.(2025{\natexlab{c}})Wu, Wang, Du, Jegelka, and Wang]{wu2025more}
Yuyang Wu, Yifei Wang, Tianqi Du, Stefanie Jegelka, and Yisen Wang.
\newblock When more is less: Understanding chain-of-thought length in llms.
\newblock \emph{arXiv preprint arXiv:2502.07266}, 2025{\natexlab{c}}.

\bibitem[Xi et~al.(2024)Xi, Cai, Zhu, Lu, Keutzer, Chen, and Han]{xi2024coat}
Haocheng Xi, Han Cai, Ligeng Zhu, Yao Lu, Kurt Keutzer, Jianfei Chen, and Song Han.
\newblock Coat: Compressing optimizer states and activation for memory-efficient fp8 training.
\newblock \emph{arXiv preprint arXiv:2410.19313}, 2024.

\bibitem[Xi et~al.(2023{\natexlab{a}})Xi, Chen, Guo, He, Ding, Hong, Zhang, Wang, Jin, Zhou, Zheng, Fan, Wang, Xiong, Zhou, Wang, Jiang, Zou, Liu, Yin, Dou, Weng, Cheng, Zhang, Qin, Zheng, Qiu, Huang, and Gui]{DBLP:journals/corr/abs-2309-07864}
Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, Rui Zheng, Xiaoran Fan, Xiao Wang, Limao Xiong, Yuhao Zhou, Weiran Wang, Changhao Jiang, Yicheng Zou, Xiangyang Liu, Zhangyue Yin, Shihan Dou, Rongxiang Weng, Wensen Cheng, Qi~Zhang, Wenjuan Qin, Yongyan Zheng, Xipeng Qiu, Xuanjing Huang, and Tao Gui.
\newblock The rise and potential of large language model based agents: {A} survey.
\newblock \emph{CoRR}, abs/2309.07864, 2023{\natexlab{a}}.
\newblock \doi{10.48550/ARXIV.2309.07864}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2309.07864}.

\bibitem[Xi et~al.(2023{\natexlab{b}})Xi, Chen, Guo, He, Ding, Hong, Zhang, Wang, Jin, Zhou, Zheng, Fan, Wang, Xiong, Zhou, Wang, Jiang, Zou, Liu, Yin, Dou, Weng, Cheng, Zhang, Qin, Zheng, Qiu, Huang, and Gui]{agent-survey-2}
Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, Rui Zheng, Xiaoran Fan, Xiao Wang, Limao Xiong, Yuhao Zhou, Weiran Wang, Changhao Jiang, Yicheng Zou, Xiangyang Liu, Zhangyue Yin, Shihan Dou, Rongxiang Weng, Wensen Cheng, Qi~Zhang, Wenjuan Qin, Yongyan Zheng, Xipeng Qiu, Xuanjing Huang, and Tao Gui.
\newblock The rise and potential of large language model based agents: A survey.
\newblock \emph{arXiv preprint arXiv: 2309.07864}, 2023{\natexlab{b}}.

\bibitem[Xia et~al.(2024{\natexlab{a}})Xia, Deng, Dunn, and Zhang]{xia2024agentless}
Chunqiu~Steven Xia, Yinlin Deng, Soren Dunn, and Lingming Zhang.
\newblock Agentless: Demystifying llm-based software engineering agents.
\newblock \emph{CoRR}, abs/2407.01489, 2024{\natexlab{a}}.
\newblock \doi{10.48550/ARXIV.2407.01489}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2407.01489}.

\bibitem[Xia et~al.(2024{\natexlab{b}})Xia, Yang, Dong, Wang, Li, Ge, Liu, Li, and Sui]{xia2024unlocking}
Heming Xia, Zhe Yang, Qingxiu Dong, Peiyi Wang, Yongqi Li, Tao Ge, Tianyu Liu, Wenjie Li, and Zhifang Sui.
\newblock Unlocking efficiency in large language model inference: A comprehensive survey of speculative decoding.
\newblock In \emph{ACL (Findings)}, 2024{\natexlab{b}}.

\bibitem[Xia et~al.(2025)Xia, Li, Leong, Wang, and Li]{xia2025tokenskip}
Heming Xia, Yongqi Li, Chak~Tou Leong, Wenjie Wang, and Wenjie Li.
\newblock Tokenskip: Controllable chain-of-thought compression in llms.
\newblock \emph{arXiv preprint arXiv:2502.12067}, 2025.

\bibitem[Xia et~al.(2024{\natexlab{c}})Xia, Malladi, Gururangan, Arora, and Chen]{xia2024less}
Mengzhou Xia, Sadhika Malladi, Suchin Gururangan, Sanjeev Arora, and Danqi Chen.
\newblock {LESS}: Selecting influential data for targeted instruction tuning.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2024{\natexlab{c}}.

\bibitem[Xiao et~al.(2023)Xiao, Lin, Seznec, Wu, Demouth, and Han]{xiao2023smoothquant}
Guangxuan Xiao, Ji~Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han.
\newblock Smoothquant: Accurate and efficient post-training quantization for large language models.
\newblock In \emph{International Conference on Machine Learning}, pages 38087--38099. PMLR, 2023.

\bibitem[Xiao et~al.(2024{\natexlab{a}})Xiao, Tang, Zuo, Guo, Yang, Tang, Fu, and Han]{xiao2024duoattentionefficientlongcontextllm}
Guangxuan Xiao, Jiaming Tang, Jingwei Zuo, Junxian Guo, Shang Yang, Haotian Tang, Yao Fu, and Song Han.
\newblock Duoattention: Efficient long-context llm inference with retrieval and streaming heads, 2024{\natexlab{a}}.
\newblock URL \url{https://arxiv.org/abs/2410.10819}.

\bibitem[Xiao et~al.(2024{\natexlab{b}})Xiao, Tian, Chen, Han, and Lewis]{attn_sink}
Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis.
\newblock Efficient streaming language models with attention sinks.
\newblock In \emph{The Twelfth International Conference on Learning Representations}, 2024{\natexlab{b}}.
\newblock URL \url{https://openreview.net/forum?id=NG7sS51zVF}.

\bibitem[Xiao et~al.(2024{\natexlab{c}})Xiao, Tian, Chen, Han, and Lewis]{xiao2024efficientstreaminglanguagemodels}
Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis.
\newblock Efficient streaming language models with attention sinks.
\newblock In \emph{{ICLR}}. OpenReview.net, 2024{\natexlab{c}}.

\bibitem[Xie et~al.(2024)Xie, Zhang, Chen, Li, Zhao, Cao, Hua, Cheng, Shin, Lei, Liu, Xu, Zhou, Savarese, Xiong, Zhong, and Yu]{xie2024osworld}
Tianbao Xie, Danyang Zhang, Jixuan Chen, Xiaochuan Li, Siheng Zhao, Ruisheng Cao, Toh~Jing Hua, Zhoujun Cheng, Dongchan Shin, Fangyu Lei, Yitao Liu, Yiheng Xu, Shuyan Zhou, Silvio Savarese, Caiming Xiong, Victor Zhong, and Tao Yu.
\newblock Osworld: Benchmarking multimodal agents for open-ended tasks in real computer environments.
\newblock In Amir Globersons, Lester Mackey, Danielle Belgrave, Angela Fan, Ulrich Paquet, Jakub~M. Tomczak, and Cheng Zhang, editors, \emph{Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024}, 2024.
\newblock URL \url{http://papers.nips.cc/paper\_files/paper/2024/hash/5d413e48f84dc61244b6be550f1cd8f5-Abstract-Datasets\_and\_Benchmarks\_Track.html}.

\bibitem[Xiong et~al.(2023)Xiong, Liu, Molybog, Zhang, Bhargava, Hou, Martin, Rungta, Sankararaman, Oguz, et~al.]{xiong2023effective}
Wenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajjwal Bhargava, Rui Hou, Louis Martin, Rashi Rungta, Karthik~Abinav Sankararaman, Barlas Oguz, et~al.
\newblock Effective long-context scaling of foundation models.
\newblock \emph{arXiv preprint arXiv:2309.16039}, 2023.

\bibitem[Xiong et~al.(2024{\natexlab{a}})Xiong, Liu, Molybog, Zhang, Bhargava, Hou, Martin, Rungta, Sankararaman, Oguz, Khabsa, Fang, Mehdad, Narang, Malik, Fan, Bhosale, Edunov, Lewis, Wang, and Ma]{xiong-etal-2024-effective}
Wenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajjwal Bhargava, Rui Hou, Louis Martin, Rashi Rungta, Karthik~Abinav Sankararaman, Barlas Oguz, Madian Khabsa, Han Fang, Yashar Mehdad, Sharan Narang, Kshitiz Malik, Angela Fan, Shruti Bhosale, Sergey Edunov, Mike Lewis, Sinong Wang, and Hao Ma.
\newblock Effective long-context scaling of foundation models.
\newblock In Kevin Duh, Helena Gomez, and Steven Bethard, editors, \emph{Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)}, pages 4643--4663, Mexico City, Mexico, June 2024{\natexlab{a}}. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2024.naacl-long.260}.
\newblock URL \url{https://aclanthology.org/2024.naacl-long.260}.

\bibitem[Xiong et~al.(2021)Xiong, Zeng, Chakraborty, Tan, Fung, Li, and Singh]{xiong2021nystromformernystrombasedalgorithmapproximating}
Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn Fung, Yin Li, and Vikas Singh.
\newblock Nystr{\"{o}}mformer: {A} nystr{\"{o}}m-based algorithm for approximating self-attention.
\newblock In \emph{{AAAI}}, pages 14138--14148. {AAAI} Press, 2021.

\bibitem[Xiong et~al.(2024{\natexlab{b}})Xiong, Papageorgiou, Lee, and Papailiopoulos]{xiong2024artificial}
Zheyang Xiong, Vasilis Papageorgiou, Kangwook Lee, and Dimitris Papailiopoulos.
\newblock From artificial needles to real haystacks: Improving retrieval capabilities in llms by finetuning on synthetic data.
\newblock \emph{arXiv preprint arXiv:2406.19292}, 2024{\natexlab{b}}.

\bibitem[Xu et~al.(2023{\natexlab{a}})Xu, Shi, and Choi]{recomp}
Fangyuan Xu, Weijia Shi, and Eunsol Choi.
\newblock Recomp: Improving retrieval-augmented lms with compression and selective augmentation.
\newblock \emph{arXiv preprint arXiv: 2310.04408}, 2023{\natexlab{a}}.

\bibitem[Xu et~al.(2023{\natexlab{b}})Xu, Song, Iyyer, and Choi]{xu2023critical}
Fangyuan Xu, Yixiao Song, Mohit Iyyer, and Eunsol Choi.
\newblock A critical evaluation of evaluations for long-form question answering.
\newblock \emph{arXiv preprint arXiv:2305.18201}, 2023{\natexlab{b}}.

\bibitem[Xu et~al.(2024{\natexlab{a}})Xu, Zhang, Guo, Hu, Liu, Wu, Feng, Sun, Shao, Guo, et~al.]{xu2024vtensor}
Jiale Xu, Rui Zhang, Cong Guo, Weiming Hu, Zihan Liu, Feiyang Wu, Yu~Feng, Shixuan Sun, Changxu Shao, Yuhong Guo, et~al.
\newblock vtensor: Flexible virtual tensor management for efficient llm serving.
\newblock \emph{arXiv preprint arXiv:2407.15309}, 2024{\natexlab{a}}.

\bibitem[Xu et~al.(2024{\natexlab{b}})Xu, Ping, Wu, Liu, Shoeybi, and Catanzaro]{Xu2024ChatQA2B}
Peng Xu, Wei Ping, Xianchao Wu, Zihan Liu, Mohammad Shoeybi, and Bryan Catanzaro.
\newblock Chatqa 2: Bridging the gap to proprietary llms in long context and rag capabilities.
\newblock \emph{ArXiv}, abs/2407.14482, 2024{\natexlab{b}}.
\newblock URL \url{https://api.semanticscholar.org/CorpusID:271310244}.

\bibitem[Xu et~al.(2024{\natexlab{c}})Xu, Ye, and Ren]{xu2024stress}
Xiaoyue Xu, Qinyuan Ye, and Xiang Ren.
\newblock Stress-testing long-context language models with lifelong icl and task haystack.
\newblock In \emph{The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track}, 2024{\natexlab{c}}.

\bibitem[Xu et~al.(2024{\natexlab{d}})Xu, Ye, Liu, Sun, Liu, Guo, Li, Liu, Huang, and Qiu]{xu2024detectiveqa}
Zhe Xu, Jiasheng Ye, Xiangyang Liu, Tianxiang Sun, Xiaoran Liu, Qipeng Guo, Linlin Li, Qun Liu, Xuanjing Huang, and Xipeng Qiu.
\newblock Detectiveqa: Evaluating long-context reasoning on detective novels.
\newblock \emph{arXiv preprint arXiv:2409.02465}, 2024{\natexlab{d}}.

\bibitem[XUANLEI et~al.(2024)XUANLEI, Jia, Zhou, Liu, Cheng, and You]{xuanlei2024hetegen}
ZHAO XUANLEI, Bin Jia, Haotian Zhou, Ziming Liu, Shenggan Cheng, and Yang You.
\newblock Hetegen: Efficient heterogeneous parallel inference for large language models on resource-constrained devices.
\newblock \emph{Proceedings of Machine Learning and Systems}, 6:\penalty0 162--172, 2024.

\bibitem[Xue et~al.(2024)Xue, Chen, Li, Hu, Zhu, Li, Fang, Tang, Yang, Liu, He, Yin, Molchanov, Kautz, Fan, Zhu, Lu, and Han]{xue-2024-arxiv-LongVILA}
Fuzhao Xue, Yukang Chen, Dacheng Li, Qinghao Hu, Ligeng Zhu, Xiuyu Li, Yunhao Fang, Haotian Tang, Shang Yang, Zhijian Liu, Ethan He, Hongxu Yin, Pavlo Molchanov, Jan Kautz, Linxi Fan, Yuke Zhu, Yao Lu, and Song Han.
\newblock Longvila: Scaling long-context visual language models for long videos.
\newblock \emph{CoRR}, abs/2408.10188, 2024.
\newblock \doi{10.48550/ARXIV.2408.10188}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2408.10188}.

\bibitem[Yang et~al.(2023)Yang, Xiao, Wang, Zhang, Bian, Yin, Lv, Pan, Wang, Yan, Yang, Deng, Wang, Liu, Ai, Dong, Zhao, Xu, Sun, Zhang, Liu, Ji, Xie, Dai, Fang, Su, Song, Liu, Ru, Ma, Wang, Liu, Lin, Nie, Guo, Sun, Tao, Li, Li, Cheng, Chen, Zeng, Wang, Chen, Men, Yu, Pan, Shen, Wang, Li, Jiang, Gao, Zhang, Zhou, and Wu]{Yang2023Baichuan2O}
Ai~Ming Yang, Bin Xiao, Bingning Wang, Borong Zhang, Ce~Bian, Chao Yin, Chenxu Lv, Da~Pan, Dian Wang, Dong Yan, Fan Yang, Fei Deng, Feng Wang, Feng Liu, Guangwei Ai, Guosheng Dong, Hai Zhao, Hang Xu, Hao-Lun Sun, Hongda Zhang, Hui Liu, Jiaming Ji, Jian Xie, Juntao Dai, Kuncheng Fang, Lei Su, Liang Song, Lifeng Liu, Liyun Ru, Luyao Ma, Mang Wang, Mickel Liu, Mingan Lin, Nuolan Nie, Pei Guo, Ruiyang Sun, Zhang Tao, Tianpeng Li, Tianyu Li, Wei Cheng, Weipeng Chen, Xiangrong Zeng, Xiaochuan Wang, Xiaoxi Chen, Xin Men, Xin Yu, Xuehai Pan, Yan-Bin Shen, Yiding Wang, Yiyu Li, Youxin Jiang, Yuchen Gao, Yupeng Zhang, Zenan Zhou, and Zhiying Wu.
\newblock Baichuan 2: Open large-scale language models.
\newblock \emph{ArXiv}, abs/2309.10305, 2023.

\bibitem[Yang et~al.(2025{\natexlab{a}})Yang, Yu, Li, Liu, Huang, Huang, Jiang, Tu, Zhang, Zhou, et~al.]{yang2025qwen2}
An~Yang, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoyan Huang, Jiandong Jiang, Jianhong Tu, Jianwei Zhang, Jingren Zhou, et~al.
\newblock Qwen2. 5-1m technical report.
\newblock \emph{arXiv preprint arXiv:2501.15383}, 2025{\natexlab{a}}.

\bibitem[Yang et~al.(2025{\natexlab{b}})Yang, Venkitesh, Talupuru, Lin, Cairuz, Blunsom, and Locatelli]{yang_hybrid_attn_rope_nope}
Bowen Yang, Bharat Venkitesh, Dwarak Talupuru, Hangyu Lin, David Cairuz, Phil Blunsom, and Acyr Locatelli.
\newblock Rope to nope and back again: A new hybrid attention strategy, 2025{\natexlab{b}}.
\newblock URL \url{https://arxiv.org/abs/2501.18795}.

\bibitem[Yang et~al.(2024{\natexlab{a}})Yang, Han, Gao, Hu, Zhang, and Zhao]{yang-etal-2024-pyramidinfer}
Dongjie Yang, Xiaodong Han, Yan Gao, Yao Hu, Shilin Zhang, and Hai Zhao.
\newblock {P}yramid{I}nfer: Pyramid {KV} cache compression for high-throughput {LLM} inference.
\newblock In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, \emph{Findings of the Association for Computational Linguistics: ACL 2024}, pages 3258--3270, Bangkok, Thailand, August 2024{\natexlab{a}}. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2024.findings-acl.195}.
\newblock URL \url{https://aclanthology.org/2024.findings-acl.195/}.

\bibitem[Yang et~al.(2024{\natexlab{b}})Yang, Yang, Jin, Miao, Zhang, Yang, Cui, Zhang, Hui, and Lin]{codearena}
Jian Yang, Jiaxi Yang, Ke~Jin, Yibo Miao, Lei Zhang, Liqun Yang, Zeyu Cui, Yichang Zhang, Binyuan Hui, and Junyang Lin.
\newblock Evaluating and aligning codellms on human preference.
\newblock \emph{arXiv preprint arXiv:2412.05210}, 2024{\natexlab{b}}.

\bibitem[Yang et~al.(2024{\natexlab{c}})Yang, Zhang, Yang, Jin, Zhang, Peng, Deng, Miao, Liu, Cui, et~al.]{execrepobench}
Jian Yang, Jiajun Zhang, Jiaxi Yang, Ke~Jin, Lei Zhang, Qiyao Peng, Ken Deng, Yibo Miao, Tianyu Liu, Zeyu Cui, et~al.
\newblock Execrepobench: Multi-level executable code completion evaluation.
\newblock \emph{arXiv preprint arXiv:2412.11990}, 2024{\natexlab{c}}.

\bibitem[Yang et~al.(2024{\natexlab{d}})Yang, Jimenez, Wettig, Lieret, Yao, Narasimhan, and Press]{DBLP:conf/nips/YangJWLYNP24}
John Yang, Carlos~E. Jimenez, Alexander Wettig, Kilian Lieret, Shunyu Yao, Karthik Narasimhan, and Ofir Press.
\newblock Swe-agent: Agent-computer interfaces enable automated software engineering.
\newblock In Amir Globersons, Lester Mackey, Danielle Belgrave, Angela Fan, Ulrich Paquet, Jakub~M. Tomczak, and Cheng Zhang, editors, \emph{Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024}, 2024{\natexlab{d}}.
\newblock URL \url{http://papers.nips.cc/paper\_files/paper/2024/hash/5a7c947568c1b1328ccc5230172e1e7c-Abstract-Conference.html}.

\bibitem[Yang et~al.(2022)Yang, Tian, Peng, and Klein]{re3}
Kevin Yang, Yuandong Tian, Nanyun Peng, and Dan Klein.
\newblock Re3: Generating longer stories with recursive reprompting and revision.
\newblock In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, \emph{Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, {EMNLP} 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022}, pages 4393--4479. Association for Computational Linguistics, 2022.
\newblock \doi{10.18653/V1/2022.EMNLP-MAIN.296}.
\newblock URL \url{https://doi.org/10.18653/v1/2022.emnlp-main.296}.

\bibitem[Yang et~al.(2024{\natexlab{e}})Yang, Zhang, Chen, Li, and Jia]{yang2024tidaldecodefastaccuratellm}
Lijie Yang, Zhihao Zhang, Zhuofu Chen, Zikun Li, and Zhihao Jia.
\newblock Tidaldecode: Fast and accurate llm decoding with position persistent sparse attention, 2024{\natexlab{e}}.
\newblock URL \url{https://arxiv.org/abs/2410.05076}.

\bibitem[Yang et~al.(2024{\natexlab{f}})Yang, Chen, Tian, Wang, Li, Yu, and Jia]{yang-2024-arxiv-visionzip}
Senqiao Yang, Yukang Chen, Zhuotao Tian, Chengyao Wang, Jingyao Li, Bei Yu, and Jiaya Jia.
\newblock Visionzip: Longer is better but not necessary in vision language models.
\newblock \emph{CoRR}, abs/2412.04467, 2024{\natexlab{f}}.
\newblock \doi{10.48550/ARXIV.2412.04467}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2412.04467}.

\bibitem[Yang et~al.(2025{\natexlab{c}})Yang, Ma, Lin, and Wei]{yang2025towards}
Wenkai Yang, Shuming Ma, Yankai Lin, and Furu Wei.
\newblock Towards thinking-optimal scaling of test-time compute for llm reasoning.
\newblock \emph{arXiv preprint arXiv:2502.18080}, 2025{\natexlab{c}}.

\bibitem[Yang et~al.(2015)Yang, Yih, and Meek]{yang2015wikiqa}
Yi~Yang, Wen-tau Yih, and Christopher Meek.
\newblock Wikiqa: A challenge dataset for open-domain question answering.
\newblock In \emph{Proceedings of the 2015 conference on empirical methods in natural language processing}, pages 2013--2018, 2015.

\bibitem[Yang et~al.(2024{\natexlab{g}})Yang, Chen, Yu, Shen, and Gan]{yang-2024-arxiv-vca}
Zeyuan Yang, Delin Chen, Xueyang Yu, Maohao Shen, and Chuang Gan.
\newblock Vca: Video curious agent for long video understanding.
\newblock \emph{CoRR}, abs/2412.10471, 2024{\natexlab{g}}.
\newblock \doi{10.48550/ARXIV.2412.10471}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2412.10471}.

\bibitem[Yang et~al.(2018)Yang, Qi, Zhang, Bengio, Cohen, Salakhutdinov, and Manning]{yang2018hotpotqa}
Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher~D Manning.
\newblock Hotpotqa: A dataset for diverse, explainable multi-hop question answering.
\newblock In \emph{Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing}, pages 2369--2380, 2018.

\bibitem[Yang et~al.(2016)Yang, Yang, Dyer, He, Smola, and Hovy]{han}
Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He, Alexander~J. Smola, and Eduard~H. Hovy.
\newblock Hierarchical attention networks for document classification.
\newblock In \emph{{HLT-NAACL}}, pages 1480--1489. The Association for Computational Linguistics, 2016.

\bibitem[Yao et~al.(2023)Yao, Zhao, Yu, Du, Shafran, Narasimhan, and Cao]{DBLP:conf/iclr/YaoZYDSN023}
Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik~R. Narasimhan, and Yuan Cao.
\newblock React: Synergizing reasoning and acting in language models.
\newblock In \emph{The Eleventh International Conference on Learning Representations, {ICLR} 2023, Kigali, Rwanda, May 1-5, 2023}. OpenReview.net, 2023.
\newblock URL \url{https://openreview.net/forum?id=WE\_vluYUL-X}.

\bibitem[Yao et~al.(2022)Yao, Yazdani~Aminabadi, Zhang, Wu, Li, and He]{yao2022zeroquant}
Zhewei Yao, Reza Yazdani~Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong Li, and Yuxiong He.
\newblock Zeroquant: Efficient and affordable post-training quantization for large-scale transformers.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 27168--27183, 2022.

\bibitem[Ye et~al.(2024{\natexlab{a}})Ye, Xu, Liu, Hu, Yan, Qian, Zhang, Huang, and Zhou]{ye-2024-arxiv-mPLUG-Owl3}
Jiabo Ye, Haiyang Xu, Haowei Liu, Anwen Hu, Ming Yan, Qi~Qian, Ji~Zhang, Fei Huang, and Jingren Zhou.
\newblock mplug-owl3: Towards long image-sequence understanding in multi-modal large language models.
\newblock \emph{CoRR}, abs/2408.04840, 2024{\natexlab{a}}.
\newblock \doi{10.48550/ARXIV.2408.04840}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2408.04840}.

\bibitem[Ye et~al.(2024{\natexlab{b}})Ye, Liu, Sun, Zhou, Zhan, and Qiu]{ye2024data}
Jiasheng Ye, Peiju Liu, Tianxiang Sun, Yunhua Zhou, Jun Zhan, and Xipeng Qiu.
\newblock Data mixing laws: Optimizing data mixtures by predicting language modeling performance.
\newblock \emph{arXiv preprint arXiv:2403.16952}, 2024{\natexlab{b}}.

\bibitem[Ye et~al.(2024{\natexlab{c}})Ye, Tao, Huang, and Li]{ye2024chunkattention}
Lu~Ye, Ze~Tao, Yong Huang, and Yang Li.
\newblock Chunkattention: Efficient self-attention with prefix-aware kv cache and two-phase partition.
\newblock In \emph{Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pages 11608--11620, 2024{\natexlab{c}}.

\bibitem[Ye et~al.(2025)Ye, Yin, He, Zhang, Yen, Gao, Durrett, and Chen]{ye2025longproc}
Xi~Ye, Fangcong Yin, Yinghui He, Joie Zhang, Howard Yen, Tianyu Gao, Greg Durrett, and Danqi Chen.
\newblock Longproc: Benchmarking long-context language models on long procedural generation.
\newblock \emph{arXiv preprint arXiv:2501.05414}, 2025.

\bibitem[Yen et~al.(2024)Yen, Gao, Hou, Ding, Fleischer, Izsak, Wasserblat, and Chen]{yen2024helmet}
Howard Yen, Tianyu Gao, Minmin Hou, Ke~Ding, Daniel Fleischer, Peter Izsak, Moshe Wasserblat, and Danqi Chen.
\newblock Helmet: How to evaluate long-context language models effectively and thoroughly.
\newblock \emph{arXiv preprint arXiv:2410.02694}, 2024.

\bibitem[Yin et~al.(2024)Yin, Fu, Zhao, Shen, Ge, Yang, Long, Dai, Xu, Sun, He, Shan, and Chen]{DBLP:journals/corr/abs-2411-19951}
Shukang Yin, Chaoyou Fu, Sirui Zhao, Yunhang Shen, Chunjiang Ge, Yan Yang, Zuwei Long, Yuhan Dai, Tong Xu, Xing Sun, Ran He, Caifeng Shan, and Enhong Chen.
\newblock T2vid: Translating long text into multi-image is the catalyst for video-llms.
\newblock \emph{CoRR}, abs/2411.19951, 2024.
\newblock \doi{10.48550/ARXIV.2411.19951}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2411.19951}.

\bibitem[Yoon et~al.(2024)Yoon, Lee, Hwang, Jeong, and Kang]{yoon-etal-2024-compact}
Chanwoong Yoon, Taewhoo Lee, Hyeon Hwang, Minbyul Jeong, and Jaewoo Kang.
\newblock {C}omp{A}ct: Compressing retrieved documents actively for question answering.
\newblock In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen, editors, \emph{Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing}, pages 21424--21439, Miami, Florida, USA, November 2024. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2024.emnlp-main.1194}.
\newblock URL \url{https://aclanthology.org/2024.emnlp-main.1194/}.

\bibitem[Yu et~al.(2024{\natexlab{a}})Yu, Wang, Shao, Zhu, Zhou, and Jiang]{yu2024twinpilots}
Chengye Yu, Tianyu Wang, Zili Shao, Linjie Zhu, Xu~Zhou, and Song Jiang.
\newblock Twinpilots: A new computing paradigm for gpu-cpu parallel llm inference.
\newblock In \emph{Proceedings of the 17th ACM International Systems and Storage Conference}, pages 91--103, 2024{\natexlab{a}}.

\bibitem[Yu et~al.(2023)Yu, Wang, Zhang, and Bi]{trams}
Haofei Yu, Cunxiang Wang, Yue Zhang, and Wei Bi.
\newblock {TRAMS:} training-free memory selection for long-range language modeling.
\newblock In \emph{{EMNLP} (Findings)}, pages 4966--4972. Association for Computational Linguistics, 2023.

\bibitem[Yu et~al.(2024{\natexlab{b}})Yu, Jin, Wang, Chen, Jin, Zuo, Xu, Sun, Zhang, Wu, Zhang, and Sun]{yu-2024-arxiv-framevoyager}
Sicheng Yu, Chengkai Jin, Huanyu Wang, Zhenghao Chen, Sheng Jin, Zhongrong Zuo, Xiaolei Xu, Zhenbang Sun, Bingni Zhang, Jiawei Wu, Hao Zhang, and Qianru Sun.
\newblock Frame-voyager: Learning to query frames for video large language models.
\newblock \emph{CoRR}, abs/2410.03226, 2024{\natexlab{b}}.
\newblock \doi{10.48550/ARXIV.2410.03226}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2410.03226}.

\bibitem[Yu et~al.(2024{\natexlab{c}})Yu, Xu, and Akkiraju]{yu2024defense}
Tan Yu, Anbang Xu, and Rama Akkiraju.
\newblock In defense of {RAG} in the era of long-context language models.
\newblock \emph{CoRR}, abs/2409.01666, 2024{\natexlab{c}}.
\newblock \doi{10.48550/ARXIV.2409.01666}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2409.01666}.

\bibitem[Yu et~al.(2024{\natexlab{d}})Yu, Gupta, Gopalswamy, Mamidala, Zhou, Huynh, Park, Diamant, Deoras, and Huan]{yu2024collage}
Tao Yu, Gaurav Gupta, Karthick Gopalswamy, Amith Mamidala, Hao Zhou, Jeffrey Huynh, Youngsuk Park, Ron Diamant, Anoop Deoras, and Luke Huan.
\newblock Collage: Light-weight low-precision strategy for llm training.
\newblock \emph{arXiv preprint arXiv:2405.03637}, 2024{\natexlab{d}}.

\bibitem[Yuan et~al.(2022)Yuan, Coenen, Reif, and Ippolito]{yuan2022wordcraft}
Ann Yuan, Andy Coenen, Emily Reif, and Daphne Ippolito.
\newblock Wordcraft: story writing with large language models.
\newblock In \emph{Proceedings of the 27th International Conference on Intelligent User Interfaces}, pages 841--852, 2022.

\bibitem[Yuan et~al.(2025{\natexlab{a}})Yuan, Liu, Li, Zhang, Wang, Cai, and Zhao]{yuan2025remambaequipmambaeffective}
Danlong Yuan, Jiahao Liu, Bei Li, Huishuai Zhang, Jingang Wang, Xunliang Cai, and Dongyan Zhao.
\newblock Remamba: Equip mamba with effective long-sequence modeling, 2025{\natexlab{a}}.
\newblock URL \url{https://arxiv.org/abs/2408.15496}.

\bibitem[Yuan et~al.(2025{\natexlab{b}})Yuan, Gao, Dai, Luo, Zhao, Zhang, Xie, Wei, Wang, Xiao, Wang, Ruan, Zhang, Liang, and Zeng]{yuan2025nativesparseattentionhardwarealigned}
Jingyang Yuan, Huazuo Gao, Damai Dai, Junyu Luo, Liang Zhao, Zhengyan Zhang, Zhenda Xie, Y.~X. Wei, Lean Wang, Zhiping Xiao, Yuqing Wang, Chong Ruan, Ming Zhang, Wenfeng Liang, and Wangding Zeng.
\newblock Native sparse attention: Hardware-aligned and natively trainable sparse attention, 2025{\natexlab{b}}.
\newblock URL \url{https://arxiv.org/abs/2502.11089}.

\bibitem[Yuan et~al.(2025{\natexlab{c}})Yuan, Gao, Dai, Luo, Zhao, Zhang, Xie, Wei, Wang, Xiao, et~al.]{yuan2025native}
Jingyang Yuan, Huazuo Gao, Damai Dai, Junyu Luo, Liang Zhao, Zhengyan Zhang, Zhenda Xie, YX~Wei, Lean Wang, Zhiping Xiao, et~al.
\newblock Native sparse attention: Hardware-aligned and natively trainable sparse attention.
\newblock \emph{arXiv preprint arXiv:2502.11089}, 2025{\natexlab{c}}.

\bibitem[Yuan et~al.(2024)Yuan, Ning, Zhou, Yang, Li, Zhuang, Tan, Yao, Lin, Li, Dai, Yan, and Wang]{yuan2024lveval}
Tao Yuan, Xuefei Ning, Dong Zhou, Zhijie Yang, Shiyao Li, Minghui Zhuang, Zheyue Tan, Zhuyu Yao, Dahua Lin, Boxun Li, Guohao Dai, Shengen Yan, and Yu~Wang.
\newblock Lv-eval: A balanced long-context benchmark with 5 length levels up to 256k, 2024.

\bibitem[Yue et~al.(2024)Yue, Yuan, Duanmu, Zhou, Wu, and Nie]{yue2024wkvquant}
Yuxuan Yue, Zhihang Yuan, Haojie Duanmu, Sifan Zhou, Jianlong Wu, and Liqiang Nie.
\newblock Wkvquant: Quantizing weight and key/value cache for large language models gains more.
\newblock \emph{arXiv preprint arXiv:2402.12065}, 2024.

\bibitem[Zaheer et~al.(2020)Zaheer, Guruganesh, Dubey, Ainslie, Alberti, Onta{\~{n}}{\'{o}}n, Pham, Ravula, Wang, Yang, and Ahmed]{zaheer2020big}
Manzil Zaheer, Guru Guruganesh, Kumar~Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Onta{\~{n}}{\'{o}}n, Philip Pham, Anirudh Ravula, Qifan Wang, Li~Yang, and Amr Ahmed.
\newblock Big bird: Transformers for longer sequences.
\newblock In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria{-}Florina Balcan, and Hsuan{-}Tien Lin, editors, \emph{Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual}, 2020.
\newblock URL \url{https://proceedings.neurips.cc/paper/2020/hash/c8512d142a2d849725f31a9a7a361ab9-Abstract.html}.

\bibitem[Zeng et~al.(2022)Zeng, Wong, Welker, Choromanski, Tombari, Purohit, Ryoo, Sindhwani, Lee, Vanhoucke, and Florence]{zeng2022socratic}
Andy Zeng, Adrian~S. Wong, Stefan Welker, K.~Choromanski, F.~Tombari, Aveek Purohit, M.~Ryoo, Vikas Sindhwani, Johnny Lee, Vincent Vanhoucke, and Peter~R. Florence.
\newblock Socratic models: Composing zero-shot multimodal reasoning with language.
\newblock \emph{International Conference on Learning Representations}, 2022.
\newblock \doi{10.48550/arXiv.2204.00598}.

\bibitem[Zeng et~al.(2025)Zeng, Cheng, Yin, Zhou, and Qiu]{zeng2025revisiting}
Zhiyuan Zeng, Qinyuan Cheng, Zhangyue Yin, Yunhua Zhou, and Xipeng Qiu.
\newblock Revisiting the test-time scaling of o1-like models: Do they truly possess test-time scaling capabilities?
\newblock \emph{arXiv preprint arXiv:2502.12215}, 2025.

\bibitem[Zhang et~al.(2025{\natexlab{a}})Zhang, Dong, Liu, Zhang, Wang, Yang, Zhang, Liu, Peng, Tan, Zhang, Wang, Wang, He, Deng, Zhou, Huang, and Zhang]{Zhang2025CodeCriticBenchAH}
Alexander Zhang, Marcus Dong, Jiaheng Liu, Wei Zhang, Yejie Wang, Jian Yang, Ge~Zhang, Tianyu Liu, Zhongyuan Peng, Yingshui Tan, Yuanxing Zhang, Zhexu Wang, Weixun Wang, Yancheng He, Ken Deng, Wangchunshu Zhou, Wenhao Huang, and Zhaoxiang Zhang.
\newblock Codecriticbench: A holistic code critique benchmark for large language models.
\newblock 2025{\natexlab{a}}.

\bibitem[Zhang et~al.(2023{\natexlab{a}})Zhang, Chen, Zhang, Keung, Liu, Zan, Mao, Lou, and Chen]{zhang2023repocoder}
Fengji Zhang, Bei Chen, Yue Zhang, Jacky Keung, Jin Liu, Daoguang Zan, Yi~Mao, Jian{-}Guang Lou, and Weizhu Chen.
\newblock Repocoder: Repository-level code completion through iterative retrieval and generation.
\newblock In Houda Bouamor, Juan Pino, and Kalika Bali, editors, \emph{Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, {EMNLP} 2023, Singapore, December 6-10, 2023}, pages 2471--2484. Association for Computational Linguistics, 2023{\natexlab{a}}.
\newblock \doi{10.18653/V1/2023.EMNLP-MAIN.151}.
\newblock URL \url{https://doi.org/10.18653/v1/2023.emnlp-main.151}.

\bibitem[Zhang et~al.(2024{\natexlab{a}})Zhang, Qu, Liu, Zhang, Lin, Yu, Pan, Cheng, Liu, Lin, et~al.]{zhang2024mapneo}
Ge~Zhang, Scott Qu, Jiaheng Liu, Chenchen Zhang, Chenghua Lin, Chou~Leuang Yu, Danny Pan, Esther Cheng, Jie Liu, Qunshu Lin, et~al.
\newblock Map-neo: Highly capable and transparent bilingual large language model series.
\newblock \emph{arXiv preprint arXiv:2405.19327}, 2024{\natexlab{a}}.

\bibitem[Zhang et~al.(2018)Zhang, Luan, Sun, Zhai, Xu, Zhang, and Liu]{zhang2018improving}
Jiacheng Zhang, Huanbo Luan, Maosong Sun, Feifei Zhai, Jingfang Xu, Min Zhang, and Yang Liu.
\newblock Improving the transformer translation model with document-level context.
\newblock In Ellen Riloff, David Chiang, Julia Hockenmaier, and Jun'ichi Tsujii, editors, \emph{Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, October 31 - November 4, 2018}, pages 533--542. Association for Computational Linguistics, 2018.
\newblock \doi{10.18653/V1/D18-1049}.
\newblock URL \url{https://doi.org/10.18653/v1/d18-1049}.

\bibitem[Zhang et~al.(2024{\natexlab{b}})Zhang, Bai, Lv, Gu, Liu, Zou, Cao, Hou, Dong, Feng, et~al.]{zhang2024longcite}
Jiajie Zhang, Yushi Bai, Xin Lv, Wanjun Gu, Danqing Liu, Minhao Zou, Shulin Cao, Lei Hou, Yuxiao Dong, Ling Feng, et~al.
\newblock Longcite: Enabling llms to generate fine-grained citations in long-context qa.
\newblock \emph{arXiv preprint arXiv:2409.02897}, 2024{\natexlab{b}}.

\bibitem[Zhang et~al.(2024{\natexlab{c}})Zhang, Hou, Lv, Cao, Hou, Niu, Hou, Dong, Feng, and Li]{zhang2024longreward}
Jiajie Zhang, Zhongni Hou, Xin Lv, Shulin Cao, Zhenyu Hou, Yilin Niu, Lei Hou, Yuxiao Dong, Ling Feng, and Juanzi Li.
\newblock Longreward: Improving long-context large language models with ai feedback.
\newblock \emph{arXiv preprint arXiv:2410.21252}, 2024{\natexlab{c}}.

\bibitem[Zhang et~al.(2020)Zhang, Zhao, Saleh, and Liu]{zhang2020pegasus}
Jingqing Zhang, Yao Zhao, Mohammad Saleh, and Peter~J. Liu.
\newblock {PEGASUS:} pre-training with extracted gap-sentences for abstractive summarization.
\newblock In \emph{Proceedings of the 37th International Conference on Machine Learning, {ICML} 2020, 13-18 July 2020, Virtual Event}, volume 119 of \emph{Proceedings of Machine Learning Research}, pages 11328--11339. {PMLR}, 2020.
\newblock URL \url{http://proceedings.mlr.press/v119/zhang20ae.html}.

\bibitem[Zhang et~al.(2025{\natexlab{b}})Zhang, Zhu, Sun, Luo, Qiao, Du, Zheng, Chen, and Zhang]{zhang2025lightthinker}
Jintian Zhang, Yuqi Zhu, Mengshu Sun, Yujie Luo, Shuofei Qiao, Lun Du, Da~Zheng, Huajun Chen, and Ningyu Zhang.
\newblock Lightthinker: Thinking step-by-step compression.
\newblock \emph{arXiv preprint arXiv:2502.15589}, 2025{\natexlab{b}}.

\bibitem[Zhang et~al.(2024{\natexlab{d}})Zhang, Wang, Li, Shou, Chen, Chen, and Mehrotra]{zhang-etal-2024-draft}
Jun Zhang, Jue Wang, Huan Li, Lidan Shou, Ke~Chen, Gang Chen, and Sharad Mehrotra.
\newblock Draft{\&} verify: Lossless large language model acceleration via self-speculative decoding.
\newblock In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, \emph{Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, August 2024{\natexlab{d}}.

\bibitem[Zhang et~al.(2024{\natexlab{e}})Zhang, Li, Zhang, and Jin]{zhang2024hirope}
Kechi Zhang, Ge~Li, Huangzhao Zhang, and Zhi Jin.
\newblock Hirope: Length extrapolation for code models using hierarchical position.
\newblock \emph{arXiv preprint arXiv:2403.19115}, 2024{\natexlab{e}}.

\bibitem[Zhang et~al.(2023{\natexlab{b}})Zhang, Li, Liu, Liu, Chen, Luo, Yang, et~al.]{zhang2023marathon}
Lei Zhang, Yunshui Li, Ziqiang Liu, Junhao Liu, Longze Chen, Run Luo, Min Yang, et~al.
\newblock Marathon: A race through the realm of long context with large language models.
\newblock \emph{arXiv preprint arXiv:2312.09542}, 2023{\natexlab{b}}.

\bibitem[Zhang et~al.(2024{\natexlab{f}})Zhang, Dong, Zang, Cao, Qian, Chen, Guo, Duan, Wang, Ouyang, Zhang, Zhang, Li, Gao, Sun, Zhang, Li, Li, Wang, Yan, He, Zhang, Chen, Dai, Qiao, Lin, and Wang]{zhang-2024-arxiv-intenlmxcomposer25}
Pan Zhang, Xiaoyi Dong, Yuhang Zang, Yuhang Cao, Rui Qian, Lin Chen, Qipeng Guo, Haodong Duan, Bin Wang, Linke Ouyang, Songyang Zhang, Wenwei Zhang, Yining Li, Yang Gao, Peng Sun, Xinyue Zhang, Wei Li, Jingwen Li, Wenhai Wang, Hang Yan, Conghui He, Xingcheng Zhang, Kai Chen, Jifeng Dai, Yu~Qiao, Dahua Lin, and Jiaqi Wang.
\newblock Internlm-xcomposer-2.5: {A} versatile large vision language model supporting long-contextual input and output.
\newblock \emph{CoRR}, abs/2407.03320, 2024{\natexlab{f}}.
\newblock \doi{10.48550/ARXIV.2407.03320}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2407.03320}.

\bibitem[Zhang et~al.(2025{\natexlab{c}})Zhang, Liu, Xiao, Shao, Ye, and Dou]{zhang2025long}
Peitian Zhang, Zheng Liu, Shitao Xiao, Ninglu Shao, Qiwei Ye, and Zhicheng Dou.
\newblock Long context compression with activation beacon.
\newblock In \emph{The Thirteenth International Conference on Learning Representations}, 2025{\natexlab{c}}.
\newblock URL \url{https://openreview.net/forum?id=1eQT9OzfNQ}.

\bibitem[Zhang et~al.(2024{\natexlab{g}})Zhang, Zhang, Pang, Zheng, and Zheng]{zhang2024adacompextractivecontextcompression}
Qianchi Zhang, Hainan Zhang, Liang Pang, Hongwei Zheng, and Zhiming Zheng.
\newblock Adacomp: Extractive context compression with adaptive predictor for retrieval-augmented large language models, 2024{\natexlab{g}}.
\newblock URL \url{https://arxiv.org/abs/2409.01579}.

\bibitem[Zhang et~al.(2022)Zhang, Roller, Goyal, Artetxe, Chen, Chen, Dewan, Diab, Li, Lin, Mihaylov, Ott, Shleifer, Shuster, Simig, Koura, Sridhar, Wang, and Zettlemoyer]{zhang2022opt_model_pretrained}
Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi~Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit~Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer.
\newblock Opt: Open pre-trained transformer language models, 2022.
\newblock URL \url{https://arxiv.org/abs/2205.01068}.

\bibitem[Zhang et~al.(2019{\natexlab{a}})Zhang, Kishore, Wu, Weinberger, and Artzi]{zhang2019bertscore}
Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian~Q Weinberger, and Yoav Artzi.
\newblock Bertscore: Evaluating text generation with bert.
\newblock \emph{arXiv preprint arXiv:1904.09675}, 2019{\natexlab{a}}.

\bibitem[Zhang et~al.(2024{\natexlab{h}})Zhang, Yi, Xu, and Shrivastava]{zhang2024kv}
Tianyi Zhang, Jonah Yi, Zhaozhuo Xu, and Anshumali Shrivastava.
\newblock Kv cache is 1 bit per channel: Efficient large language model inference with coupled quantization.
\newblock \emph{Advances in Neural Information Processing Systems}, 37:\penalty0 3304--3331, 2024{\natexlab{h}}.

\bibitem[Zhang et~al.(2024{\natexlab{i}})Zhang, Wan, Zhang, Cheung, Tian, Shen, and Ye]{arith_head_icml}
Wei Zhang, Chaoqun Wan, Yonggang Zhang, Yiu-Ming Cheung, Xinmei Tian, Xu~Shen, and Jieping Ye.
\newblock Interpreting and improving large language models in arithmetic calculation.
\newblock In Ruslan Salakhutdinov, Zico Kolter, Katherine Heller, Adrian Weller, Nuria Oliver, Jonathan Scarlett, and Felix Berkenkamp, editors, \emph{Proceedings of the 41st International Conference on Machine Learning}, volume 235 of \emph{Proceedings of Machine Learning Research}, pages 59932--59950. PMLR, 21--27 Jul 2024{\natexlab{i}}.
\newblock URL \url{https://proceedings.mlr.press/v235/zhang24bk.html}.

\bibitem[Zhang et~al.(2024{\natexlab{j}})Zhang, Zhang, Ma, Li, Zhang, Li, Yao, Xu, Zhou, Zhang-Li, et~al.]{zhang2024tablellm}
Xiaokang Zhang, Jing Zhang, Zeyao Ma, Yang Li, Bohan Zhang, Guanlin Li, Zijun Yao, Kangli Xu, Jinchang Zhou, Daniel Zhang-Li, et~al.
\newblock Tablellm: Enabling tabular data manipulation by llms in real office usage scenarios.
\newblock \emph{arXiv preprint arXiv:2403.19318}, 2024{\natexlab{j}}.

\bibitem[Zhang et~al.(2019{\natexlab{b}})Zhang, Wei, and Zhou]{zhang2019hibertdocumentlevelpretraining}
Xingxing Zhang, Furu Wei, and Ming Zhou.
\newblock {HIBERT:} document level pre-training of hierarchical bidirectional transformers for document summarization.
\newblock In Anna Korhonen, David~R. Traum, and Llu{\'{\i}}s M{\`{a}}rquez, editors, \emph{Proceedings of the 57th Conference of the Association for Computational Linguistics, {ACL} 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers}, pages 5059--5069. Association for Computational Linguistics, 2019{\natexlab{b}}.
\newblock \doi{10.18653/V1/P19-1499}.
\newblock URL \url{https://doi.org/10.18653/v1/p19-1499}.

\bibitem[Zhang et~al.(2024{\natexlab{k}})Zhang, Chen, Hu, Xu, Chen, Hao, Han, Thai, Wang, Liu, et~al.]{zhang2024bench}
Xinrong Zhang, Yingfa Chen, Shengding Hu, Zihang Xu, Junhao Chen, Moo Hao, Xu~Han, Zhen Thai, Shuo Wang, Zhiyuan Liu, et~al.
\newblock Extending long context evaluation beyond 100k tokens.
\newblock In \emph{Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pages 15262--15277, 2024{\natexlab{k}}.

\bibitem[Zhang et~al.(2025{\natexlab{d}})Zhang, Zhang, Du, Du, Pang, Gao, and Lin]{zhang2025lighttransfer}
Xuan Zhang, Fengzhuo Zhang, Cunxiao Du, Chao Du, Tianyu Pang, Wei Gao, and Min Lin.
\newblock Lighttransfer: Your long-context {LLM} is secretly a hybrid model with effortless adaptation.
\newblock In \emph{Workshop on Reasoning and Planning for Large Language Models}, 2025{\natexlab{d}}.
\newblock URL \url{https://openreview.net/forum?id=DfgfGTfObm}.

\bibitem[Zhang et~al.(2024{\natexlab{l}})Zhang, Ruan, Fan, and Roychoudhury]{DBLP:conf/issta/0002RFR24}
Yuntong Zhang, Haifeng Ruan, Zhiyu Fan, and Abhik Roychoudhury.
\newblock Autocoderover: Autonomous program improvement.
\newblock In Maria Christakis and Michael Pradel, editors, \emph{Proceedings of the 33rd {ACM} {SIGSOFT} International Symposium on Software Testing and Analysis, {ISSTA} 2024, Vienna, Austria, September 16-20, 2024}, pages 1592--1604. {ACM}, 2024{\natexlab{l}}.
\newblock \doi{10.1145/3650212.3680384}.
\newblock URL \url{https://doi.org/10.1145/3650212.3680384}.

\bibitem[Zhang et~al.(2024{\natexlab{m}})Zhang, Sun, Chen, Pfister, Zhang, and Arik]{chain-of-agent}
Yusen Zhang, Ruoxi Sun, Yanfei Chen, Tomas Pfister, Rui Zhang, and Sercan~Ö. Arik.
\newblock Chain of agents: Large language models collaborating on long-context tasks.
\newblock \emph{arXiv preprint arXiv: 2406.02818}, 2024{\natexlab{m}}.

\bibitem[Zhang et~al.(2024{\natexlab{n}})Zhang, Wu, Yang, Shu, Xiao, Kong, and Sang]{zhang2024o1}
Yuxiang Zhang, Shangxi Wu, Yuqi Yang, Jiangming Shu, Jinlin Xiao, Chao Kong, and Jitao Sang.
\newblock o1-coder: an o1 replication for coding.
\newblock \emph{arXiv preprint arXiv:2412.00154}, 2024{\natexlab{n}}.

\bibitem[Zhang et~al.(2023{\natexlab{c}})Zhang, Sheng, Zhou, Chen, Zheng, Cai, Song, Tian, R\'{e}, Barrett, Wang, and Chen]{NEURIPS2023_6ceefa7b}
Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher R\'{e}, Clark Barrett, Zhangyang~"Atlas" Wang, and Beidi Chen.
\newblock H2o: Heavy-hitter oracle for efficient generative inference of large language models.
\newblock In A.~Oh, T.~Naumann, A.~Globerson, K.~Saenko, M.~Hardt, and S.~Levine, editors, \emph{Advances in Neural Information Processing Systems}, volume~36, pages 34661--34710. Curran Associates, Inc., 2023{\natexlab{c}}.
\newblock URL \url{https://proceedings.neurips.cc/paper_files/paper/2023/file/6ceefa7b15572587b78ecfcebb2827f8-Paper-Conference.pdf}.

\bibitem[Zhang et~al.(2024{\natexlab{o}})Zhang, Chen, Liu, Yao, Ruwase, Chen, Wu, and Wang]{zhang2024found}
Zhenyu Zhang, Runjin Chen, Shiwei Liu, Zhewei Yao, Olatunji Ruwase, Beidi Chen, Xiaoxia Wu, and Zhangyang Wang.
\newblock Found in the middle: How language models use long contexts better via plug-and-play positional encoding.
\newblock \emph{arXiv preprint arXiv:2403.04797}, 2024{\natexlab{o}}.

\bibitem[Zhang et~al.(2024{\natexlab{p}})Zhang, Cao, Ye, Ma, Liao, and Chua]{zhang2024analyzing}
Zhihan Zhang, Yixin Cao, Chenchen Ye, Yunshan Ma, Lizi Liao, and Tat-Seng Chua.
\newblock Analyzing temporal complex events with large language models? a benchmark towards temporal, long context understanding.
\newblock \emph{arXiv preprint arXiv:2406.02472}, 2024{\natexlab{p}}.

\bibitem[Zhao et~al.(2025)Zhao, Zhao, Li, and Xu]{deepgemm2025}
Chenggang Zhao, Liang Zhao, Jiashi Li, and Zhean Xu.
\newblock Deepgemm: clean and efficient fp8 gemm kernels with fine-grained scaling.
\newblock \url{https://github.com/deepseek-ai/DeepGEMM}, 2025.

\bibitem[Zhao et~al.(2023{\natexlab{a}})Zhao, Yang, Cheng, Tian, Ren, Xiao, Yuan, Chen, Liu, Zhang, et~al.]{zhao2023goldminer}
Hanyu Zhao, Zhi Yang, Yu~Cheng, Chao Tian, Shiru Ren, Wencong Xiao, Man Yuan, Langshi Chen, Kaibo Liu, Yang Zhang, et~al.
\newblock Goldminer: Elastic scaling of training data pre-processing pipelines for deep learning.
\newblock \emph{Proceedings of the ACM on Management of Data}, 1\penalty0 (2):\penalty0 1--25, 2023{\natexlab{a}}.

\bibitem[Zhao et~al.(2024{\natexlab{a}})Zhao, Zu, Xu, Lu, He, Ding, Gui, Zhang, and Huang]{longagent}
Jun Zhao, Can Zu, Hao Xu, Yi~Lu, Wei He, Yiwen Ding, Tao Gui, Qi~Zhang, and Xuanjing Huang.
\newblock Longagent: Scaling language models to 128k context through multi-agent collaboration.
\newblock \emph{arXiv preprint arXiv: 2402.11550}, 2024{\natexlab{a}}.

\bibitem[Zhao et~al.(2024{\natexlab{b}})Zhao, Feng, Feng, Zhong, Xu, Yang, Liu, Qin, and Liu]{zhao-etal-2024-length}
Liang Zhao, Xiachong Feng, Xiaocheng Feng, Weihong Zhong, Dongliang Xu, Qing Yang, Hongtao Liu, Bing Qin, and Ting Liu.
\newblock Length extrapolation of transformers: A survey from the perspective of positional encoding.
\newblock In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen, editors, \emph{Findings of the Association for Computational Linguistics: EMNLP 2024}, pages 9959--9977, Miami, Florida, USA, November 2024{\natexlab{b}}. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2024.findings-emnlp.582}.
\newblock URL \url{https://aclanthology.org/2024.findings-emnlp.582/}.

\bibitem[Zhao et~al.(2024{\natexlab{c}})Zhao, Wei, Zeng, Cheng, Yang, Cheng, Wang, Li, Wu, Zhu, et~al.]{zhao2024longskywork}
Liang Zhao, Tianwen Wei, Liang Zeng, Cheng Cheng, Liu Yang, Peng Cheng, Lijie Wang, Chenxia Li, Xuejie Wu, Bo~Zhu, et~al.
\newblock Longskywork: A training recipe for efficiently extending context length in large language models.
\newblock \emph{arXiv preprint arXiv:2406.00605}, 2024{\natexlab{c}}.

\bibitem[Zhao et~al.(2024{\natexlab{d}})Zhao, Zhang, Lee, Liu, Zhang, Fang, Liao, Jiang, Ma, and Xu]{zhao-2024-arxiv-OmChat}
Tiancheng Zhao, Qianqian Zhang, Kyusong Lee, Peng Liu, Lu~Zhang, Chunxin Fang, Jiajia Liao, Kelei Jiang, Yibo Ma, and Ruochen Xu.
\newblock Omchat: {A} recipe to train multimodal language models with strong long context and video understanding.
\newblock \emph{CoRR}, abs/2407.04923, 2024{\natexlab{d}}.
\newblock \doi{10.48550/ARXIV.2407.04923}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2407.04923}.

\bibitem[Zhao et~al.(2023{\natexlab{b}})Zhao, Gu, Varma, Luo, Huang, Xu, Wright, Shojanazeri, Ott, Shleifer, et~al.]{zhao2023pytorch}
Yanli Zhao, Andrew Gu, Rohan Varma, Liang Luo, Chien-Chin Huang, Min Xu, Less Wright, Hamid Shojanazeri, Myle Ott, Sam Shleifer, et~al.
\newblock Pytorch fsdp: experiences on scaling fully sharded data parallel.
\newblock \emph{arXiv preprint arXiv:2304.11277}, 2023{\natexlab{b}}.

\bibitem[Zhao et~al.(2024{\natexlab{e}})Zhao, Lin, Zhu, Ye, Chen, Zheng, Ceze, Krishnamurthy, Chen, and Kasikci]{zhao2024atom}
Yilong Zhao, Chien-Yu Lin, Kan Zhu, Zihao Ye, Lequn Chen, Size Zheng, Luis Ceze, Arvind Krishnamurthy, Tianqi Chen, and Baris Kasikci.
\newblock Atom: Low-bit quantization for efficient and accurate llm serving.
\newblock \emph{Proceedings of Machine Learning and Systems}, 6:\penalty0 196--209, 2024{\natexlab{e}}.

\bibitem[Zhao et~al.(2024{\natexlab{f}})Zhao, Yin, Zeng, Wang, Shi, Lyu, Wang, Luo, and Zhang]{zhao2024marco}
Yu~Zhao, Huifeng Yin, Bo~Zeng, Hao Wang, Tianqi Shi, Chenyang Lyu, Longyue Wang, Weihua Luo, and Kaifu Zhang.
\newblock Marco-o1: Towards open reasoning models for open-ended solutions.
\newblock \emph{arXiv preprint arXiv:2411.14405}, 2024{\natexlab{f}}.

\bibitem[Zheng et~al.(2024)Zheng, Gao, Shi, Xiong, Sun, Li, Huang, Ren, Ng, Jiang, et~al.]{zheng2024dape}
Chuanyang Zheng, Yihang Gao, Han Shi, Jing Xiong, Jiankai Sun, Jingyao Li, Minbin Huang, Xiaozhe Ren, Michael Ng, Xin Jiang, et~al.
\newblock Dape v2: Process attention score as feature map for length extrapolation.
\newblock \emph{arXiv preprint arXiv:2410.04798}, 2024.

\bibitem[Zheng et~al.(2025)Zheng, Gao, Shi, Huang, Li, Xiong, Ren, Ng, Jiang, Li, et~al.]{zheng2025dape}
Chuanyang Zheng, Yihang Gao, Han Shi, Minbin Huang, Jingyao Li, Jing Xiong, Xiaozhe Ren, Michael Ng, Xin Jiang, Zhenguo Li, et~al.
\newblock Dape: Data-adaptive positional encoding for length extrapolation.
\newblock \emph{Advances in Neural Information Processing Systems}, 37:\penalty0 26659--26700, 2025.

\bibitem[Zheng et~al.(2023{\natexlab{a}})Zheng, Chiang, Sheng, Zhuang, Wu, Zhuang, Lin, Li, Li, Xing, et~al.]{zheng2023judging}
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi~Lin, Zhuohan Li, Dacheng Li, Eric Xing, et~al.
\newblock Judging llm-as-a-judge with mt-bench and chatbot arena.
\newblock \emph{Advances in Neural Information Processing Systems}, 36:\penalty0 46595--46623, 2023{\natexlab{a}}.

\bibitem[Zheng et~al.(2023{\natexlab{b}})Zheng, Yin, Xie, Huang, Sun, Yu, Cao, Kozyrakis, Stoica, Gonzalez, et~al.]{zheng2023efficiently}
Lianmin Zheng, Liangsheng Yin, Zhiqiang Xie, Jeff Huang, Chuyue Sun, Cody\_Hao Yu, Shiyi Cao, Christos Kozyrakis, Ion Stoica, Joseph~E Gonzalez, et~al.
\newblock Efficiently programming large language models using sglang.
\newblock 2023{\natexlab{b}}.

\bibitem[Zhong et~al.(2021)Zhong, Yin, Yu, Zaidi, Mutuma, Jha, Hassan, Celikyilmaz, Liu, Qiu, et~al.]{zhong2021qmsum}
Ming Zhong, Da~Yin, Tao Yu, Ahmad Zaidi, Mutethia Mutuma, Rahul Jha, Ahmed Hassan, Asli Celikyilmaz, Yang Liu, Xipeng Qiu, et~al.
\newblock Qmsum: A new benchmark for query-based multi-domain meeting summarization.
\newblock In \emph{Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies}, pages 5905--5921, 2021.

\bibitem[Zhong et~al.(2024{\natexlab{a}})Zhong, Guo, Gao, Ye, and Wang]{DBLP:conf/aaai/ZhongGGYW24}
Wanjun Zhong, Lianghong Guo, Qiqi Gao, He~Ye, and Yanlin Wang.
\newblock Memorybank: Enhancing large language models with long-term memory.
\newblock In Michael~J. Wooldridge, Jennifer~G. Dy, and Sriraam Natarajan, editors, \emph{Thirty-Eighth {AAAI} Conference on Artificial Intelligence, {AAAI} 2024, Thirty-Sixth Conference on Innovative Applications of Artificial Intelligence, {IAAI} 2024, Fourteenth Symposium on Educational Advances in Artificial Intelligence, {EAAI} 2014, February 20-27, 2024, Vancouver, Canada}, pages 19724--19731. {AAAI} Press, 2024{\natexlab{a}}.
\newblock \doi{10.1609/AAAI.V38I17.29946}.
\newblock URL \url{https://doi.org/10.1609/aaai.v38i17.29946}.

\bibitem[Zhong et~al.(2024{\natexlab{b}})Zhong, Liu, Chen, Hu, Zhu, Liu, Jin, and Zhang]{zhong2024distserve}
Yinmin Zhong, Shengyu Liu, Junda Chen, Jianbo Hu, Yibo Zhu, Xuanzhe Liu, Xin Jin, and Hao Zhang.
\newblock $\{$DistServe$\}$: Disaggregating prefill and decoding for goodput-optimized large language model serving.
\newblock In \emph{18th USENIX Symposium on Operating Systems Design and Implementation (OSDI 24)}, pages 193--210, 2024{\natexlab{b}}.

\bibitem[Zhou et~al.(2024{\natexlab{a}})Zhou, Shu, Zhao, Wu, Xiao, Yang, Xiong, Zhang, Huang, and Liu]{zhou-2024-arxiv-mlvu}
Junjie Zhou, Yan Shu, Bo~Zhao, Boya Wu, Shitao Xiao, Xi~Yang, Yongping Xiong, Bo~Zhang, Tiejun Huang, and Zheng Liu.
\newblock {MLVU:} {A} comprehensive benchmark for multi-task long video understanding.
\newblock \emph{CoRR}, abs/2406.04264, 2024{\natexlab{a}}.
\newblock \doi{10.48550/ARXIV.2406.04264}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2406.04264}.

\bibitem[Zhou et~al.(2024{\natexlab{b}})Zhou, Xu, Zhu, Zhou, Lo, Sridhar, Cheng, Ou, Bisk, Fried, Alon, and Neubig]{zhou2023webarena}
Shuyan Zhou, Frank~F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan Bisk, Daniel Fried, Uri Alon, and Graham Neubig.
\newblock Webarena: {A} realistic web environment for building autonomous agents.
\newblock In \emph{The Twelfth International Conference on Learning Representations, {ICLR} 2024, Vienna, Austria, May 7-11, 2024}. OpenReview.net, 2024{\natexlab{b}}.
\newblock URL \url{https://openreview.net/forum?id=oKn9c6ytLx}.

\bibitem[Zhou et~al.(2023{\natexlab{a}})Zhou, Jiang, Cotterell, and Sachan]{zhou2023dynaicl}
Wangchunshu Zhou, Yuchen~Eleanor Jiang, Ryan Cotterell, and Mrinmaya Sachan.
\newblock Efficient prompting via dynamic in-context learning, 2023{\natexlab{a}}.
\newblock URL \url{https://arxiv.org/abs/2305.11170}.

\bibitem[Zhou et~al.(2023{\natexlab{b}})Zhou, Jiang, Cui, Wang, Xiao, Hou, Cotterell, and Sachan]{recurrentgpt}
Wangchunshu Zhou, Yuchen~Eleanor Jiang, Peng Cui, Tiannan Wang, Zhenxin Xiao, Yifan Hou, Ryan Cotterell, and Mrinmaya Sachan.
\newblock Recurrentgpt: Interactive generation of (arbitrarily) long text.
\newblock \emph{arXiv preprint arXiv: 2305.13304}, 2023{\natexlab{b}}.

\bibitem[Zhou et~al.(2023{\natexlab{c}})Zhou, Jiang, Li, Wu, Wang, Qiu, Zhang, Chen, Wu, Wang, Zhu, Chen, Zhang, Tang, Zhang, Chen, Cui, and Sachan]{zhou2023agents}
Wangchunshu Zhou, Yuchen~Eleanor Jiang, Long Li, Jialong Wu, Tiannan Wang, Shi Qiu, Jintian Zhang, Jing Chen, Ruipu Wu, Shuai Wang, Shiding Zhu, Jiyu Chen, Wentao Zhang, Xiangru Tang, Ningyu Zhang, Huajun Chen, Peng Cui, and Mrinmaya Sachan.
\newblock Agents: An open-source framework for autonomous language agents, 2023{\natexlab{c}}.
\newblock URL \url{https://arxiv.org/abs/2309.07870}.

\bibitem[Zhou et~al.(2023{\natexlab{d}})Zhou, Jiang, Li, Wu, Wang, Qiu, Zhang, Chen, Wu, Wang, Zhu, Chen, Zhang, Zhang, Chen, Cui, and Sachan]{DBLP:journals/corr/abs-2309-07870}
Wangchunshu Zhou, Yuchen~Eleanor Jiang, Long Li, Jialong Wu, Tiannan Wang, Shi Qiu, Jintian Zhang, Jing Chen, Ruipu Wu, Shuai Wang, Shiding Zhu, Jiyu Chen, Wentao Zhang, Ningyu Zhang, Huajun Chen, Peng Cui, and Mrinmaya Sachan.
\newblock Agents: An open-source framework for autonomous language agents.
\newblock \emph{CoRR}, abs/2309.07870, 2023{\natexlab{d}}.
\newblock \doi{10.48550/ARXIV.2309.07870}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2309.07870}.

\bibitem[Zhou et~al.(2024{\natexlab{c}})Zhou, Ou, Ding, Li, Wu, Wang, Chen, Wang, Xu, Zhang, Chen, and Jiang]{DBLP:journals/corr/abs-2406-18532}
Wangchunshu Zhou, Yixin Ou, Shengwei Ding, Long Li, Jialong Wu, Tiannan Wang, Jiamin Chen, Shuai Wang, Xiaohua Xu, Ningyu Zhang, Huajun Chen, and Yuchen~Eleanor Jiang.
\newblock Symbolic learning enables self-evolving agents.
\newblock \emph{CoRR}, abs/2406.18532, 2024{\natexlab{c}}.
\newblock \doi{10.48550/ARXIV.2406.18532}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2406.18532}.

\bibitem[Zhou et~al.(2025)Zhou, Wang, Zeng, Guo, Liu, Shen, Zhang, and Ding]{zhou2025dynamickvtaskawareadaptivekv}
Xiabin Zhou, Wenbin Wang, Minyan Zeng, Jiaxian Guo, Xuebo Liu, Li~Shen, Min Zhang, and Liang Ding.
\newblock Dynamickv: Task-aware adaptive kv cache compression for long context llms, 2025.
\newblock URL \url{https://arxiv.org/abs/2412.14838}.

\bibitem[Zhou et~al.(2024{\natexlab{d}})Zhou, Wang, Lin, and Chen]{zhou-etal-2024-num_sys}
Zhejian Zhou, JIayu Wang, Dahua Lin, and Kai Chen.
\newblock Scaling behavior for large language models regarding numeral systems: An example using pythia.
\newblock In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen, editors, \emph{Findings of the Association for Computational Linguistics: EMNLP 2024}, pages 3806--3820, Miami, Florida, USA, November 2024{\natexlab{d}}. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2024.findings-emnlp.218}.
\newblock URL \url{https://aclanthology.org/2024.findings-emnlp.218/}.

\bibitem[Zhu et~al.(2023{\natexlab{a}})Zhu, Yang, Wang, Song, Wu, Wei, and Li]{zhu2023pose}
Dawei Zhu, Nan Yang, Liang Wang, Yifan Song, Wenhao Wu, Furu Wei, and Sujian Li.
\newblock Pose: Efficient context window extension of llms via positional skip-wise training.
\newblock In \emph{The Twelfth International Conference on Learning Representations}, 2023{\natexlab{a}}.

\bibitem[Zhu et~al.(2024{\natexlab{a}})Zhu, Wang, Yang, Song, Wu, Wei, and Li]{DBLP:conf/emnlp/ZhuW0SWWL24}
Dawei Zhu, Liang Wang, Nan Yang, Yifan Song, Wenhao Wu, Furu Wei, and Sujian Li.
\newblock Longembed: Extending embedding models for long context retrieval.
\newblock In Yaser Al{-}Onaizan, Mohit Bansal, and Yun{-}Nung Chen, editors, \emph{Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, {EMNLP} 2024, Miami, FL, USA, November 12-16, 2024}, pages 802--816. Association for Computational Linguistics, 2024{\natexlab{a}}.
\newblock URL \url{https://aclanthology.org/2024.emnlp-main.47}.

\bibitem[Zhu et~al.(2024{\natexlab{b}})Zhu, Wang, Yang, Song, Wu, Wei, and Li]{zhu-etal-2024-longembed}
Dawei Zhu, Liang Wang, Nan Yang, Yifan Song, Wenhao Wu, Furu Wei, and Sujian Li.
\newblock {L}ong{E}mbed: Extending embedding models for long context retrieval.
\newblock In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen, editors, \emph{Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing}, pages 802--816, Miami, Florida, USA, November 2024{\natexlab{b}}. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2024.emnlp-main.47}.
\newblock URL \url{https://aclanthology.org/2024.emnlp-main.47}.

\bibitem[Zhu et~al.(2024{\natexlab{c}})Zhu, Xie, Liang, Zheng, and Guo]{zhu-2024-arxiv-focusllava}
Yuke Zhu, Chi Xie, Shuang Liang, Bo~Zheng, and Sheng Guo.
\newblock Focusllava: {A} coarse-to-fine approach for efficient and effective visual token compression.
\newblock \emph{CoRR}, abs/2411.14228, 2024{\natexlab{c}}.
\newblock \doi{10.48550/ARXIV.2411.14228}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2411.14228}.

\bibitem[Zhu et~al.(2023{\natexlab{b}})Zhu, Yuan, Wang, Liu, Liu, Deng, Dou, and Wen]{zhu2023large}
Yutao Zhu, Huaying Yuan, Shuting Wang, Jiongnan Liu, Wenhan Liu, Chenlong Deng, Zhicheng Dou, and Ji{-}Rong Wen.
\newblock Large language models for information retrieval: {A} survey.
\newblock \emph{CoRR}, abs/2308.07107, 2023{\natexlab{b}}.
\newblock \doi{10.48550/ARXIV.2308.07107}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2308.07107}.

\bibitem[Zhuge et~al.(2024)Zhuge, Wang, Kirsch, Faccio, Khizbullin, and Schmidhuber]{DBLP:conf/icml/ZhugeWKFKS24}
Mingchen Zhuge, Wenyi Wang, Louis Kirsch, Francesco Faccio, Dmitrii Khizbullin, and J{\"{u}}rgen Schmidhuber.
\newblock Gptswarm: Language agents as optimizable graphs.
\newblock In \emph{Forty-first International Conference on Machine Learning, {ICML} 2024, Vienna, Austria, July 21-27, 2024}. OpenReview.net, 2024.
\newblock URL \url{https://openreview.net/forum?id=uTC9AFXIhg}.

\bibitem[Zou et~al.(2025)Zou, Luo, Xie, Lv, Wang, Chen, Wang, Zhang, Zhang, et~al.]{zou-2025-arxiv-hlv1k}
Heqing Zou, Tianze Luo, Guiyang Xie, Fengmao Lv, Guangcong Wang, Junyang Chen, Zhuochen Wang, Hansheng Zhang, Huaijian Zhang, et~al.
\newblock Hlv-1k: A large-scale hour-long video benchmark for time-specific long video understanding.
\newblock \emph{CoRR}, abs/2501.01645, 2025.
\newblock \doi{10.48550/ARXIV.2501.01645}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2501.01645}.

\bibitem[Zou et~al.(2024)Zou, Khalifa, and Wang]{zou2024retrieval}
Kaijian Zou, Muhammad Khalifa, and Lu~Wang.
\newblock Retrieval or global context understanding? on many-shot in-context learning for long-context evaluation.
\newblock \emph{arXiv preprint arXiv:2411.07130}, 2024.

\end{thebibliography}
