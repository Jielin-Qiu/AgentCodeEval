@inproceedings{10.5555/3495724.3495883,
 address = {Red Hook, NY, USA},
 articleno = {159},
 author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
 booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
 isbn = {9781713829546},
 location = {Vancouver, BC, Canada},
 numpages = {25},
 publisher = {Curran Associates Inc.},
 series = {NIPS '20},
 title = {Language models are few-shot learners},
 year = {2020}
}
@article{chen2024generative,
  title   = {Generative Adapter: Contextualizing Language Models in Parameters with A Single Forward Pass},
  author  = {Tong Chen and Hao Fang and Patrick Xia and Xiaodong Liu and Benjamin Van Durme and Luke Zettlemoyer and Jianfeng Gao and Hao Cheng},
  year    = {2024},
  journal = {arXiv preprint arXiv: 2411.05877}
}
@article{10.5555/3648699.3648939,
 articleno = {240},
 author = {Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and Schuh, Parker and Shi, Kensen and Tsvyashchenko, Sashank and Maynez, Joshua and Rao, Abhishek and Barnes, Parker and Tay, Yi and Shazeer, Noam and Prabhakaran, Vinodkumar and Reif, Emily and Du, Nan and Hutchinson, Ben and Pope, Reiner and Bradbury, James and Austin, Jacob and Isard, Michael and Gur-Ari, Guy and Yin, Pengcheng and Duke, Toju and Levskaya, Anselm and Ghemawat, Sanjay and Dev, Sunipa and Michalewski, Henryk and Garcia, Xavier and Misra, Vedant and Robinson, Kevin and Fedus, Liam and Zhou, Denny and Ippolito, Daphne and Luan, David and Lim, Hyeontaek and Zoph, Barret and Spiridonov, Alexander and Sepassi, Ryan and Dohan, David and Agrawal, Shivani and Omernick, Mark and Dai, Andrew M. and Pillai, Thanumalayan Sankaranarayana and Pellat, Marie and Lewkowycz, Aitor and Moreira, Erica and Child, Rewon and Polozov, Oleksandr and Lee, Katherine and Zhou, Zongwei and Wang, Xuezhi and Saeta, Brennan and Diaz, Mark and Firat, Orhan and Catasta, Michele and Wei, Jason and Meier-Hellstern, Kathy and Eck, Douglas and Dean, Jeff and Petrov, Slav and Fiedel, Noah},
 issn = {1532-4435},
 issue_date = {January 2023},
 journal = {J. Mach. Learn. Res.},
 keywords = {large language models, few-shot learning, natural language processing, scalable deep learning},
 month = {March},
 number = {1},
 numpages = {113},
 publisher = {JMLR.org},
 title = {PaLM: scaling language modeling with pathways},
 volume = {24},
 year = {2024}
}

@inproceedings{10.5555/3666122.3667628,
 author = {Zhang, Zhenyu and Sheng, Ying and Zhou, Tianyi and Chen, Tianlong and Zheng, Lianmin and Cai, Ruisi and Song, Zhao and Tian, Yuandong and R\'{e}, Christopher and Barrett, Clark and Wang, Zhangyang and Chen, Beidi},
 booktitle = {Proceedings of the 37th International Conference on Neural Information Processing Systems},
 title = {H2O: heavy-hitter oracle for efficient generative inference of large language models},
 year = {2024}
}

@article{adams2024longhealth,
 author = {Adams, Lisa and Busch, Felix and Han, Tianyu and Excoffier, Jean-Baptiste and Ortala, Matthieu and L{\"o}ser, Alexander and Aerts, Hugo JWL and Kather, Jakob Nikolas and Truhn, Daniel and Bressem, Keno},
 journal = {arXiv preprint arXiv:2401.14490},
 title = {LongHealth: A Question Answering Benchmark with Long Clinical Documents},
 year = {2024}
}

@article{adaplanner,
 author = {Haotian Sun and Yuchen Zhuang and Lingkai Kong and Bo Dai and Chao Zhang},
 journal = {arXiv preprint arXiv: 2305.16653},
 title = {AdaPlanner: Adaptive Planning from Feedback with Language Models},
 year = {2023}
}

@article{mceval,
  title={McEval: Massively Multilingual Code Evaluation},
  author={Chai, Linzheng and Liu, Shukai and Yang, Jian and Yin, Yuwei and Jin, Ke and Liu, Jiaheng and Sun, Tao and Zhang, Ge and Ren, Changyu and Guo, Hongcheng and others},
  journal={arXiv preprint arXiv:2406.07436},
  year={2024}
}

@article{execrepobench,
  title={Execrepobench: Multi-level executable code completion evaluation},
  author={Yang, Jian and Zhang, Jiajun and Yang, Jiaxi and Jin, Ke and Zhang, Lei and Peng, Qiyao and Deng, Ken and Miao, Yibo and Liu, Tianyu and Cui, Zeyu and others},
  journal={arXiv preprint arXiv:2412.11990},
  year={2024}
}

@article{xcoder,
  title={Multi-Agent Collaboration for Multilingual Code Instruction Tuning},
  author={Yang, Jian and Zhang, Wei and Yang, Jiaxi and Miao, Yibo and Quan, Shanghaoran and Wu, Zhenhe and Peng, Qiyao and Yang, Liqun and Liu, Tianyu and Cui, Zeyu and others},
  journal={arXiv preprint arXiv:2502.07487},
  year={2025}
}

@article{codearena,
  title={Evaluating and aligning codellms on human preference},
  author={Yang, Jian and Yang, Jiaxi and Jin, Ke and Miao, Yibo and Zhang, Lei and Yang, Liqun and Cui, Zeyu and Zhang, Yichang and Hui, Binyuan and Lin, Junyang},
  journal={arXiv preprint arXiv:2412.05210},
  year={2024}
}



@article{mdeval,
  title={MdEval: Massively Multilingual Code Debugging},
  author={Liu, Shukai and Chai, Linzheng and Yang, Jian and Shi, Jiajun and Zhu, He and Wang, Liran and Jin, Ke and Zhang, Wei and Zhu, Hualei and Guo, Shuyue and others},
  journal={arXiv preprint arXiv:2411.02310},
  year={2024}
}

@article{tablebench,
  title={TableBench: A Comprehensive and Complex Benchmark for Table Question Answering},
  author={Wu, Xianjie and Yang, Jian and Chai, Linzheng and Zhang, Ge and Liu, Jiaheng and Du, Xinrun and Liang, Di and Shu, Daixin and Cheng, Xianfu and Sun, Tianzhen and others},
  journal={arXiv preprint arXiv:2408.09174},
  year={2024}
}

@article{adapter,
 author = {Neil Houlsby and Andrei Giurgiu and Stanislaw Jastrzebski and Bruna Morrone and Quentin de Laroussilhe and Andrea Gesmundo and Mona Attariyan and Sylvain Gelly},
 journal = {arXiv preprint arXiv: 1902.00751},
 title = {Parameter-Efficient Transfer Learning for NLP},
 year = {2019}
}

@article{agarwal2024litllm,
  author       = {Shubham Agarwal and
                  Issam H. Laradji and
                  Laurent Charlin and
                  Christopher Pal},
  title        = {LitLLM: {A} Toolkit for Scientific Literature Review},
  journal      = {CoRR},
  volume       = {abs/2402.01788},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2402.01788},
  doi          = {10.48550/ARXIV.2402.01788},
  eprinttype    = {arXiv},
  eprint       = {2402.01788},
  timestamp    = {Fri, 09 Feb 2024 12:18:48 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2402-01788.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{agent-survey-1,
 author = {Lei Wang and Chengbang Ma and Xueyang Feng and Zeyu Zhang and Hao-ran Yang and Jingsen Zhang and Zhi-Yang Chen and Jiakai Tang and Xu Chen and Yankai Lin and Wayne Xin Zhao and Zhewei Wei and Ji-rong Wen},
 bibsource = {Semantic Scholar https://www.semanticscholar.org/paper/28c6ac721f54544162865f41c5692e70d61bccab},
 doi = {10.1007/s11704-024-40231-1},
 journal = {Frontiers Comput. Sci.},
 title = {A Survey on Large Language Model based Autonomous Agents},
 year = {2023}
}

@article{agent-survey-2,
 author = {Zhiheng Xi and Wenxiang Chen and Xin Guo and Wei He and Yiwen Ding and Boyang Hong and Ming Zhang and Junzhe Wang and Senjie Jin and Enyu Zhou and Rui Zheng and Xiaoran Fan and Xiao Wang and Limao Xiong and Yuhao Zhou and Weiran Wang and Changhao Jiang and Yicheng Zou and Xiangyang Liu and Zhangyue Yin and Shihan Dou and Rongxiang Weng and Wensen Cheng and Qi Zhang and Wenjuan Qin and Yongyan Zheng and Xipeng Qiu and Xuanjing Huang and Tao Gui},
 journal = {arXiv preprint arXiv: 2309.07864},
 title = {The Rise and Potential of Large Language Model Based Agents: A Survey},
 year = {2023}
}

@inproceedings{agrawal2024evaluating,
 author = {Agrawal, Ameeta and Dang, Andy and Nezhad, Sina Bagheri and Pokharel, Rhitabrat and Scheinberg, Russell},
 booktitle = {Proceedings of the Fourth Workshop on Multilingual Representation Learning (MRL 2024)},
 pages = {216--231},
 title = {Evaluating Multilingual Long-Context Models for Retrieval and Reasoning},
 year = {2024}
}

@inproceedings{ainslie2023gqa,
 author = {Joshua Ainslie and James Lee-Thorp and Michiel de Jong and Yury Zemlyanskiy and Federico Lebron and Sumit Sanghai},
 booktitle = {The 2023 Conference on Empirical Methods in Natural Language Processing},
 title = {{GQA}: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints},
 url = {https://openreview.net/forum?id=hmOwOZWzYE},
 year = {2023}
}

@inproceedings{ainslie2023gqatraininggeneralizedmultiquery,
  author       = {Joshua Ainslie and
                  James Lee{-}Thorp and
                  Michiel de Jong and
                  Yury Zemlyanskiy and
                  Federico Lebr{\'{o}}n and
                  Sumit Sanghai},
  title        = {{GQA:} Training Generalized Multi-Query Transformer Models from Multi-Head
                  Checkpoints},
  booktitle    = {{EMNLP}},
  pages        = {4895--4901},
  publisher    = {Association for Computational Linguistics},
  year         = {2023}
}
@article{al2023position,
 author = {Al-Khateeb, Faisal and Dey, Nolan and Soboleva, Daria and Hestness, Joel},
 journal = {arXiv preprint arXiv:2310.13017},
 title = {Position Interpolation Improves ALiBi Extrapolation},
 year = {2023}
}

@inproceedings{Alayrac-2022-nips-flamingo,
 author = {Jean{-}Baptiste Alayrac and
Jeff Donahue and
Pauline Luc and
Antoine Miech and
Iain Barr and
Yana Hasson and
Karel Lenc and
Arthur Mensch and
Katherine Millican and
Malcolm Reynolds and
Roman Ring and
Eliza Rutherford and
Serkan Cabi and
Tengda Han and
Zhitao Gong and
Sina Samangooei and
Marianne Monteiro and
Jacob L. Menick and
Sebastian Borgeaud and
Andy Brock and
Aida Nematzadeh and
Sahand Sharifzadeh and
Mikolaj Binkowski and
Ricardo Barreira and
Oriol Vinyals and
Andrew Zisserman and
Kar{\'{e}}n Simonyan},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/nips/AlayracDLMBHLMM22.bib},
 booktitle = {Advances in Neural Information Processing Systems 35: Annual Conference
on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans,
LA, USA, November 28 - December 9, 2022},
 editor = {Sanmi Koyejo and
S. Mohamed and
A. Agarwal and
Danielle Belgrave and
K. Cho and
A. Oh},
 timestamp = {Mon, 08 Jan 2024 16:31:26 +0100},
 title = {Flamingo: a Visual Language Model for Few-Shot Learning},
 url = {http://papers.nips.cc/paper\_files/paper/2022/hash/960a172bc7fbf0177ccccbb411a7d800-Abstract-Conference.html},
 year = {2022}
}

@article{an2023eval,
 author = {An, Chenxin and Gong, Shansan and Zhong, Ming and Zhao, Xingjian and Li, Mukai and Zhang, Jun and Kong, Lingpeng and Qiu, Xipeng},
 journal = {arXiv preprint arXiv:2307.11088},
 title = {L-eval: Instituting standardized evaluation for long context language models},
 year = {2023}
}

@article{an2024does,
 author = {An, Chenxin and Zhang, Jun and Zhong, Ming and Li, Lei and Gong, Shansan and Luo, Yao and Xu, Jingjing and Kong, Lingpeng},
 journal = {arXiv preprint arXiv:2410.18745},
 title = {Why Does the Effective Context Length of LLMs Fall Short?},
 year = {2024}
}

@inproceedings{an2024make,
 author = {Shengnan An and Zexiong Ma and Zeqi Lin and Nanning Zheng and Jian-Guang Lou and Weizhu Chen},
 booktitle = {The Thirty-eighth Annual Conference on Neural Information Processing Systems},
 title = {Make Your {LLM} Fully Utilize the Context},
 url = {https://openreview.net/forum?id=YGTVEmBXtV},
 year = {2024}
}


@article{an2024training,
 author = {An, Chenxin and Huang, Fei and Zhang, Jun and Gong, Shansan and Qiu, Xipeng and Zhou, Chang and Kong, Lingpeng},
 journal = {arXiv preprint arXiv:2402.17463},
 title = {Training-free long-context scaling of large language models},
 year = {2024}
}

@article{armt,
 author = {Ivan Rodkin and
Yuri Kuratov and
Aydar Bulatov and
Mikhail Burtsev},
 journal = {CoRR},
 title = {Associative Recurrent Memory Transformer},
 volume = {abs/2407.04841},
 year = {2024}
}


@misc{arora2024simplelinearattentionlanguage,
      title={Simple linear attention language models balance the recall-throughput tradeoff}, 
      author={Simran Arora and Sabri Eyuboglu and Michael Zhang and Aman Timalsina and Silas Alberti and Dylan Zinsley and James Zou and Atri Rudra and Christopher Ré},
      year={2024},
      eprint={2402.18668},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.18668}, 
}
@article{assogba2024evaluating,
 author = {Assogba, Yannick and Ren, Donghao},
 journal = {arXiv preprint arXiv:2407.21049},
 title = {Evaluating Long Range Dependency Handling in Code Generation Models using Multi-Step Key Retrieval},
 year = {2024}
}

@article{atlas,
 author = {Gautier Izacard and Patrick S. H. Lewis and Maria Lomeli and Lucas Hosseini and Fabio Petroni and Timo Schick and Jane Dwivedi{-}Yu and Armand Joulin and Sebastian Riedel and Edouard Grave},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/journals/jmlr/IzacardLLHPSDJRG23.bib},
 journal = {J. Mach. Learn. Res.},
 pages = {251:1-251:43},
 timestamp = {Wed, 11 Sep 2024 14:41:28 +0200},
 title = {Atlas: Few-shot Learning with Retrieval Augmented Language Models},
 url = {https://jmlr.org/papers/v24/23-0037.html},
 volume = {24},
 year = {2023}
}

@misc{attn_off_by_one,
 author = {Miller, Evan},
 day = {24},
 month = {7},
 title = {Attention Is Off By One},
 url = {https://www.evanmiller.org/attention-is-off-by-one.html},
 urldate = {2025-02-19},
 year = {2023}
}

@inproceedings{attn_sink,
 author = {Guangxuan Xiao and Yuandong Tian and Beidi Chen and Song Han and Mike Lewis},
 booktitle = {The Twelfth International Conference on Learning Representations},
 title = {Efficient Streaming Language Models with Attention Sinks},
 url = {https://openreview.net/forum?id=NG7sS51zVF},
 year = {2024}
}

@inproceedings{bai2020pipeswitch,
 author = {Bai, Zhihao and Zhang, Zhen and Zhu, Yibo and Jin, Xin},
 booktitle = {14th USENIX Symposium on Operating Systems Design and Implementation (OSDI 20)},
 pages = {499--514},
 title = {$\{$PipeSwitch$\}$: Fast pipelined context switching for deep learning applications},
 year = {2020}
}

@article{bai2023longbench,
 author = {Bai, Yushi and Lv, Xin and Zhang, Jiajie and Lyu, Hongchang and Tang, Jiankai and Huang, Zhidian and Du, Zhengxiao and Liu, Xiao and Zeng, Aohan and Hou, Lei and others},
 journal = {arXiv preprint arXiv:2308.14508},
 title = {Longbench: A bilingual, multitask benchmark for long context understanding},
 year = {2023}
}

@article{bai2024longalign,
 author = {Bai, Yushi and Lv, Xin and Zhang, Jiajie and He, Yuze and Qi, Ji and Hou, Lei and Tang, Jie and Dong, Yuxiao and Li, Juanzi},
 journal = {arXiv preprint arXiv:2401.18058},
 title = {Longalign: A recipe for long context alignment of large language models},
 year = {2024}
}

@article{bai2024longbench2,
 author = {Yushi Bai and Shangqing Tu and Jiajie Zhang and Hao Peng and Xiaozhi Wang and Xin Lv and Shulin Cao and Jiazheng Xu and Lei Hou and Yuxiao Dong and Jie Tang and Juanzi Li},
 journal = {arXiv preprint arXiv:2412.15204},
 title = {LongBench v2: Towards Deeper Understanding and Reasoning on Realistic Long-context Multitasks},
 year = {2024}
}

@article{bai2024longwriter,
 author = {Yushi Bai and Jiajie Zhang and Xin Lv and Linzhi Zheng and Siqi Zhu and Lei Hou and Yuxiao Dong and Jie Tang and Juanzi Li},
 journal = {arXiv preprint arXiv:2408.07055},
 title = {LongWriter: Unleashing 10,000+ Word Generation from Long Context LLMs},
 year = {2024}
}

@inproceedings{banerjee2005meteor,
 author = {Banerjee, Satanjeev and Lavie, Alon},
 booktitle = {Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization},
 pages = {65--72},
 title = {METEOR: An automatic metric for MT evaluation with improved correlation with human judgments},
 year = {2005}
}

@incollection{Bang18,
 address = {Rijeka},
 author = {Youngjoo Kim and Hyochoong Bang},
 booktitle = {Introduction and Implementations of the Kalman Filter},
 chapter = {2},
 editor = {Felix Govaers},
 publisher = {IntechOpen},
 title = {Introduction to Kalman Filter and Its Applications},
 year = {2018}
}

@inproceedings{bao2021g,
  author       = {Guangsheng Bao and
                  Yue Zhang and
                  Zhiyang Teng and
                  Boxing Chen and
                  Weihua Luo},
  editor       = {Chengqing Zong and
                  Fei Xia and
                  Wenjie Li and
                  Roberto Navigli},
  title        = {G-Transformer for Document-Level Machine Translation},
  booktitle    = {Proceedings of the 59th Annual Meeting of the Association for Computational
                  Linguistics and the 11th International Joint Conference on Natural
                  Language Processing, {ACL/IJCNLP} 2021, (Volume 1: Long Papers), Virtual
                  Event, August 1-6, 2021},
  pages        = {3442--3455},
  publisher    = {Association for Computational Linguistics},
  year         = {2021},
  url          = {https://doi.org/10.18653/v1/2021.acl-long.267},
  doi          = {10.18653/V1/2021.ACL-LONG.267},
  timestamp    = {Sun, 19 Jan 2025 13:21:47 +0100},
  biburl       = {https://dblp.org/rec/conf/acl/Bao0TCL20.bib}
}


@article{DBLP:journals/corr/abs-2401-07872,
  author       = {Saurav Pawar and
                  S. M. Towhidul Islam Tonmoy and
                  S. M. Mehedi Zaman and
                  Vinija Jain and
                  Aman Chadha and
                  Amitava Das},
  title        = {The What, Why, and How of Context Length Extension Techniques in Large
                  Language Models - {A} Detailed Survey},
  journal      = {CoRR},
  volume       = {abs/2401.07872},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2401.07872},
  doi          = {10.48550/ARXIV.2401.07872},
  eprinttype    = {arXiv},
  eprint       = {2401.07872},
  timestamp    = {Thu, 01 Feb 2024 15:35:36 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2401-07872.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{huang2024advancing,
      title={Advancing Transformer Architecture in Long-Context Large Language Models: A Comprehensive Survey}, 
      author={Yunpeng Huang and Jingwei Xu and Junyu Lai and Zixu Jiang and Taolue Chen and Zenan Li and Yuan Yao and Xiaoxing Ma and Lijuan Yang and Hao Chen and Shupeng Li and Penghao Zhao},
      year={2024},
      eprint={2311.12351},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{zhao-etal-2024-length,
    title = "Length Extrapolation of Transformers: A Survey from the Perspective of Positional Encoding",
    author = "Zhao, Liang  and
      Feng, Xiachong  and
      Feng, Xiaocheng  and
      Zhong, Weihong  and
      Xu, Dongliang  and
      Yang, Qing  and
      Liu, Hongtao  and
      Qin, Bing  and
      Liu, Ting",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2024",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-emnlp.582/",
    doi = "10.18653/v1/2024.findings-emnlp.582",
    pages = "9959--9977"
}

@misc{beck2024xlstmextendedlongshortterm,
 archiveprefix = {arXiv},
 author = {Maximilian Beck and Korbinian Pöppel and Markus Spanring and Andreas Auer and Oleksandra Prudnikova and Michael Kopp and Günter Klambauer and Johannes Brandstetter and Sepp Hochreiter},
 eprint = {2405.04517},
 primaryclass = {cs.LG},
 title = {xLSTM: Extended Long Short-Term Memory},
 url = {https://arxiv.org/abs/2405.04517},
 year = {2024}
}


@misc{beltagy2020longformerlongdocumenttransformer,
 archiveprefix = {arXiv},
 author = {Iz Beltagy and Matthew E. Peters and Arman Cohan},
 eprint = {2004.05150},
 primaryclass = {cs.CL},
 title = {Longformer: The Long-Document Transformer},
 url = {https://arxiv.org/abs/2004.05150},
 year = {2020}
}

@article{DBLP:journals/corr/abs-2004-05150,
  author       = {Iz Beltagy and
                  Matthew E. Peters and
                  Arman Cohan},
  title        = {Longformer: The Long-Document Transformer},
  journal      = {CoRR},
  volume       = {abs/2004.05150},
  year         = {2020},
  url          = {https://arxiv.org/abs/2004.05150},
  eprinttype    = {arXiv},
  eprint       = {2004.05150},
  timestamp    = {Tue, 14 Apr 2020 16:40:34 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2004-05150.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@incollection{Bengio+chapter2007,
 author = {Bengio, Yoshua and LeCun, Yann},
 booktitle = {Large Scale Kernel Machines},
 publisher = {MIT Press},
 title = {Scaling Learning Algorithms Towards {AI}},
 year = {2007}
}

@misc{benkish2024decimambaexploringlengthextrapolation,
 archiveprefix = {arXiv},
 author = {Assaf Ben-Kish and Itamar Zimerman and Shady Abu-Hussein and Nadav Cohen and Amir Globerson and Lior Wolf and Raja Giryes},
 eprint = {2406.14528},
 primaryclass = {cs.LG},
 title = {DeciMamba: Exploring the Length Extrapolation Potential of Mamba},
 url = {https://arxiv.org/abs/2406.14528},
 year = {2024}
}

@article{bertsch2024context,
 author = {Bertsch, Amanda and Ivgi, Maor and Alon, Uri and Berant, Jonathan and Gormley, Matthew R and Neubig, Graham},
 journal = {arXiv preprint arXiv:2405.00200},
 title = {In-context learning with long-context models: An in-depth exploration},
 year = {2024}
}

@inproceedings{BlockRecurrentTransformers,
  author       = {DeLesley Hutchins and
                  Imanol Schlag and
                  Yuhuai Wu and
                  Ethan Dyer and
                  Behnam Neyshabur},
  title        = {Block-Recurrent Transformers},
  booktitle    = {NeurIPS},
  year         = {2022}
}

@article{bogomolov2024long,
 author = {Bogomolov, Egor and Eliseeva, Aleksandra and Galimzyanov, Timur and Glukhov, Evgeniy and Shapkin, Anton and Tigina, Maria and Golubev, Yaroslav and Kovrigin, Alexander and van Deursen, Arie and Izadi, Maliheh and others},
 journal = {arXiv preprint arXiv:2406.11612},
 title = {Long Code Arena: a Set of Benchmarks for Long-Context Code Models},
 year = {2024}
}

@article{bosselut2018discourse,
 author = {Bosselut, Antoine and Celikyilmaz, Asli and He, Xiaodong and Gao, Jianfeng and Huang, Po-Sen and Choi, Yejin},
 journal = {arXiv preprint arXiv:1805.03766},
 title = {Discourse-aware neural rewards for coherent text generation},
 year = {2018}
}

@article{brakel2024model,
 author = {Brakel, Felix and Odyurt, Uraz and Varbanescu, Ana-Lucia},
 journal = {arXiv preprint arXiv:2403.03699},
 title = {Model Parallelism on Distributed Infrastructure: A Literature Review from Theory to LLM Case-Studies},
 year = {2024}
}

@article{brandon2023striped,
 author = {Brandon, William and Nrusimha, Aniruddha and Qian, Kevin and Ankner, Zachary and Jin, Tian and Song, Zhiye and Ragan-Kelley, Jonathan},
 journal = {arXiv preprint arXiv:2311.09431},
 title = {Striped attention: Faster ring attention for causal transformers},
 year = {2023}
}

@misc{brandon2024reducingtransformerkeyvaluecache,
 archiveprefix = {arXiv},
 author = {William Brandon and Mayank Mishra and Aniruddha Nrusimha and Rameswar Panda and Jonathan Ragan Kelly},
 eprint = {2405.12981},
 primaryclass = {cs.LG},
 title = {Reducing Transformer Key-Value Cache Size with Cross-Layer Attention},
 url = {https://arxiv.org/abs/2405.12981},
 year = {2024}
}

@inproceedings{brants2007large,
 author = {Brants, Thorsten and Popat, Ashok and Xu, Peng and Och, Franz Josef and Dean, Jeffrey},
 booktitle = {Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)},
 pages = {858--867},
 title = {Large language models in machine translation},
 year = {2007}
}

@article{Brown2020LanguageMA,
 author = {Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeff Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Ma-teusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
 journal = {ArXiv},
 title = {Language Models are Few-Shot Learners},
 volume = {abs/2005.14165},
 year = {2020}
}

@misc{buckman2024,
 author = {Buckman, Jacob},
 date = {2024-08-14},
 langid = {en},
 publisher = {Manifest AI},
 title = {LongCrawl64: {A} {Long-Context} {Natural-Language} {Dataset}}
}

@misc{c4ai-command-r7b-12-2024,
 author = {Cohere and Cohere For AI},
 month = {December},
 note = {Access requires agreement to the License Agreement and adherence to C4AI's Acceptable Use Policy.},
 title = {C4AI Command R7B: A 7 Billion Parameter Multilingual Model},
 url = {https://huggingface.co/CohereForAI/c4ai-command-r7b-12-2024},
 year = {2024}
}

@inproceedings{cai2024medusa,
  title={MEDUSA: Simple LLM inference acceleration framework with multiple decoding heads},
  author={Cai, Tianle and Li, Yuhong and Geng, Zhengyang and Peng, Hongwu and Lee, Jason D and Chen, Deming and Dao, Tri},
  booktitle={Proceedings of the 41st International Conference on Machine Learning},
  pages={5209--5235},
  year={2024}
}

@misc{cai2024pyramidkvdynamickvcache,
 archiveprefix = {arXiv},
 author = {Zefan Cai and Yichi Zhang and Bofei Gao and Yuliang Liu and Tianyu Liu and Keming Lu and Wayne Xiong and Yue Dong and Baobao Chang and Junjie Hu and Wen Xiao},
 eprint = {2406.02069},
 primaryclass = {cs.CL},
 title = {PyramidKV: Dynamic KV Cache Compression based on Pyramidal Information Funneling},
 url = {https://arxiv.org/abs/2406.02069},
 year = {2024}
}

@inproceedings{cao2024instruction,
 author = {Yihan Cao and Yanbin Kang and Chi Wang and Lichao Sun},
 booktitle = {First Conference on Language Modeling},
 title = {Instruction Mining: Instruction Data Selection for Tuning Large Language Models},
 year = {2024}
}

@article{chain-of-agent,
 author = {Yusen Zhang and Ruoxi Sun and Yanfei Chen and Tomas Pfister and Rui Zhang and Sercan Ö. Arik},
 journal = {arXiv preprint arXiv: 2406.02818},
 title = {Chain of Agents: Large Language Models Collaborating on Long-Context Tasks},
 year = {2024}
}

@article{chan2024mle,
 author = {Chan, Jun Shern and Chowdhury, Neil and Jaffe, Oliver and Aung, James and Sherburn, Dane and Mays, Evan and Starace, Giulio and Liu, Kevin and Maksin, Leon and Patwardhan, Tejal and others},
 journal = {arXiv preprint arXiv:2410.07095},
 title = {Mle-bench: Evaluating machine learning agents on machine learning engineering},
 year = {2024}
}

@article{DBLP:journals/corr/abs-2410-07095,
  author       = {Jun Shern Chan and
                  Neil Chowdhury and
                  Oliver Jaffe and
                  James Aung and
                  Dane Sherburn and
                  Evan Mays and
                  Giulio Starace and
                  Kevin Liu and
                  Leon Maksin and
                  Tejal Patwardhan and
                  Lilian Weng and
                  Aleksander Madry},
  title        = {MLE-bench: Evaluating Machine Learning Agents on Machine Learning
                  Engineering},
  journal      = {CoRR},
  volume       = {abs/2410.07095},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2410.07095},
  doi          = {10.48550/ARXIV.2410.07095},
  eprinttype    = {arXiv},
  eprint       = {2410.07095},
  timestamp    = {Tue, 19 Nov 2024 08:58:21 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2410-07095.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{Chandrasegaran-2024-arxiv-videohour,
 author = {Keshigeyan Chandrasegaran and
Agrim Gupta and
Lea M. Hadzic and
Taran Kota and
Jimming He and
Crist{\'{o}}bal Eyzaguirre and
Zane Durante and
Manling Li and
Jiajun Wu and
Li Fei{-}Fei},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/journals/corr/abs-2411-04998.bib},
 doi = {10.48550/ARXIV.2411.04998},
 eprint = {2411.04998},
 eprinttype = {arXiv},
 journal = {CoRR},
 timestamp = {Wed, 01 Jan 2025 11:02:28 +0100},
 title = {HourVideo: 1-Hour Video-Language Understanding},
 url = {https://doi.org/10.48550/arXiv.2411.04998},
 volume = {abs/2411.04998},
 year = {2024}
}

@misc{character_ai,
 author = {{Character AI}},
 howpublished = {Retrieved September 14, 2023 from \url{https://character.ai/}},
 title = {Character AI},
 url = {https://character.ai/},
 year = {2023}
}

@article{chen-arxiv-2024-internvl15,
 author = {Zhe Chen and
Weiyun Wang and
Hao Tian and
Shenglong Ye and
Zhangwei Gao and
Erfei Cui and
Wenwen Tong and
Kongzhi Hu and
Jiapeng Luo and
Zheng Ma and
Ji Ma and
Jiaqi Wang and
Xiaoyi Dong and
Hang Yan and
Hewei Guo and
Conghui He and
Botian Shi and
Zhenjiang Jin and
Chao Xu and
Bin Wang and
Xingjian Wei and
Wei Li and
Wenjian Zhang and
Bo Zhang and
Pinlong Cai and
Licheng Wen and
Xiangchao Yan and
Min Dou and
Lewei Lu and
Xizhou Zhu and
Tong Lu and
Dahua Lin and
Yu Qiao and
Jifeng Dai and
Wenhai Wang},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/journals/corr/abs-2404-16821.bib},
 doi = {10.48550/ARXIV.2404.16821},
 eprint = {2404.16821},
 eprinttype = {arXiv},
 journal = {CoRR},
 timestamp = {Tue, 19 Nov 2024 16:54:45 +0100},
 title = {How Far Are We to GPT-4V? Closing the Gap to Commercial Multimodal
Models with Open-Source Suites},
 url = {https://doi.org/10.48550/arXiv.2404.16821},
 volume = {abs/2404.16821},
 year = {2024}
}

@inproceedings{chen2022summscreen,
 author = {Chen, Mingda and Chu, Zewei and Wiseman, Sam and Gimpel, Kevin},
 booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
 pages = {8602--8615},
 title = {SummScreen: A Dataset for Abstractive Screenplay Summarization},
 year = {2022}
}

@article{chen2023clex,
 author = {Chen, Guanzheng and Li, Xin and Meng, Zaiqiao and Liang, Shangsong and Bing, Lidong},
 journal = {arXiv preprint arXiv:2310.16450},
 title = {Clex: Continuous length extrapolation for large language models},
 year = {2023}
}

@article{chen2023extending,
 author = {Chen, Shouyuan and Wong, Sherman and Chen, Liangjian and Tian, Yuandong},
 journal = {arXiv preprint arXiv:2306.15595},
 title = {Extending context window of large language models via positional interpolation},
 year = {2023}
}

@misc{chen2023extendingcontextwindowlarge,
 archiveprefix = {arXiv},
 author = {Shouyuan Chen and Sherman Wong and Liangjian Chen and Yuandong Tian},
 eprint = {2306.15595},
 primaryclass = {cs.CL},
 title = {Extending Context Window of Large Language Models via Positional Interpolation},
 url = {https://arxiv.org/abs/2306.15595},
 year = {2023}
}


@article{chen2023fortify,
 author = {Chen, Yuhan and Lv, Ang and Lin, Ting-En and Chen, Changyu and Wu, Yuchuan and Huang, Fei and Li, Yongbin and Yan, Rui},
 journal = {arXiv preprint arXiv:2312.04455},
 title = {Fortify the shortest stave in attention: Enhancing context awareness of large language models for effective tool use},
 year = {2023}
}

@article{chen2023longlora,
 author = {Chen, Yukang and Qian, Shengju and Tang, Haotian and Lai, Xin and Liu, Zhijian and Han, Song and Jia, Jiaya},
 journal = {arXiv preprint arXiv:2309.12307},
 title = {LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models},
 year = {2023}
}

@inproceedings{chen2024alpagasus,
 author = {Lichang Chen and Shiyang Li and Jun Yan and Hai Wang and Kalpa Gunaratna and Vikas Yadav and Zheng Tang and Vijay Srinivasan and Tianyi Zhou and Heng Huang and Hongxia Jin},
 booktitle = {The Twelfth International Conference on Learning Representations},
 title = {AlpaGasus: Training a Better Alpaca with Fewer Data},
 year = {2024}
}


@article{chen2024bge,
  author       = {Jianlv Chen and
                  Shitao Xiao and
                  Peitian Zhang and
                  Kun Luo and
                  Defu Lian and
                  Zheng Liu},
  title        = {{BGE} M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity
                  Text Embeddings Through Self-Knowledge Distillation},
  journal      = {CoRR},
  volume       = {abs/2402.03216},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2402.03216},
  doi          = {10.48550/ARXIV.2402.03216},
  eprinttype    = {arXiv},
  eprint       = {2402.03216},
  timestamp    = {Sun, 19 Jan 2025 13:42:15 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2402-03216.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{chen2024centauri,
 author = {Chen, Chang and Li, Xiuhong and Zhu, Qianchao and Duan, Jiangfei and Sun, Peng and Zhang, Xingcheng and Yang, Chao},
 booktitle = {Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3},
 pages = {178--191},
 title = {Centauri: Enabling efficient scheduling for communication-computation overlap in large model training via communication partitioning},
 year = {2024}
}

@inproceedings{chen2024clex,
 author = {Chen, Guanzheng and Li, Xin and Meng, Zaiqiao and Liang, Shangsong and Bing, Lidong},
 booktitle = {The Twelfth International Conference on Learning Representations},
 title = {CLEX: Continuous Length Extrapolation for Large Language Models},
 year = {2024}
}

@article{chen2024fintextqa,
 author = {Chen, Jian and Zhou, Peilin and Hua, Yining and Loh, Yingxin and Chen, Kehui and Li, Ziyuan and Zhu, Bing and Liang, Junwei},
 journal = {arXiv preprint arXiv:2405.09980},
 title = {FinTextQA: A Dataset for Long-form Financial Question Answering},
 year = {2024}
}

@article{chen2024hope,
 author = {Chen, Yuhan and Lv, Ang and Luan, Jian and Wang, Bin and Liu, Wei},
 journal = {arXiv preprint arXiv:2410.21216},
 title = {HoPE: A Novel Positional Encoding Without Long-Term Decay for Enhanced Context Awareness and Extrapolation},
 year = {2024}
}

@article{chen2024magicdec,
 author = {Chen, Jian and Tiwari, Vashisth and Sadhukhan, Ranajoy and Chen, Zhuoming and Shi, Jinyuan and Yen, Ian En-Hsu and Chen, Beidi},
 journal = {arXiv preprint arXiv:2408.11049},
 title = {Magicdec: Breaking the latency-throughput tradeoff for long context generation with speculative decoding},
 year = {2024}
}

@article{chen2024not,
 author = {Chen, Xingyu and Xu, Jiahao and Liang, Tian and He, Zhiwei and Pang, Jianhui and Yu, Dian and Song, Linfeng and Liu, Qiuzhi and Zhou, Mengfei and Zhang, Zhuosheng and others},
 journal = {arXiv preprint arXiv:2412.21187},
 title = {Do not think that much for 2+ 3=? on the overthinking of o1-like llms},
 year = {2024}
}

@article{chen2024stuffed,
 author = {Chen, Yingfa and Zhang, Xinrong and Hu, Shengding and Han, Xu and Liu, Zhiyuan and Sun, Maosong},
 journal = {arXiv preprint arXiv:2410.07145},
 title = {Stuffed mamba: State collapse and state capacity of rnn-based long-context modeling},
 year = {2024}
}

@misc{chen2024stuffedmambastatecollapse,
 archiveprefix = {arXiv},
 author = {Yingfa Chen and Xinrong Zhang and Shengding Hu and Xu Han and Zhiyuan Liu and Maosong Sun},
 eprint = {2410.07145},
 primaryclass = {cs.CL},
 title = {Stuffed Mamba: State Collapse and State Capacity of RNN-Based Long-Context Modeling},
 url = {https://arxiv.org/abs/2410.07145},
 year = {2024}
}


@article{chen2024travelagent,
  author       = {Aili Chen and
                  Xuyang Ge and
                  Ziquan Fu and
                  Yanghua Xiao and
                  Jiangjie Chen},
  title        = {TravelAgent: An {AI} Assistant for Personalized Travel Planning},
  journal      = {CoRR},
  volume       = {abs/2409.08069},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2409.08069},
  doi          = {10.48550/ARXIV.2409.08069},
  eprinttype    = {arXiv},
  eprint       = {2409.08069},
  timestamp    = {Sat, 12 Oct 2024 00:13:22 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2409-08069.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{Chen2024WhatAT,
 author = {Zhi Chen and Qiguang Chen and Libo Qin and Qipeng Guo and Haijun Lv and Yicheng Zou and Wanxiang Che and Hang Yan and Kai Chen and Dahua Lin},
 journal = {ArXiv},
 title = {What are the Essential Factors in Crafting Effective Long Context Multi-Hop Instruction Datasets? Insights and Best Practices},
 volume = {abs/2409.01893},
 year = {2024}
}

@misc{chen2025r1v,
 author = {Chen, Liang and Li, Lei and Zhao, Haozhe and Song, Yifan and Vinci},
 howpublished = {\url{https://github.com/Deep-Agent/R1-V}},
 note = {Accessed: 2025-02-02},
 title = {R1-V: Reinforcing Super Generalization Ability in Vision-Language Models with Less Than \$3},
 year = {2025}
}

@misc{cheng2024xragextremecontextcompression,
 archiveprefix = {arXiv},
 author = {Xin Cheng and Xun Wang and Xingxing Zhang and Tao Ge and Si-Qing Chen and Furu Wei and Huishuai Zhang and Dongyan Zhao},
 eprint = {2405.13792},
 primaryclass = {cs.CL},
 title = {xRAG: Extreme Context Compression for Retrieval-augmented Generation with One Token},
 url = {https://arxiv.org/abs/2405.13792},
 year = {2024}
}

@inproceedings{
cheng2024xrag,
title={x{RAG}: Extreme Context Compression for Retrieval-augmented Generation with One Token},
author={Xin Cheng and Xun Wang and Xingxing Zhang and Tao Ge and Si-Qing Chen and Furu Wei and Huishuai Zhang and Dongyan Zhao},
booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
year={2024},
url={https://openreview.net/forum?id=6pTlXqrO0p}
}

@inproceedings{chevalier-etal-2023-adapting,
 abstract = {Transformer-based language models (LMs) are powerful and widely-applicable tools, but their usefulness is constrained by a finite context window and the expensive computational cost of processing long text documents. We propose to adapt pre-trained LMs into AutoCompressors. These language models are capable of compressing long contexts into summary vectors, which are then accessible to the model as soft prompts. Summary vectors are trained with an unsupervised objective, whereby long documents are processed in segments, and summary vectors from all previous segments are used in language modeling. We fine-tune OPT and Llama-2 models on sequences of up to 30,720 tokens and show that AutoCompressors can utilize long contexts to improve perplexity. We evaluate AutoCompressors on in-context learning by compressing task demonstrations and find that summary vectors are good substitutes for plain-text demonstrations, increasing accuracy while reducing inference costs. Finally, we explore the benefits of pre-computing summary vectors for large corpora by applying summary vectors to retrieval-augmented language modeling and a passage re-ranking task. Overall, AutoCompressors emerge as a simple and inexpensive solution to extend the context window of LMs while speeding up inference over long contexts.},
 address = {Singapore},
 author = {Chevalier, Alexis  and
Wettig, Alexander  and
Ajith, Anirudh  and
Chen, Danqi},
 booktitle = {Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
 doi = {10.18653/v1/2023.emnlp-main.232},
 editor = {Bouamor, Houda  and
Pino, Juan  and
Bali, Kalika},
 month = {December},
 pages = {3829--3846},
 publisher = {Association for Computational Linguistics},
 title = {Adapting Language Models to Compress Contexts},
 url = {https://aclanthology.org/2023.emnlp-main.232/},
 year = {2023}
}

@article{chi2022dissecting,
 author = {Chi, Ta-Chung and Fan, Ting-Han and Rudnicky, Alexander I and Ramadge, Peter J},
 journal = {arXiv preprint arXiv:2212.10356},
 title = {Dissecting transformer length extrapolation via the lens of receptive field analysis},
 year = {2022}
}

@article{chi2022kerple,
 author = {Chi, Ta-Chung and Fan, Ting-Han and Ramadge, Peter J and Rudnicky, Alexander},
 journal = {Advances in Neural Information Processing Systems},
 pages = {8386--8399},
 title = {Kerple: Kernelized relative positional embedding for length extrapolation},
 volume = {35},
 year = {2022}
}

@article{chi2023latent,
 author = {Chi, Ta-Chung and Fan, Ting-Han and Chen, Li-Wei and Rudnicky, Alexander I and Ramadge, Peter J},
 journal = {arXiv preprint arXiv:2305.13571},
 title = {Latent positional information is in the self-attention variance of transformer language models without positional embeddings},
 year = {2023}
}

@article{chia-2024-arxiv-mlongdoc,
 author = {Yew Ken Chia and
Liying Cheng and
Hou Pong Chan and
Chaoqun Liu and
Maojia Song and
Sharifah Mahani Aljunied and
Soujanya Poria and
Lidong Bing},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/journals/corr/abs-2411-06176.bib},
 doi = {10.48550/ARXIV.2411.06176},
 eprint = {2411.06176},
 eprinttype = {arXiv},
 journal = {CoRR},
 timestamp = {Wed, 01 Jan 2025 11:02:35 +0100},
 title = {M-Longdoc: {A} Benchmark For Multimodal Super-Long Document Understanding
And {A} Retrieval-Aware Tuning Framework},
 url = {https://doi.org/10.48550/arXiv.2411.06176},
 volume = {abs/2411.06176},
 year = {2024}
}

@misc{child2019generatinglongsequencessparse,
 archiveprefix = {arXiv},
 author = {Rewon Child and Scott Gray and Alec Radford and Ilya Sutskever},
 eprint = {1904.10509},
 primaryclass = {cs.LG},
 title = {Generating Long Sequences with Sparse Transformers},
 url = {https://arxiv.org/abs/1904.10509},
 year = {2019}
}

@misc{chinnakonduru2024weightedgroupedqueryattention,
 archiveprefix = {arXiv},
 author = {Sai Sena Chinnakonduru and Astarag Mohapatra},
 eprint = {2407.10855},
 primaryclass = {cs.CL},
 title = {Weighted Grouped Query Attention in Transformers},
 url = {https://arxiv.org/abs/2407.10855},
 year = {2024}
}

@article{cho2018towards,
 author = {Cho, Woon Sang and Zhang, Pengchuan and Zhang, Yizhe and Li, Xiujun and Galley, Michel and Brockett, Chris and Wang, Mengdi and Gao, Jianfeng},
 journal = {arXiv preprint arXiv:1811.00511},
 title = {Towards coherent and cohesive long-form text generation},
 year = {2018}
}


@inproceedings{choromanski2022rethinkingattentionperformers,
  author       = {Krzysztof Marcin Choromanski and
                  Valerii Likhosherstov and
                  David Dohan and
                  Xingyou Song and
                  Andreea Gane and
                  Tam{\'{a}}s Sarl{\'{o}}s and
                  Peter Hawkins and
                  Jared Quincy Davis and
                  Afroz Mohiuddin and
                  Lukasz Kaiser and
                  David Benjamin Belanger and
                  Lucy J. Colwell and
                  Adrian Weller},
  title        = {Rethinking Attention with Performers},
  booktitle    = {{ICLR}},
  publisher    = {OpenReview.net},
  year         = {2021}
}

@inproceedings{chuang-etal-2024-learning,
 abstract = {Large language models (LLMs) are great at processing multiple natural language processing tasks, but their abilities are constrained by inferior performance with long context, slow inference speed, and the high cost of computing the results. Deploying LLMs with precise and informative context helps users process large-scale datasets more effectively and cost-efficiently. Existing works rely on compressing long prompt contexts into soft prompts. However, soft prompt compression encounters limitations in transferability across different LLMs, especially API-based LLMs. To this end, this work aims to compress lengthy prompts in the form of natural language with LLM transferability. This poses two challenges: (i) Natural Language (NL) prompts are incompatible with back-propagation, and (ii) NL prompts lack flexibility in imposing length constraints. In this work, we propose a Natural Language Prompt Encapsulation (Nano-Capsulator) framework compressing original prompts into NL formatted Capsule Prompt while maintaining prompt utility and transferability. Specifically, to tackle the first challenge, the Nano-Capsulator is optimized by a reward function that interacts with the proposed semantics preserving loss. To address the second question, the Nano-Capsulator is optimized by a reward function featuring length constraints. Experimental results demonstrate that the Capsule Prompt can reduce 81.4{\%} of the original length, decrease inference latency up to 4.5x, and save 80.1{\%} of budget overheads while providing transferability across diverse LLMs and different datasets.},
 address = {Mexico City, Mexico},
 author = {Chuang, Yu-Neng  and
Xing, Tianwei  and
Chang, Chia-Yuan  and
Liu, Zirui  and
Chen, Xun  and
Hu, Xia},
 booktitle = {Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)},
 doi = {10.18653/v1/2024.naacl-long.429},
 editor = {Duh, Kevin  and
Gomez, Helena  and
Bethard, Steven},
 month = {June},
 pages = {7756--7767},
 publisher = {Association for Computational Linguistics},
 title = {Learning to Compress Prompt in Natural Language Formats},
 url = {https://aclanthology.org/2024.naacl-long.429},
 year = {2024}
}

@article{churin2024long,
 author = {Churin, Igor and Apishev, Murat and Tikhonova, Maria and Shevelev, Denis and Bulatov, Aydar and Kuratov, Yuri and Averkiev, Sergej and Fenogenova, Alena},
 journal = {arXiv preprint arXiv:2408.02439},
 title = {Long Input Benchmark for Russian Analysis},
 year = {2024}
}

@misc{clevert2016fastaccuratedeepnetwork,
 archiveprefix = {arXiv},
 author = {Djork-Arné Clevert and Thomas Unterthiner and Sepp Hochreiter},
 eprint = {1511.07289},
 primaryclass = {cs.LG},
 title = {Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)},
 url = {https://arxiv.org/abs/1511.07289},
 year = {2016}
}

@article{cobbe2021training,
 author = {Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and others},
 journal = {arXiv preprint arXiv:2110.14168},
 title = {Training verifiers to solve math word problems},
 year = {2021}
}


@inproceedings{CompressiveTransformers,
  author       = {Jack W. Rae and
                  Anna Potapenko and
                  Siddhant M. Jayakumar and
                  Chloe Hillier and
                  Timothy P. Lillicrap},
  title        = {Compressive Transformers for Long-Range Sequence Modelling},
  booktitle    = {{ICLR}},
  publisher    = {OpenReview.net},
  year         = {2020}
}

@article{costa2024lcfo,
 author = {Costa-juss{\`a}, Marta R and Andrews, Pierre and Meglioli, Mariano Coria and Chen, Joy and Chuang, Joe and Dale, David and Ropers, Christophe and Mourachko, Alexandre and S{\'a}nchez, Eduardo and Schwenk, Holger and others},
 journal = {arXiv preprint arXiv:2412.08268},
 title = {LCFO: Long Context and Long Form Output Dataset and Benchmarking},
 year = {2024}
}

@article{cua2025,
 author = {OpenAI},
 title = {Computer-Using Agent: Introducing a universal interface for AI to interact with the digital world},
 url = {https://openai.com/index/computer-using-agent},
 year = {2025}
}

@misc{cursor_ai_2025,
 author = {Anysphere},
 howpublished = {\url{https://www.cursor.com/en}},
 title = {Cursor - The AI Code Editor},
 url = {https://www.cursor.com},
 year = {2025}
}

@article{D2,
 author = {Adyasha Maharana and
Prateek Yadav and
Mohit Bansal},
 doi = {10.48550/ARXIV.2310.07931},
 eprint = {2310.07931},
 eprinttype = {arXiv},
 journal = {CoRR},
 timestamp = {Tue, 24 Oct 2023 14:46:18 +0200},
 title = {{D2} Pruning: Message Passing for Balancing Diversity and Difficulty
in Data Pruning},
 url = {https://doi.org/10.48550/arXiv.2310.07931},
 volume = {abs/2310.07931},
 year = {2023}
}

@article{D4,
 author = {Kushal Tirumala and
Daniel Simig and
Armen Aghajanyan and
Ari S. Morcos},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/journals/corr/abs-2308-12284.bib},
 doi = {10.48550/ARXIV.2308.12284},
 eprint = {2308.12284},
 eprinttype = {arXiv},
 journal = {CoRR},
 timestamp = {Wed, 30 Aug 2023 17:27:54 +0200},
 title = {{D4:} Improving {LLM} Pretraining via Document De-Duplication and
Diversification},
 url = {https://doi.org/10.48550/arXiv.2308.12284},
 volume = {abs/2308.12284},
 year = {2023}
}

@misc{dai2019transformerxlattentivelanguagemodels,
 archiveprefix = {arXiv},
 author = {Zihang Dai and Zhilin Yang and Yiming Yang and Jaime Carbonell and Quoc V. Le and Ruslan Salakhutdinov},
 eprint = {1901.02860},
 primaryclass = {cs.LG},
 title = {Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context},
 url = {https://arxiv.org/abs/1901.02860},
 year = {2019}
}

@misc{dai2024cormcacheoptimizationrecent,
 archiveprefix = {arXiv},
 author = {Jincheng Dai and Zhuowei Huang and Haiyun Jiang and Chen Chen and Deng Cai and Wei Bi and Shuming Shi},
 eprint = {2404.15949},
 primaryclass = {cs.CL},
 title = {CORM: Cache Optimization with Recent Message for Large Language Model Inference},
 url = {https://arxiv.org/abs/2404.15949},
 year = {2024}
}

@article{dai2024deniahl,
 author = {Dai, Hui and Pechi, Dan and Yang, Xinyi and Banga, Garvit and Mantri, Raghav},
 journal = {arXiv preprint arXiv:2411.19360},
 title = {DENIAHL: In-Context Features Influence LLM Needle-In-A-Haystack Abilities},
 year = {2024}
}

@article{dao2022flashattention,
 author = {Dao, Tri and Fu, Dan and Ermon, Stefano and Rudra, Atri and R{\'e}, Christopher},
 journal = {Advances in neural information processing systems},
 pages = {16344--16359},
 title = {Flashattention: Fast and memory-efficient exact attention with io-awareness},
 volume = {35},
 year = {2022}
}

@inproceedings{dao2024flashattention,
  title={FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning},
  author={Dao, Tri},
  year={2024},
  booktitle={The Twelfth International Conference on Learning Representations}
}

@article{dao2024transformers,
 author = {Dao, Tri and Gu, Albert},
 journal = {arXiv preprint arXiv:2405.21060},
 title = {Transformers are SSMs: Generalized models and efficient algorithms through structured state space duality},
 year = {2024}
}

@inproceedings{dasigi2021dataset,
 author = {Dasigi, Pradeep and Lo, Kyle and Beltagy, Iz and Cohan, Arman and Smith, Noah A and Gardner, Matt},
 booktitle = {Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
 pages = {4599--4610},
 title = {A Dataset of Information-Seeking Questions and Answers Anchored in Research Papers},
 year = {2021}
}

@article{de2024griffin,
 author = {De, Soham and Smith, Samuel L and Fernando, Anushan and Botev, Aleksandar and Cristian-Muraru, George and Gu, Albert and Haroun, Ruba and Berrada, Leonard and Chen, Yutian and Srinivasan, Srivatsan and others},
 journal = {arXiv preprint arXiv:2402.19427},
 title = {Griffin: Mixing gated linear recurrences with local attention for efficient language models},
 year = {2024}
}

@misc{deepsearch,
  author = {Rupesh Bansal, Shiwangi Shah},
  title = {Deepsearch: Semantic search on multimedia sources like audio, video and images},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/deepsearch-ai/deepsearch}},
  url = {https://github.com/deepsearch-ai/deepsearch}
}

@misc{bing_copilot,
 author = {Microsoft},
 title = {Microsoft Copilot},
 url = {https://copilot.microsoft.com/},
 year = {2023}
}

@misc{SearchGPT,
 author = {OpenAI},
 title = {introducing-chatgpt-search},
 url = {https://openai.com/index/introducing-chatgpt-search/},
 year = {2024}
}

@misc{deepseek_web_2023,
 author = {DeepSeek},
 title = {DeepSeek Official Website},
 url = {https://www.deepseek.com/},
 year = {2023}
}

@article{dehghani2018universal,
 author = {Mostafa Dehghani and Stephan Gouws and O. Vinyals and Jakob Uszkoreit and Lukasz Kaiser},
 bibsource = {Semantic Scholar https://www.semanticscholar.org/paper/ac4dafdef1d2b685b7f28a11837414573d39ff4e},
 journal = {International Conference on Learning Representations},
 title = {Universal Transformers},
 year = {2018}
}

@inproceedings{deng2020joint,
 author = {Deng, Yang and Lam, Wai and Xie, Yuexiang and Chen, Daoyuan and Li, Yaliang and Yang, Min and Shen, Ying},
 booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
 number = {05},
 pages = {7651--7658},
 title = {Joint learning of answer selection and answer summary generation in community question answering},
 volume = {34},
 year = {2020}
}

@article{dettmers2022gpt3,
 author = {Dettmers, Tim and Lewis, Mike and Belkada, Younes and Zettlemoyer, Luke},
 journal = {Advances in Neural Information Processing Systems},
 pages = {30318--30332},
 title = {Gpt3. int8 (): 8-bit matrix multiplication for transformers at scale},
 volume = {35},
 year = {2022}
}

@article{dettmers2023spqr,
 author = {Dettmers, Tim and Svirschevski, Ruslan and Egiazarian, Vage and Kuznedelev, Denis and Frantar, Elias and Ashkboos, Saleh and Borzunov, Alexander and Hoefler, Torsten and Alistarh, Dan},
 journal = {arXiv preprint arXiv:2306.03078},
 title = {Spqr: A sparse-quantized representation for near-lossless llm weight compression},
 year = {2023}
}

@article{dettmers2024qlora,
 author = {Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
 journal = {Advances in Neural Information Processing Systems},
 title = {Qlora: Efficient finetuning of quantized llms},
 volume = {36},
 year = {2024}
}

@inproceedings{devlin-etal-2019-bert,
 address = {Minneapolis, Minnesota},
 author = {Devlin, Jacob  and
Chang, Ming-Wei  and
Lee, Kenton  and
Toutanova, Kristina},
 booktitle = {Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
 doi = {10.18653/v1/N19-1423},
 editor = {Burstein, Jill  and
Doran, Christy  and
Solorio, Thamar},
 month = {June},
 pages = {4171--4186},
 publisher = {Association for Computational Linguistics},
 title = {{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding},
 url = {https://aclanthology.org/N19-1423/},
 year = {2019}
}

@inproceedings{Devlin2019BERTPO,
 author = {Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
 booktitle = {North American Chapter of the Association for Computational Linguistics},
 title = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
 year = {2019}
}

@misc{devoto2024simpleeffectivel2normbased,
 archiveprefix = {arXiv},
 author = {Alessio Devoto and Yu Zhao and Simone Scardapane and Pasquale Minervini},
 eprint = {2406.11430},
 primaryclass = {cs.CL},
 title = {A Simple and Effective $L_2$ Norm-Based Strategy for KV Cache Compression},
 url = {https://arxiv.org/abs/2406.11430},
 year = {2024}
}

@article{ding2022cocomic,
 author = {Ding, Yangruibo and Wang, Zijian and Ahmad, Wasi Uddin and Ramanathan, Murali Krishna and Nallapati, Ramesh and Bhatia, Parminder and Roth, Dan and Xiang, Bing},
 journal = {arXiv preprint arXiv:2212.10007},
 title = {Cocomic: Code completion by jointly modeling in-file and cross-file context},
 year = {2022}
}

@article{ding2024longrope,
  title={Longrope: Extending llm context window beyond 2 million tokens},
  author={Ding, Yiran and Zhang, Li Lyna and Zhang, Chengruidong and Xu, Yuanyuan and Shang, Ning and Xu, Jiahang and Yang, Fan and Yang, Mao},
  journal={arXiv preprint arXiv:2402.13753},
  year={2024}
}

@inproceedings{dong2020eflops,
 author = {Dong, Jianbo and Cao, Zheng and Zhang, Tao and Ye, Jianxi and Wang, Shaochuang and Feng, Fei and Zhao, Li and Liu, Xiaoyong and Song, Liuyihan and Peng, Liwei and others},
 booktitle = {2020 IEEE International Symposium on High Performance Computer Architecture (HPCA)},
 organization = {IEEE},
 pages = {610--622},
 title = {Eflops: Algorithm and system co-design for a high performance distributed training platform},
 year = {2020}
}

@article{dong2023bamboo,
 author = {Dong, Zican and Tang, Tianyi and Li, Junyi and Zhao, Wayne Xin and Wen, Ji-Rong},
 journal = {arXiv preprint arXiv:2309.13345},
 title = {Bamboo: A comprehensive benchmark for evaluating long text modeling capacities of large language models},
 year = {2023}
}

@article{dong2024hymba,
 author = {Dong, Xin and Fu, Yonggan and Diao, Shizhe and Byeon, Wonmin and Chen, Zijia and Mahabaleshwarkar, Ameya Sunil and Liu, Shih-Yang and Van Keirsbilck, Matthijs and Chen, Min-Hung and Suhara, Yoshi and others},
 journal = {arXiv preprint arXiv:2411.13676},
 title = {Hymba: A hybrid-head architecture for small language models},
 year = {2024}
}

@misc{dong2024qaqqualityadaptivequantization,
 archiveprefix = {arXiv},
 author = {Shichen Dong and Wen Cheng and Jiayu Qin and Wei Wang},
 eprint = {2403.04643},
 primaryclass = {cs.CL},
 title = {QAQ: Quality Adaptive Quantization for LLM KV Cache},
 url = {https://arxiv.org/abs/2403.04643},
 year = {2024}
}

@inproceedings{drqa,
 author = {Chen, Danqi and Fisch, Adam and Weston, Jason and Bordes, Antoine},
 booktitle = {Association for Computational Linguistics (ACL)},
 title = {Reading {Wikipedia} to Answer Open-Domain Questions},
 year = {2017}
}

@article{dsi,
 author = {Yi Tay and Vinh Q. Tran and Mostafa Dehghani and Jianmo Ni and Dara Bahri and Harsh Mehta and Zhen Qin and Kai Hui and Zhe Zhao and Jai Gupta and Tal Schuster and William W. Cohen and Donald Metzler},
 journal = {arXiv preprint arXiv: 2202.06991},
 title = {Transformer Memory as a Differentiable Search Index},
 year = {2022}
}

@article{dsiplusplus,
 author = {Sanket Vaibhav Mehta and Jai Gupta and Yi Tay and Mostafa Dehghani and Vinh Q. Tran and J. Rao and Marc Najork and Emma Strubell and Donald Metzler},
 bibsource = {Semantic Scholar https://www.semanticscholar.org/paper/eebe62c685bffd1c8880fb3ce70e3b6d42b4b9e9},
 doi = {10.48550/arXiv.2212.09744},
 journal = {Conference on Empirical Methods in Natural Language Processing},
 title = {DSI++: Updating Transformer Memory with New Documents},
 year = {2022}
}

@article{du-2024-arxiv-VIM,
 author = {Yifan Du and
Kun Zhou and
Yuqi Huo and
Yifan Li and
Wayne Xin Zhao and
Haoyu Lu and
Zijia Zhao and
Bingning Wang and
Weipeng Chen and
Ji{-}Rong Wen},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/journals/corr/abs-2406-14129.bib},
 doi = {10.48550/ARXIV.2406.14129},
 eprint = {2406.14129},
 eprinttype = {arXiv},
 journal = {CoRR},
 timestamp = {Fri, 12 Jul 2024 19:38:50 +0200},
 title = {Towards Event-oriented Long Video Understanding},
 url = {https://doi.org/10.48550/arXiv.2406.14129},
 volume = {abs/2406.14129},
 year = {2024}
}

@article{DBLP:journals/corr/abs-2406-14129,
  author       = {Yifan Du and
                  Kun Zhou and
                  Yuqi Huo and
                  Yifan Li and
                  Wayne Xin Zhao and
                  Haoyu Lu and
                  Zijia Zhao and
                  Bingning Wang and
                  Weipeng Chen and
                  Ji{-}Rong Wen},
  title        = {Towards Event-oriented Long Video Understanding},
  journal      = {CoRR},
  volume       = {abs/2406.14129},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2406.14129},
  doi          = {10.48550/ARXIV.2406.14129},
  eprinttype    = {arXiv},
  eprint       = {2406.14129},
  timestamp    = {Fri, 12 Jul 2024 19:38:50 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2406-14129.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{du-etal-2022-glm,
 address = {Dublin, Ireland},
 author = {Du, Zhengxiao  and
Qian, Yujie  and
Liu, Xiao  and
Ding, Ming  and
Qiu, Jiezhong  and
Yang, Zhilin  and
Tang, Jie},
 booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
 doi = {10.18653/v1/2022.acl-long.26},
 editor = {Muresan, Smaranda  and
Nakov, Preslav  and
Villavicencio, Aline},
 month = {May},
 pages = {320--335},
 publisher = {Association for Computational Linguistics},
 title = {{GLM}: General Language Model Pretraining with Autoregressive Blank Infilling},
 url = {https://aclanthology.org/2022.acl-long.26/},
 year = {2022}
}

@inproceedings{Du2021GLaMES,
 author = {Nan Du and Yanping Huang and Andrew M. Dai and Simon Tong and Dmitry Lepikhin and Yuanzhong Xu and Maxim Krikun and Yanqi Zhou and Adams Wei Yu and Orhan Firat and Barret Zoph and Liam Fedus and Maarten Bosma and Zongwei Zhou and Tao Wang and Yu Emma Wang and Kellie Webster and Marie Pellat and Kevin Robinson and Kathleen S. Meier-Hellstern and Toju Duke and Lucas Dixon and Kun Zhang and Quoc V. Le and Yonghui Wu and Z. Chen and Claire Cui},
 booktitle = {International Conference on Machine Learning},
 title = {GLaM: Efficient Scaling of Language Models with Mixture-of-Experts},
 year = {2021}
}

@article{duan2024efficient,
 author = {Duan, Jiangfei and Zhang, Shuo and Wang, Zerui and Jiang, Lijuan and Qu, Wenwen and Hu, Qinghao and Wang, Guoteng and Weng, Qizhen and Yan, Hang and Zhang, Xingcheng and others},
 journal = {arXiv preprint arXiv:2407.20018},
 title = {Efficient training of large language models on distributed infrastructures: a survey},
 year = {2024}
}


@article{duan2025let,
  author       = {Sirui Duan and
                  Mengya Ouyang and
                  Rong Wang and
                  Qian Li and
                  Yunpeng Xiao},
  title        = {Let long-term interests talk: An disentangled learning model for recommendation
                  based on short-term interests generation},
  journal      = {Inf. Process. Manag.},
  volume       = {62},
  number       = {3},
  pages        = {103997},
  year         = {2025},
  url          = {https://doi.org/10.1016/j.ipm.2024.103997},
  doi          = {10.1016/J.IPM.2024.103997},
  timestamp    = {Mon, 03 Mar 2025 22:14:56 +0100},
  biburl       = {https://dblp.org/rec/journals/ipm/DuanOWLX25.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{duanmuskvq,
  title={SKVQ: Sliding-window Key and Value Cache Quantization for Large Language Models},
  author={Duanmu, Haojie and Yuan, Zhihang and Li, Xiuhong and Duan, Jiangfei and ZHANG, Xingcheng and Lin, Dahua},
  booktitle={First Conference on Language Modeling}
}

@article{grattafiori2024llama,
  title={The llama 3 herd of models},
  author={Grattafiori, Aaron and Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Vaughan, Alex and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}

@article{dufter2022position,
 author = {Dufter, Philipp and Schmitt, Martin and Sch{\"u}tze, Hinrich},
 journal = {Computational Linguistics},
 number = {3},
 pages = {733--763},
 title = {Position Information in Transformers: An Overview},
 volume = {48},
 year = {2022}
}

@article{dumitru2024change,
 author = {Dumitru, Razvan-Gabriel and Clotan, Paul-Ioan and Yadav, Vikas and Peteleaza, Darius and Surdeanu, Mihai},
 journal = {arXiv preprint arXiv:2411.03513},
 title = {Change Is the Only Constant: Dynamic LLM Slicing based on Layer Redundancy},
 year = {2024}
}

@inproceedings{elhoushi-etal-2024-layerskip,
 author = {Elhoushi, Mostafa  and
Shrivastava, Akshat  and
Liskovich, Diana  and
Hosmer, Basil  and
Wasti, Bram  and
Lai, Liangzhen  and
Mahmoud, Anas  and
Acun, Bilge  and
Agarwal, Saurabh  and
Roman, Ahmed  and
Aly, Ahmed  and
Chen, Beidi  and
Wu, Carole-Jean},
 booktitle = {Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
 editor = {Ku, Lun-Wei  and
Martins, Andre  and
Srikumar, Vivek},
 month = {August},
 title = {{L}ayer{S}kip: Enabling Early Exit Inference and Self-Speculative Decoding},
 year = {2024}
}

@article{ERNIE-SPARSE,
 author = {Yang Liu and
Jiaxiang Liu and
Li Chen and
Yuxiang Lu and
Shikun Feng and
Zhida Feng and
Yu Sun and
Hao Tian and
Hua Wu and
Haifeng Wang},
 journal = {CoRR},
 title = {{ERNIE-SPARSE:} Learning Hierarchical Efficient Transformer Through
Regularized Self-Attention},
 volume = {abs/2203.12276},
 year = {2022}
}

@article{extensibleembedding,
 author = {Ninglu Shao and
Shitao Xiao and
Zheng Liu and
Peitian Zhang},
 journal = {CoRR},
 title = {Extensible Embedding: {A} Flexible Multipler For LLM's Context Length},
 volume = {abs/2402.11577},
 year = {2024}
}

@inproceedings{fabbri2019multi,
 author = {Fabbri, Alexander Richard and Li, Irene and She, Tianwei and Li, Suyi and Radev, Dragomir},
 booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
 pages = {1074--1084},
 title = {Multi-News: A Large-Scale Multi-Document Summarization Dataset and Abstractive Hierarchical Model},
 year = {2019}
}

@article{fabbri2021qafacteval,
 author = {Fabbri, Alexander R and Wu, Chien-Sheng and Liu, Wenhao and Xiong, Caiming},
 journal = {arXiv preprint arXiv:2112.08542},
 title = {QAFactEval: Improved QA-based factual consistency evaluation for summarization},
 year = {2021}
}

@article{fan2019eli5,
 author = {Fan, Angela and Jernite, Yacine and Perez, Ethan and Grangier, David and Weston, Jason and Auli, Michael},
 journal = {arXiv preprint arXiv:1907.09190},
 title = {ELI5: Long form question answering},
 year = {2019}
}

@article{fan2024medodyssey,
 author = {Fan, Yongqi and Sun, Hongli and Xue, Kui and Zhang, Xiaofan and Zhang, Shaoting and Ruan, Tong},
 journal = {arXiv preprint arXiv:2406.15019},
 title = {MedOdyssey: A Medical Domain Benchmark for Long Context Evaluation Up to 200K Tokens},
 year = {2024}
}

@article{DBLP:journals/corr/abs-2406-15019,
  author       = {Yongqi Fan and
                  Hongli Sun and
                  Kui Xue and
                  Xiaofan Zhang and
                  Shaoting Zhang and
                  Tong Ruan},
  title        = {MedOdyssey: {A} Medical Domain Benchmark for Long Context Evaluation
                  Up to 200K Tokens},
  journal      = {CoRR},
  volume       = {abs/2406.15019},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2406.15019},
  doi          = {10.48550/ARXIV.2406.15019},
  eprinttype    = {arXiv},
  eprint       = {2406.15019},
  timestamp    = {Tue, 16 Jul 2024 16:17:53 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2406-15019.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{fang2024scaling,
 author = {Yan Fang and Jingtao Zhan and Qingyao Ai and Jiaxin Mao and Weihang Su and Jia Chen and Yiqun Liu},
 journal = {arXiv preprint arXiv: 2403.18684},
 title = {Scaling Laws For Dense Retrieval},
 year = {2024}
}

@article{fang2024wrong,
 author = {Fang, Lizhe and Wang, Yifei and Liu, Zhaoyang and Zhang, Chenheng and Jegelka, Stefanie and Gao, Jinyang and Ding, Bolin and Wang, Yisen},
 journal = {arXiv preprint arXiv:2410.23771},
 title = {What is Wrong with Perplexity for Long-context Language Modeling?},
 year = {2024}
}

@misc{fang2024wrongperplexitylongcontextlanguage,
 archiveprefix = {arXiv},
 author = {Lizhe Fang and Yifei Wang and Zhaoyang Liu and Chenheng Zhang and Stefanie Jegelka and Jinyang Gao and Bolin Ding and Yisen Wang},
 eprint = {2410.23771},
 primaryclass = {cs.CL},
 title = {What is Wrong with Perplexity for Long-context Language Modeling?},
 url = {https://arxiv.org/abs/2410.23771},
 year = {2024}
}

@article{fathullah2023multi,
 author = {Fathullah, Yassir and Wu, Chunyang and Shangguan, Yuan and Jia, Junteng and Xiong, Wenhan and Mahadeokar, Jay and Liu, Chunxi and Shi, Yangyang and Kalinli, Ozlem and Seltzer, Mike and others},
 journal = {arXiv preprint arXiv:2305.12498},
 title = {Multi-head state space model for speech recognition},
 year = {2023}
}

@article{faure-2024-arxiv-hermes,
 author = {Gueter Josmy Faure and
Jia-Fong Yeh and
Min-Hung Chen and
Hung-Ting Su and
Shang-Hong Lai and
Winston H. Hsu},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/journals/corr/abs-2408-17443.bib},
 doi = {10.48550/ARXIV.2408.17443},
 eprint = {2408.17443},
 eprinttype = {arXiv},
 journal = {CoRR},
 timestamp = {Thu, 17 Oct 2024 12:28:14 +0200},
 title = {HERMES: temporal-coHERent long-forM understanding with Episodes and Semantics},
 url = {https://doi.org/10.48550/arXiv.2408.17443},
 volume = {abs/2408.17443},
 year = {2024}
}

@misc{fei2023extendingcontextwindowlarge,
 archiveprefix = {arXiv},
 author = {Weizhi Fei and Xueyan Niu and Pingyi Zhou and Lu Hou and Bo Bai and Lei Deng and Wei Han},
 eprint = {2312.09571},
 primaryclass = {cs.CL},
 title = {Extending Context Window of Large Language Models via Semantic Compression},
 url = {https://arxiv.org/abs/2312.09571},
 year = {2023}
}

@misc{feng2024adakvoptimizingkvcache,
 archiveprefix = {arXiv},
 author = {Yuan Feng and Junlin Lv and Yukun Cao and Xike Xie and S. Kevin Zhou},
 eprint = {2407.11550},
 primaryclass = {cs.CL},
 title = {Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for Efficient LLM Inference},
 url = {https://arxiv.org/abs/2407.11550},
 year = {2024}
}

@misc{feng2025adakvoptimizingkvcache,
 archiveprefix = {arXiv},
 author = {Yuan Feng and Junlin Lv and Yukun Cao and Xike Xie and S. Kevin Zhou},
 eprint = {2407.11550},
 primaryclass = {cs.CL},
 title = {Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for Efficient LLM Inference},
 url = {https://arxiv.org/abs/2407.11550},
 year = {2025}
}

@article{frantar2022gptq,
 author = {Frantar, Elias and Ashkboos, Saleh and Hoefler, Torsten and Alistarh, Dan},
 journal = {arXiv preprint arXiv:2210.17323},
 title = {Gptq: Accurate post-training quantization for generative pre-trained transformers},
 year = {2022}
}

@misc{fu2024challengesdeployinglongcontexttransformers,
 archiveprefix = {arXiv},
 author = {Yao Fu},
 eprint = {2405.08944},
 primaryclass = {cs.LG},
 title = {Challenges in Deploying Long-Context Transformers: A Theoretical Peak Performance Analysis},
 url = {https://arxiv.org/abs/2405.08944},
 year = {2024}
}

@article{Fu2024DataEF,
 author = {Yao Fu and Rameswar Panda and Xinyao Niu and Xiang Yue and Hanna Hajishirzi and Yoon Kim and Hao Peng},
 journal = {ArXiv},
 title = {Data Engineering for Scaling Language Models to 128K Context},
 volume = {abs/2402.10171},
 year = {2024}
}

@misc{fu2024headsmatterheadlevelkv,
 archiveprefix = {arXiv},
 author = {Yu Fu and Zefan Cai and Abedelkadir Asi and Wayne Xiong and Yue Dong and Wen Xiao},
 eprint = {2410.19258},
 primaryclass = {cs.CL},
 title = {Not All Heads Matter: A Head-Level KV Cache Compression Method with Integrated Retrieval and Reasoning},
 url = {https://arxiv.org/abs/2410.19258},
 year = {2024}
}

@misc{fu2024lazyllmdynamictokenpruning,
 archiveprefix = {arXiv},
 author = {Qichen Fu and Minsik Cho and Thomas Merth and Sachin Mehta and Mohammad Rastegari and Mahyar Najibi},
 eprint = {2407.14057},
 primaryclass = {cs.CL},
 title = {LazyLLM: Dynamic Token Pruning for Efficient Long Context LLM Inference},
 url = {https://arxiv.org/abs/2407.14057},
 year = {2024}
}

@misc{fu2024moamixturesparseattention,
 archiveprefix = {arXiv},
 author = {Tianyu Fu and Haofeng Huang and Xuefei Ning and Genghan Zhang and Boju Chen and Tianqi Wu and Hongyi Wang and Zixiao Huang and Shiyao Li and Shengen Yan and Guohao Dai and Huazhong Yang and Yu Wang},
 eprint = {2406.14909},
 primaryclass = {cs.LG},
 title = {MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression},
 url = {https://arxiv.org/abs/2406.14909},
 year = {2024}
}

@article{fu2024moa,
  title={MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression},
  author={Fu, Tianyu and Huang, Haofeng and Ning, Xuefei and Zhang, Genghan and Chen, Boju and Wu, Tianqi and Wang, Hongyi and Huang, Zixiao and Li, Shiyao and Yan, Shengen and others},
  journal={CoRR},
  year={2024}
}



@inproceedings{fu2025not,
 author = {Yu Fu and Zefan Cai and Abedelkadir Asi and Wayne Xiong and Yue Dong and Wen Xiao},
 booktitle = {The Thirteenth International Conference on Learning Representations},
 title = {Not All Heads Matter: A Head-Level {KV} Cache Compression Method with Integrated Retrieval and Reasoning},
 url = {https://openreview.net/forum?id=FJFVmeXusW},
 year = {2025}
}

@inproceedings{fusion-in-decoder,
 abstract = {Generative models for open domain question answering have proven to be competitive, without resorting to external knowledge. While promising, this approach requires to use models with billions of parameters, which are expensive to train and query. In this paper, we investigate how much these models can benefit from retrieving text passages, potentially containing evidence. We obtain state-of-the-art results on the Natural Questions and TriviaQA open benchmarks. Interestingly, we observe that the performance of this method significantly improves when increasing the number of retrieved passages. This is evidence that sequence-to-sequence models offers a flexible framework to efficiently aggregate and combine evidence from multiple passages.},
 address = {Online},
 author = {Izacard, Gautier and Grave, Edouard},
 booktitle = {Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume},
 doi = {10.18653/v1/2021.eacl-main.74},
 editor = {Merlo, Paola and Tiedemann, Jorg and Tsarfaty, Reut},
 month = {apr},
 pages = {874-880},
 publisher = {Association for Computational Linguistics},
 title = {Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering},
 url = {https://aclanthology.org/2021.eacl-main.74/},
 year = {2021}
}

@article{gang-2024-arxiv-longvale,
 author = {Tiantian Geng and
Jinrui Zhang and
Qingni Wang and
Teng Wang and
Jinming Duan and
Feng Zheng},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/journals/corr/abs-2411-19772.bib},
 doi = {10.48550/ARXIV.2411.19772},
 eprint = {2411.19772},
 eprinttype = {arXiv},
 journal = {CoRR},
 timestamp = {Wed, 01 Jan 2025 14:15:41 +0100},
 title = {LongVALE: Vision-Audio-Language-Event Benchmark Towards Time-Aware
Omni-Modal Perception of Long Videos},
 url = {https://doi.org/10.48550/arXiv.2411.19772},
 volume = {abs/2411.19772},
 year = {2024}
}

@inproceedings{gao2019abstractive,
  author       = {Shen Gao and
                  Xiuying Chen and
                  Piji Li and
                  Zhaochun Ren and
                  Lidong Bing and
                  Dongyan Zhao and
                  Rui Yan},
  title        = {Abstractive Text Summarization by Incorporating Reader Comments},
  booktitle    = {The Thirty-Third {AAAI} Conference on Artificial Intelligence, {AAAI}
                  2019, The Thirty-First Innovative Applications of Artificial Intelligence
                  Conference, {IAAI} 2019, The Ninth {AAAI} Symposium on Educational
                  Advances in Artificial Intelligence, {EAAI} 2019, Honolulu, Hawaii,
                  USA, January 27 - February 1, 2019},
  pages        = {6399--6406},
  publisher    = {{AAAI} Press},
  year         = {2019},
  url          = {https://doi.org/10.1609/aaai.v33i01.33016399},
  doi          = {10.1609/AAAI.V33I01.33016399},
  timestamp    = {Mon, 04 Sep 2023 12:29:24 +0200},
  biburl       = {https://dblp.org/rec/conf/aaai/GaoCLRB0Y19.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{Gao2020ThePA,
 author = {Leo Gao and Stella Biderman and Sid Black and Laurence Golding and Travis Hoppe and Charles Foster and Jason Phang and Horace He and Anish Thite and Noa Nabeshima and Shawn Presser and Connor Leahy},
 journal = {ArXiv},
 title = {The Pile: An 800GB Dataset of Diverse Text for Language Modeling},
 volume = {abs/2101.00027},
 year = {2020}
}

@article{gao2024prolong,
 author = {Gao, Tianyu and Wettig, Alexander and Yen, Howard and Chen, Danqi},
 journal = {arXiv preprint arXiv:2410.02660},
 title = {How to Train Long-Context Language Models (Effectively)},
 year = {2024}
}

@misc{gao2024seerattentionlearningintrinsicsparse,
 archiveprefix = {arXiv},
 author = {Yizhao Gao and Zhichen Zeng and Dayou Du and Shijie Cao and Hayden Kwok-Hay So and Ting Cao and Fan Yang and Mao Yang},
 eprint = {2410.13276},
 primaryclass = {cs.CL},
 title = {SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs},
 url = {https://arxiv.org/abs/2410.13276},
 year = {2024}
}

@misc{gao2024unifyingdemonstrationselectioncompression,
 archiveprefix = {arXiv},
 author = {Jun Gao and Ziqiang Cao and Wenjie Li},
 eprint = {2405.17062},
 primaryclass = {cs.CL},
 title = {Unifying Demonstration Selection and Compression for In-Context Learning},
 url = {https://arxiv.org/abs/2405.17062},
 year = {2024}
}

@article{gavin2024longins,
 author = {Gavin, Shawn and Zheng, Tuney and Liu, Jiaheng and Que, Quehry and Wang, Noah and Yang, Jian and Zhang, Chenchen and Huang, Wenhao and Chen, Wenhu and Zhang, Ge},
 journal = {arXiv preprint arXiv:2406.17588},
 title = {LongIns: A Challenging Long-context Instruction-based Exam for LLMs},
 year = {2024}
}

@article{ge-2024-arxiv-v2pe,
 author = {Junqi Ge and
Ziyi Chen and
Jintao Lin and
Jinguo Zhu and
Xihui Liu and
Jifeng Dai and
Xizhou Zhu},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/journals/corr/abs-2412-09616.bib},
 doi = {10.48550/ARXIV.2412.09616},
 eprint = {2412.09616},
 eprinttype = {arXiv},
 journal = {CoRR},
 timestamp = {Thu, 17 Oct 2024 12:28:14 +0200},
 title = {V2PE: Improving Multimodal Long-Context Capability of Vision-Language Models with Variable Visual Position Encoding},
 url = {https://doi.org/10.48550/arXiv.2412.09616},
 volume = {abs/2412.09616},
 year = {2024}
}

@inproceedings{ge-etal-2024-clustering,
 address = {Miami, Florida, USA},
 author = {Ge, Yuan  and
Liu, Yilun  and
Hu, Chi  and
Meng, Weibin  and
Tao, Shimin  and
Zhao, Xiaofeng  and
Xia, Mahong  and
Li, Zhang  and
Chen, Boxing  and
Yang, Hao  and
Li, Bei  and
Xiao, Tong  and
Zhu, JingBo},
 booktitle = {Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
 doi = {10.18653/v1/2024.emnlp-main.28},
 editor = {Al-Onaizan, Yaser  and
Bansal, Mohit  and
Chen, Yun-Nung},
 month = {November},
 pages = {464--478},
 publisher = {Association for Computational Linguistics},
 title = {Clustering and Ranking: Diversity-preserved Instruction Selection through Expert-aligned Quality Estimation},
 url = {https://aclanthology.org/2024.emnlp-main.28},
 year = {2024}
}

@misc{ge2024incontextautoencodercontextcompression,
 archiveprefix = {arXiv},
 author = {Tao Ge and Jing Hu and Lei Wang and Xun Wang and Si-Qing Chen and Furu Wei},
 eprint = {2307.06945},
 primaryclass = {cs.CL},
 title = {In-context Autoencoder for Context Compression in a Large Language Model},
 url = {https://arxiv.org/abs/2307.06945},
 year = {2024}
}

@inproceedings{
ge2024incontext,
title={In-context Autoencoder for Context Compression in a Large Language Model},
author={Tao Ge and Hu Jing and Lei Wang and Xun Wang and Si-Qing Chen and Furu Wei},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=uREj4ZuGJE}
}

@inproceedings{ge2024model,
 author = {Suyu Ge and Yunan Zhang and Liyuan Liu and Minjia Zhang and Jiawei Han and Jianfeng Gao},
 booktitle = {The Twelfth International Conference on Learning Representations},
 title = {Model Tells You What to Discard: Adaptive {KV} Cache Compression for {LLM}s},
 url = {https://openreview.net/forum?id=uNrFpDPMyo},
 year = {2024}
}

@misc{ge2024modeltellsdiscardadaptive,
 archiveprefix = {arXiv},
 author = {Suyu Ge and Yunan Zhang and Liyuan Liu and Minjia Zhang and Jiawei Han and Jianfeng Gao},
 eprint = {2310.01801},
 primaryclass = {cs.CL},
 title = {Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs},
 url = {https://arxiv.org/abs/2310.01801},
 year = {2024}
}

@inproceedings{gehring2017convolutional,
 author = {Gehring, Jonas and Auli, Michael and Grangier, David and Yarats, Denis and Dauphin, Yann N},
 booktitle = {International conference on machine learning},
 organization = {PMLR},
 pages = {1243--1252},
 title = {Convolutional sequence to sequence learning},
 year = {2017}
}

@article{generative-agents,
 author = {J. Park and Joseph C. O'Brien and Carrie J. Cai and M. Morris and Percy Liang and Michael S. Bernstein},
 bibsource = {Semantic Scholar https://www.semanticscholar.org/paper/5278a8eb2ba2429d4029745caf4e661080073c81},
 doi = {10.1145/3586183.3606763},
 journal = {ACM Symposium on User Interface Software and Technology},
 title = {Generative Agents: Interactive Simulacra of Human Behavior},
 year = {2023}
}

@misc{genspark,
 author = {genspark},
 title = {genspark.ai},
 url = {https://www.genspark.ai/}
}

@inproceedings{geteau,
 author = {Shuzheng Si and Haozhe Zhao and Gang Chen and Yunshui Li and Kangyang Luo and Chuancheng Lv and Kaikai An and Fanchao Qi and Baobao Chang and Maosong Sun},
 title = {GATEAU: Selecting Influential Sample for Long Context Alignment},
 year = {2024}
}

@article{gholami2024ai,
 author = {Gholami, Amir and Yao, Zhewei and Kim, Sehoon and Hooper, Coleman and Mahoney, Michael W and Keutzer, Kurt},
 journal = {IEEE Micro},
 publisher = {IEEE},
 title = {AI and memory wall},
 year = {2024}
}

@misc{github_copilot,
 author = {GitHub},
 title = {GitHub Copilot},
 url = {https://github.com/copilot},
 year = {2022}
}

@inproceedings{glam,
 author = {Nan Du and
Yanping Huang and
Andrew M. Dai and
Simon Tong and
Dmitry Lepikhin and
Yuanzhong Xu and
Maxim Krikun and
Yanqi Zhou and
Adams Wei Yu and
Orhan Firat and
Barret Zoph and
Liam Fedus and
Maarten P. Bosma and
Zongwei Zhou and
Tao Wang and
Yu Emma Wang and
Kellie Webster and
Marie Pellat and
Kevin Robinson and
Kathleen S. Meier{-}Hellstern and
Toju Duke and
Lucas Dixon and
Kun Zhang and
Quoc V. Le and
Yonghui Wu and
Zhifeng Chen and
Claire Cui},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/icml/DuHDTLXKZYFZFBZ22.bib},
 booktitle = {International Conference on Machine Learning, {ICML} 2022, 17-23 July
2022, Baltimore, Maryland, {USA}},
 editor = {Kamalika Chaudhuri and
Stefanie Jegelka and
Le Song and
Csaba Szepesv{\'{a}}ri and
Gang Niu and
Sivan Sabato},
 pages = {5547--5569},
 publisher = {{PMLR}},
 series = {Proceedings of Machine Learning Research},
 timestamp = {Wed, 16 Aug 2023 16:10:28 +0200},
 title = {GLaM: Efficient Scaling of Language Models with Mixture-of-Experts},
 url = {https://proceedings.mlr.press/v162/du22c.html},
 volume = {162},
 year = {2022}
}

@misc{glm2024chatglm,
 archiveprefix = {arXiv},
 author = {Team GLM and Aohan Zeng and Bin Xu and Bowen Wang and Chenhui Zhang and Da Yin and Diego Rojas and Guanyu Feng and Hanlin Zhao and Hanyu Lai and Hao Yu and Hongning Wang and Jiadai Sun and Jiajie Zhang and Jiale Cheng and Jiayi Gui and Jie Tang and Jing Zhang and Juanzi Li and Lei Zhao and Lindong Wu and Lucen Zhong and Mingdao Liu and Minlie Huang and Peng Zhang and Qinkai Zheng and Rui Lu and Shuaiqi Duan and Shudan Zhang and Shulin Cao and Shuxun Yang and Weng Lam Tam and Wenyi Zhao and Xiao Liu and Xiao Xia and Xiaohan Zhang and Xiaotao Gu and Xin Lv and Xinghan Liu and Xinyi Liu and Xinyue Yang and Xixuan Song and Xunkai Zhang and Yifan An and Yifan Xu and Yilin Niu and Yuantao Yang and Yueyan Li and Yushi Bai and Yuxiao Dong and Zehan Qi and Zhaoyu Wang and Zhen Yang and Zhengxiao Du and Zhenyu Hou and Zihan Wang},
 eprint = {2406.12793},
 primaryclass = {id='cs.CL' full_name='Computation and Language' is_active=True alt_name='cmp-lg' in_archive='cs' is_general=False description='Covers natural language processing. Roughly includes material in ACM Subject Class I.2.7. Note that work on artificial languages (programming languages, logics, formal systems) that does not explicitly address natural-language issues broadly construed (natural-language processing, computational linguistics, speech, text retrieval, etc.) is not appropriate for this area.'},
 title = {ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools},
 year = {2024}
}

@article{glorioso2024zamba,
 author = {Glorioso, Paolo and Anthony, Quentin and Tokpanov, Yury and Whittington, James and Pilault, Jonathan and Ibrahim, Adam and Millidge, Beren},
 journal = {arXiv preprint arXiv:2405.16712},
 title = {Zamba: A compact 7b ssm hybrid model},
 year = {2024}
}

@article{glorioso2024zamba2,
 author = {Glorioso, Paolo and Anthony, Quentin and Tokpanov, Yury and Golubeva, Anna and Shyam, Vasudev and Whittington, James and Pilault, Jonathan and Millidge, Beren},
 journal = {arXiv preprint arXiv:2411.15242},
 title = {The Zamba2 Suite: Technical Report},
 year = {2024}
}

@article{glorioso2024zambacompact7bssm,
 archiveprefix = {arXiv},
 author = {Paolo Glorioso and Quentin Anthony and Yury Tokpanov and James Whittington and Jonathan Pilault and Adam Ibrahim and Beren Millidge},
 eprint = {2405.16712},
 primaryclass = {cs.LG},
 title = {Zamba: A Compact 7B SSM Hybrid Model},
 url = {https://arxiv.org/abs/2405.16712},
 year = {2024}
}


@article{godbole2024leveraging,
  author       = {Aditi S. Godbole and
                  Jabin Geevarghese George and
                  Smita Shandilya},
  title        = {Leveraging Long-Context Large Language Models for Multi-Document Understanding
                  and Summarization in Enterprise Applications},
  journal      = {CoRR},
  volume       = {abs/2409.18454},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2409.18454},
  doi          = {10.48550/ARXIV.2409.18454},
  eprinttype    = {arXiv},
  eprint       = {2409.18454},
  timestamp    = {Thu, 17 Oct 2024 17:43:24 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2409-18454.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{goldman2024really,
 author = {Goldman, Omer and Jacovi, Alon and Slobodkin, Aviv and Maimon, Aviya and Dagan, Ido and Tsarfaty, Reut},
 booktitle = {Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
 pages = {16576--16586},
 title = {Is It Really Long Context if All You Need Is Retrieval? Towards Genuinely Difficult Long Context NLP},
 year = {2024}
}

@article{goldstein2024goldfinch,
 author = {Goldstein, Daniel and Obeid, Fares and Alcaide, Eric and Song, Guangyu and Cheah, Eugene},
 journal = {arXiv preprint arXiv:2407.12077},
 title = {Goldfinch: High performance rwkv/transformer hybrid with linear pre-fill and extreme kv-cache compression},
 year = {2024}
}

@article{golovneva2024contextual,
 author = {Golovneva, Olga and Wang, Tianlu and Weston, Jason and Sukhbaatar, Sainbayar},
 journal = {arXiv preprint arXiv:2405.18719},
 title = {Contextual Position Encoding: Learning to Count What's Important},
 year = {2024}
}

@book{goodfellow2016deep,
 author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
 publisher = {MIT Press},
 title = {Deep learning},
 volume = {1},
 year = {2016}
}

@article{gopher,
 author = {Jack W. Rae and
Sebastian Borgeaud and
Trevor Cai and
Katie Millican and
Jordan Hoffmann and
H. Francis Song and
John Aslanides and
Sarah Henderson and
Roman Ring and
Susannah Young and
Eliza Rutherford and
Tom Hennigan and
Jacob Menick and
Albin Cassirer and
Richard Powell and
George van den Driessche and
Lisa Anne Hendricks and
Maribeth Rauh and
Po{-}Sen Huang and
Amelia Glaese and
Johannes Welbl and
Sumanth Dathathri and
Saffron Huang and
Jonathan Uesato and
John Mellor and
Irina Higgins and
Antonia Creswell and
Nat McAleese and
Amy Wu and
Erich Elsen and
Siddhant M. Jayakumar and
Elena Buchatskaya and
David Budden and
Esme Sutherland and
Karen Simonyan and
Michela Paganini and
Laurent Sifre and
Lena Martens and
Xiang Lorraine Li and
Adhiguna Kuncoro and
Aida Nematzadeh and
Elena Gribovskaya and
Domenic Donato and
Angeliki Lazaridou and
Arthur Mensch and
Jean{-}Baptiste Lespiau and
Maria Tsimpoukelli and
Nikolai Grigorev and
Doug Fritz and
Thibault Sottiaux and
Mantas Pajarskas and
Toby Pohlen and
Zhitao Gong and
Daniel Toyama and
Cyprien de Masson d'Autume and
Yujia Li and
Tayfun Terzi and
Vladimir Mikulik and
Igor Babuschkin and
Aidan Clark and
Diego de Las Casas and
Aurelia Guy and
Chris Jones and
James Bradbury and
Matthew J. Johnson and
Blake A. Hechtman and
Laura Weidinger and
Iason Gabriel and
William Isaac and
Edward Lockhart and
Simon Osindero and
Laura Rimell and
Chris Dyer and
Oriol Vinyals and
Kareem Ayoub and
Jeff Stanway and
Lorrayne Bennett and
Demis Hassabis and
Koray Kavukcuoglu and
Geoffrey Irving},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/journals/corr/abs-2112-11446.bib},
 eprint = {2112.11446},
 eprinttype = {arXiv},
 journal = {CoRR},
 timestamp = {Sat, 02 Dec 2023 13:23:51 +0100},
 title = {Scaling Language Models: Methods, Analysis {\&} Insights from
Training Gopher},
 url = {https://arxiv.org/abs/2112.11446},
 volume = {abs/2112.11446},
 year = {2021}
}

@article{graphreader,
 author = {Shilong Li and Yancheng He and Hangyu Guo and Xingyuan Bu and Ge Bai and Jie Liu and Jiaheng Liu and Xingwei Qu and Yangguang Li and Wanli Ouyang and Wenbo Su and Bo Zheng},
 bibsource = {Semantic Scholar https://www.semanticscholar.org/paper/dbfb49de5ae1b351991d6c55e8179ff4640ed60e},
 doi = {10.48550/arXiv.2406.14550},
 journal = {Conference on Empirical Methods in Natural Language Processing},
 title = {GraphReader: Building Graph-based Agent to Enhance Long-Context Abilities of Large Language Models},
 year = {2024}
}

@article{gritlm,
 author = {Niklas Muennighoff and Hongjin Su and Liang Wang and Nan Yang and Furu Wei and Tao Yu and Amanpreet Singh and Douwe Kiela},
 journal = {arXiv preprint arXiv: 2402.09906},
 title = {Generative Representational Instruction Tuning},
 year = {2024}
}

@article{growlength,
 author = {Hongye Jin and Xiaotian Han and Jingfeng Yang and Zhimeng Jiang and Chia-yuan Chang and Xia Hu},
 journal = {ArXiv},
 title = {GrowLength: Accelerating LLMs Pretraining by Progressively Growing Training Length},
 volume = {abs/2310.00576},
 year = {2023}
}


@inproceedings{Gu2020HiPPORM,
  author       = {Albert Gu and
                  Tri Dao and
                  Stefano Ermon and
                  Atri Rudra and
                  Christopher R{\'{e}}},
  title        = {HiPPO: Recurrent Memory with Optimal Polynomial Projections},
  booktitle    = {NeurIPS},
  year         = {2020}
}

@inproceedings{gu2021combining,
  author       = {Albert Gu and
                  Isys Johnson and
                  Karan Goel and
                  Khaled Saab and
                  Tri Dao and
                  Atri Rudra and
                  Christopher R{\'{e}}},
  title        = {Combining Recurrent, Convolutional, and Continuous-time Models with
                  Linear State Space Layers},
  booktitle    = {NeurIPS},
  pages        = {572--585},
  year         = {2021}
}

@inproceedings{gu2022efficiently,
 author = {Gu, Albert and Goel, Karan and R\'e, Christopher},
 booktitle = {The International Conference on Learning Representations ({ICLR})},
 title = {Efficiently Modeling Long Sequences with Structured State Spaces},
 year = {2022}
}

@article{gu2023mamba,
 author = {Gu, Albert and Dao, Tri},
 journal = {arXiv preprint arXiv:2312.00752},
 title = {Mamba: Linear-time sequence modeling with selective state spaces},
 year = {2023}
}

@inproceedings{gu2024mamba,
 author = {Albert Gu and Tri Dao},
 booktitle = {First Conference on Language Modeling},
 title = {Mamba: Linear-Time Sequence Modeling with Selective State Spaces},
 url = {https://openreview.net/forum?id=tEYskw1VY2},
 year = {2024}
}

@article{guan2022lot,
 author = {Guan, Jian and Feng, Zhuoer and Chen, Yamei and He, Ruilin and Mao, Xiaoxi and Fan, Changjie and Huang, Minlie},
 journal = {Transactions of the Association for Computational Linguistics},
 pages = {434--451},
 publisher = {MIT Press One Broadway, 12th Floor, Cambridge, Massachusetts 02142, USA~…},
 title = {LOT: A story-centric benchmark for evaluating Chinese long text understanding and generation},
 volume = {10},
 year = {2022}
}

@inproceedings{guan2024aptq,
 author = {Guan, Ziyi and Huang, Hantao and Su, Yupeng and Huang, Hong and Wong, Ngai and Yu, Hao},
 booktitle = {Proceedings of the 61st ACM/IEEE Design Automation Conference},
 pages = {1--6},
 title = {Aptq: Attention-aware post-training mixed-precision quantization for large language models},
 year = {2024}
}

@article{guo2021longt5,
 author = {Guo, Mandy and Ainslie, Joshua and Uthus, David and Ontanon, Santiago and Ni, Jianmo and Sung, Yun-Hsuan and Yang, Yinfei},
 journal = {arXiv preprint arXiv:2112.07916},
 title = {LongT5: Efficient text-to-text transformer for long sequences},
 year = {2021}
}

@inproceedings{DBLP:conf/naacl/GuoAUONSY22,
  author       = {Mandy Guo and
                  Joshua Ainslie and
                  David C. Uthus and
                  Santiago Onta{\~{n}}{\'{o}}n and
                  Jianmo Ni and
                  Yun{-}Hsuan Sung and
                  Yinfei Yang},
  editor       = {Marine Carpuat and
                  Marie{-}Catherine de Marneffe and
                  Iv{\'{a}}n Vladimir Meza Ru{\'{\i}}z},
  title        = {LongT5: Efficient Text-To-Text Transformer for Long Sequences},
  booktitle    = {Findings of the Association for Computational Linguistics: {NAACL}
                  2022, Seattle, WA, United States, July 10-15, 2022},
  pages        = {724--736},
  publisher    = {Association for Computational Linguistics},
  year         = {2022},
  url          = {https://doi.org/10.18653/v1/2022.findings-naacl.55},
  doi          = {10.18653/V1/2022.FINDINGS-NAACL.55},
  timestamp    = {Mon, 01 Aug 2022 16:28:01 +0200},
  biburl       = {https://dblp.org/rec/conf/naacl/GuoAUONSY22.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{guo2023longcoder,
 author = {Guo, Daya and Xu, Canwen and Duan, Nan and Yin, Jian and McAuley, Julian},
 booktitle = {International Conference on Machine Learning},
 organization = {PMLR},
 pages = {12098--12107},
 title = {Longcoder: A long-range pre-trained language model for code completion},
 year = {2023}
}

@article{guo2025deepseek,
 author = {Guo, Daya and Yang, Dejian and Zhang, Haowei and Song, Junxiao and Zhang, Ruoyu and Xu, Runxin and Zhu, Qihao and Ma, Shirong and Wang, Peiyi and Bi, Xiao and others},
 journal = {arXiv preprint arXiv:2501.12948},
 title = {Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning},
 year = {2025}
}


@article{gupta2024comprehensive,
  author       = {Shailja Gupta and
                  Rajesh Ranjan and
                  Surya Narayan Singh},
  title        = {A Comprehensive Survey of Retrieval-Augmented Generation {(RAG):}
                  Evolution, Current Landscape and Future Directions},
  journal      = {CoRR},
  volume       = {abs/2410.12837},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2410.12837},
  doi          = {10.48550/ARXIV.2410.12837},
  eprinttype    = {arXiv},
  eprint       = {2410.12837},
  timestamp    = {Sun, 24 Nov 2024 18:57:58 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2410-12837.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@article{günther2024jinaembeddings28192token,
  author       = {Michael G{\"{u}}nther and
                  Jackmin Ong and
                  Isabelle Mohr and
                  Alaeddine Abdessalem and
                  Tanguy Abel and
                  Mohammad Kalim Akram and
                  Susana Guzman and
                  Georgios Mastrapas and
                  Saba Sturua and
                  Bo Wang and
                  Maximilian Werk and
                  Nan Wang and
                  Han Xiao},
  title        = {Jina Embeddings 2: 8192-Token General-Purpose Text Embeddings for
                  Long Documents},
  journal      = {CoRR},
  volume       = {abs/2310.19923},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2310.19923},
  doi          = {10.48550/ARXIV.2310.19923},
  eprinttype    = {arXiv},
  eprint       = {2310.19923},
  timestamp    = {Fri, 03 Nov 2023 10:56:40 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2310-19923.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{günther2024late,
 author = {Michael Günther and Isabelle Mohr and Daniel James Williams and Bo Wang and Han Xiao},
 journal = {arXiv preprint arXiv: 2409.04701},
 title = {Late Chunking: Contextual Chunk Embeddings Using Long-Context Embedding Models},
 year = {2024}
}

@inproceedings{han,
 author = {Zichao Yang and
Diyi Yang and
Chris Dyer and
Xiaodong He and
Alexander J. Smola and
Eduard H. Hovy},
 booktitle = {{HLT-NAACL}},
 pages = {1480--1489},
 publisher = {The Association for Computational Linguistics},
 title = {Hierarchical Attention Networks for Document Classification},
 year = {2016}
}

@article{han-2024-arixv-ficoco,
 author = {Yuhang Han and
Xuyang Liu and
Pengxiang Ding and
Donglin Wang and
Honggang Chen and
Qingsen Yan and
Siteng Huang},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/journals/corr/abs-2411-17686.bib},
 doi = {10.48550/ARXIV.2411.17686},
 eprint = {2411.17686},
 eprinttype = {arXiv},
 journal = {CoRR},
 timestamp = {Wed, 01 Jan 2025 14:15:31 +0100},
 title = {Rethinking Token Reduction in MLLMs: Towards a Unified Paradigm for
Training-Free Acceleration},
 url = {https://doi.org/10.48550/arXiv.2411.17686},
 volume = {abs/2411.17686},
 year = {2024}
}


@inproceedings{han2023flattentransformervisiontransformer,
  author       = {Dongchen Han and
                  Xuran Pan and
                  Yizeng Han and
                  Shiji Song and
                  Gao Huang},
  title        = {FLatten Transformer: Vision Transformer using Focused Linear Attention},
  booktitle    = {{ICCV}},
  pages        = {5938--5948},
  publisher    = {{IEEE}},
  year         = {2023}
}
@inproceedings{han-etal-2024-lm,
    title = "{LM}-Infinite: Zero-Shot Extreme Length Generalization for Large Language Models",
    author = "Han, Chi  and
      Wang, Qifan  and
      Peng, Hao  and
      Xiong, Wenhan  and
      Chen, Yu  and
      Ji, Heng  and
      Wang, Sinong",
    editor = "Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven",
    booktitle = "Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.naacl-long.222/",
    doi = "10.18653/v1/2024.naacl-long.222",
    pages = "3991--4008",
    abstract = "Today`s large language models (LLMs) typically train on short text segments (e.g., {\ensuremath{<}}4K tokens) due to the quadratic complexity of their Transformer architectures. As a result, their performance suffers drastically on inputs longer than those encountered during training, substantially limiting their applications in real-world tasks involving long contexts such as encod- ing scientific articles, code repositories, or long dialogues. Through both theoretical analysis and empirical investigation, this work identifies three major factors contributing to this length generalization failure. Our theoretical analysis reveals that commonly used techniques like using a sliding-window attention pattern or relative positional encodings are inadequate to address them. Answering these challenges, we propose LM-Infinite, a simple and effective method for enhancing LLMs' capabilities of handling long contexts. LM-Infinite is highly flexible and can be used with most modern LLMs off-the-shelf. Without any parameter updates, it allows LLMs pre-trained with 2K or 4K-long segments to generalize to up to 200M length inputs while retaining perplexity. It also improves performance on downstream tasks such as Passkey Retrieval and Qasper in the zero-shot setting. LM-Infinite brings substantial efficiency improvements: it achieves 2.7{\texttimes} decoding speed up and 7.5{\texttimes} memory saving over the original model. Our code will be publicly available upon publication."
}

@article{han2024ragqaarena,
 author = {Rujun Han and Yuhao Zhang and Peng Qi and Yumo Xu and Jenyuan Wang and Lan Liu and William Yang Wang and Bonan Min and Vittorio Castelli},
 journal = {arXiv preprint arXiv:2407.13998},
 title = {RAG-QA Arena: Evaluating Domain Robustness for Long-form Retrieval Augmented Question Answering},
 url = {https://arxiv.org/abs/2407.13998},
 year = {2024}
}

@article{hannan-2024-arxiv-revisionllm,
 author = {Tanveer Hannan and
Md Mohaiminul Islam and
Jindong Gu and
Thomas Seidl and
Gedas Bertasius},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/journals/corr/abs-2411-14901.bib},
 doi = {10.48550/ARXIV.2411.14901},
 eprint = {2411.14901},
 eprinttype = {arXiv},
 journal = {CoRR},
 timestamp = {Wed, 01 Jan 2025 13:20:27 +0100},
 title = {ReVisionLLM: Recursive Vision-Language Model for Temporal Grounding
in Hour-Long Videos},
 url = {https://doi.org/10.48550/arXiv.2411.14901},
 volume = {abs/2411.14901},
 year = {2024}
}

@inproceedings{he2018dureader,
 author = {He, Wei and Liu, Kai and Liu, Jing and Lyu, Yajuan and Zhao, Shiqi and Xiao, Xinyan and Liu, Yuan and Wang, Yizhong and Wu, Hua and She, Qiaoqiao and others},
 booktitle = {Proceedings of the Workshop on Machine Reading for Question Answering},
 pages = {37--46},
 title = {DuReader: a Chinese Machine Reading Comprehension Dataset from Real-world Applications},
 year = {2018}
}

@article{He2023WanJuanAC,
 author = {Conghui He and Zhenjiang Jin and Chaoxi Xu and Jiantao Qiu and Bin Wang and Wei Li and Hang Yan and Jiaqi Wang and Da Lin},
 journal = {ArXiv},
 title = {WanJuan: A Comprehensive Multimodal Dataset for Advancing English and Chinese Large Models},
 url = {https://api.semanticscholar.org/CorpusID:261049100},
 volume = {abs/2308.10755},
 year = {2023}
}

@article{he2024fastdecode,
 author = {He, Jiaao and Zhai, Jidong},
 journal = {arXiv preprint arXiv:2403.11421},
 title = {Fastdecode: High-throughput gpu-efficient llm serving using heterogeneous pipelines},
 year = {2024}
}

@article{he2023never,
  title={Never Lost in the Middle: Mastering Long-Context Question Answering with Position-Agnostic Decompositional Training},
  author={He, Junqing and Pan, Kunhao and Dong, Xiaoqun and Song, Zhuoyang and Liu, Yibo and Sun, Qianguo and Liang, Yuxin and Wang, Hao and Zhang, Enming and Zhang, Jiaxing},
  journal={arXiv preprint arXiv:2311.09198},
  year={2023}
}

@article{he2024two,
 author = {He, Zhenyu and Feng, Guhao and Luo, Shengjie and Yang, Kai and Wang, Liwei and Xu, Jingjing and Zhang, Zhi and Yang, Hongxia and He, Di},
 journal = {arXiv preprint arXiv:2401.16421},
 title = {Two stones hit one bird: Bilevel positional encoding for better length extrapolation},
 year = {2024}
}

@article{he2025can,
 author = {He, Yancheng and Li, Shilong and Liu, Jiaheng and Wang, Weixun and Bu, Xingyuan and Zhang, Ge and Peng, Zhongyuan and Zhang, Zhaoxiang and Su, Wenbo and Zheng, Bo},
 journal = {arXiv preprint arXiv:2502.19361},
 title = {Can Large Language Models Detect Errors in Long Chain-of-Thought Reasoning?},
 year = {2025}
}

@article{hendrycks2020measuring,
 author = {Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou, Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob},
 journal = {arXiv preprint arXiv:2009.03300},
 title = {Measuring massive multitask language understanding},
 year = {2020}
}

@misc{hengle2024multilingualneedlehaystackinvestigating,
 archiveprefix = {arXiv},
 author = {Amey Hengle and Prasoon Bajpai and Soham Dan and Tanmoy Chakraborty},
 eprint = {2408.10151},
 primaryclass = {cs.CL},
 title = {Multilingual Needle in a Haystack: Investigating Long-Context Behavior of Multilingual Large Language Models},
 url = {https://arxiv.org/abs/2408.10151},
 year = {2024}
}


@article{herold2023improving,
  author       = {Christian Herold and
                  Hermann Ney},
  title        = {Improving Long Context Document-Level Machine Translation},
  journal      = {CoRR},
  volume       = {abs/2306.05183},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2306.05183},
  doi          = {10.48550/ARXIV.2306.05183},
  eprinttype    = {arXiv},
  eprint       = {2306.05183},
  timestamp    = {Wed, 14 Jun 2023 13:17:00 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2306-05183.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{hi-transformer,
 author = {Chuhan Wu and
Fangzhao Wu and
Tao Qi and
Yongfeng Huang},
 booktitle = {{ACL/IJCNLP} {(2)}},
 pages = {848--853},
 publisher = {Association for Computational Linguistics},
 title = {Hi-Transformer: Hierarchical Interactive Transformer for Efficient
and Effective Long Document Modeling},
 year = {2021}
}


@inproceedings{hilgert2024evaluating,
    title = "Evaluating and Training Long-Context Large Language Models for Question Answering on Scientific Papers",
    author = "Hilgert, Lukas  and
      Liu, Danni  and
      Niehues, Jan",
    editor = "Kumar, Sachin  and
      Balachandran, Vidhisha  and
      Park, Chan Young  and
      Shi, Weijia  and
      Hayati, Shirley Anugrah  and
      Tsvetkov, Yulia  and
      Smith, Noah  and
      Hajishirzi, Hannaneh  and
      Kang, Dongyeop  and
      Jurgens, David",
    booktitle = "Proceedings of the 1st Workshop on Customizable NLP: Progress and Challenges in Customizing NLP for a Domain, Application, Group, or Individual (CustomNLP4U)",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.customnlp4u-1.17/",
    doi = "10.18653/v1/2024.customnlp4u-1.17",
    pages = "220--236",
    abstract = "With the number of scientific papers published every year growing and current large language models (LLMs) showing state-of-the-art performance on natural language processing (NLP) tasks, we ask the question if LLMs could be utilized to answer questions on scientific papers.We investigate how well state-of-the-art large language models (LLMs) can answer questions on scientific paper by experimenting with long-context versions of the LLaMA 2 model and evaluating and training on the Qasper dataset.We analyze how well the LLMs handle longer papers and questions that can only be answered by accessing information from far out paragraphs. During our experiments, we see that the performance of these LLMs drops with growing length and position of relevant information.We employ different measures from simple prompts to chain-of-thought prompts and zero-shot usage to fine-tuning with QLoRA.While we still observe a performance loss with increased context length, our measures reduce the effects of this flaw, and we can achieve $F_{1}$ scores similar to bigger models like GPT-4."
}

@article{Hinton06,
 author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee Whye},
 journal = {Neural Computation},
 pages = {1527--1554},
 title = {A Fast Learning Algorithm for Deep Belief Nets},
 volume = {18},
 year = {2006}
}

@inproceedings{ho2020constructing,
 author = {Ho, Xanh and Nguyen, Anh-Khoa Duong and Sugawara, Saku and Aizawa, Akiko},
 booktitle = {Proceedings of the 28th International Conference on Computational Linguistics},
 pages = {6609--6625},
 title = {Constructing A Multi-hop QA Dataset for Comprehensive Evaluation of Reasoning Steps},
 year = {2020}
}

@inproceedings{hoffmann2022an,
 author = {Jordan Hoffmann and Sebastian Borgeaud and Arthur Mensch and Elena Buchatskaya and Trevor Cai and Eliza Rutherford and Diego de las Casas and Lisa Anne Hendricks and Johannes Welbl and Aidan Clark and Tom Hennigan and Eric Noland and Katherine Millican and George van den Driessche and Bogdan Damoc and Aurelia Guy and Simon Osindero and Karen Simonyan and Erich Elsen and Oriol Vinyals and Jack William Rae and Laurent Sifre},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
 title = {An empirical analysis of compute-optimal large language model training},
 year = {2022}
}

@article{hooper2023speed,
 author = {Hooper, Coleman and Kim, Sehoon and Mohammadzadeh, Hiva and Genc, Hasan and Keutzer, Kurt and Gholami, Amir and Shao, Sophia},
 journal = {arXiv preprint arXiv:2310.12072},
 title = {Speed: Speculative pipelined execution for efficient decoding},
 year = {2023}
}

@article{hooper2025kvquant,
  title={Kvquant: Towards 10 million context length llm inference with kv cache quantization},
  author={Hooper, Coleman and Kim, Sehoon and Mohammadzadeh, Hiva and Mahoney, Michael W and Shao, Sophia and Keutzer, Kurt and Gholami, Amir},
  journal={Advances in Neural Information Processing Systems},
  volume={37},
  pages={1270--1303},
  year={2025}
}

@article{hosseini2024benchmark,
 author = {Hosseini, Pedram and Sin, Jessica M and Ren, Bing and Thomas, Bryceton G and Nouri, Elnaz and Farahanchi, Ali and Hassanpour, Saeed},
 journal = {arXiv preprint arXiv:2411.09834},
 title = {A Benchmark for Long-Form Medical Question Answering},
 year = {2024}
}

@inproceedings{hou2024large,
 author = {Hou, Yupeng and Zhang, Junjie and Lin, Zihan and Lu, Hongyu and Xie, Ruobing and McAuley, Julian and Zhao, Wayne Xin},
 booktitle = {European Conference on Information Retrieval},
 organization = {Springer},
 pages = {364--381},
 title = {Large language models are zero-shot rankers for recommender systems},
 year = {2024}
}

@inproceedings{hsieh2024ruler,
  title={RULER: What’s the Real Context Size of Your Long-Context Language Models?},
  author={Hsieh, Cheng-Ping and Sun, Simeng and Kriman, Samuel and Acharya, Shantanu and Rekesh, Dima and Jia, Fei and Ginsburg, Boris},
  year={2024},
  booktitle={First Conference on Language Modeling}
}

@inproceedings{hu2024can,
  title={Can Perplexity Reflect Large Language Model's Ability in Long Text Understanding?},
  author={Hu, Yutong and Huang, Quzhe and Tao, Mingxu and Zhang, Chen and Feng, Yansong},
  booktitle={The Second Tiny Papers Track at ICLR 2024}
}

@article{Hu2024CanPR,
 author = {Yutong Hu and Quzhe Huang and Mingxu Tao and Chen Zhang and Yansong Feng},
 journal = {ArXiv},
 title = {Can Perplexity Reflect Large Language Model's Ability in Long Text Understanding?},
 url = {https://api.semanticscholar.org/CorpusID:269741336},
 volume = {abs/2405.06105},
 year = {2024}
}

@article{hu2024dawn,
 author = {Hu, Siyuan and Ouyang, Mingyu and Gao, Difei and Shou, Mike Zheng},
 journal = {arXiv preprint arXiv:2411.10323},
 title = {The Dawn of GUI Agent: A Preliminary Case Study with Claude 3.5 Computer Use},
 year = {2024}
}

@article{DBLP:journals/corr/abs-2411-10323,
  author       = {Siyuan Hu and
                  Mingyu Ouyang and
                  Difei Gao and
                  Mike Zheng Shou},
  title        = {The Dawn of {GUI} Agent: {A} Preliminary Case Study with Claude 3.5
                  Computer Use},
  journal      = {CoRR},
  volume       = {abs/2411.10323},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2411.10323},
  doi          = {10.48550/ARXIV.2411.10323},
  eprinttype    = {arXiv},
  eprint       = {2411.10323},
  timestamp    = {Wed, 01 Jan 2025 13:20:07 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2411-10323.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{hu2024kindstokensbenefitdistant,
 archiveprefix = {arXiv},
 author = {Yutong Hu and Quzhe Huang and Kangcheng Luo and Yansong Feng},
 eprint = {2406.11238},
 primaryclass = {cs.CL},
 title = {What Kinds of Tokens Benefit from Distant Text? An Analysis on Long Context Language Modeling},
 url = {https://arxiv.org/abs/2406.11238},
 year = {2024}
}

@article{hu2024longrecipe,
 author = {Hu, Zhiyuan and Liu, Yuliang and Zhao, Jinman and Wang, Suyuchen and Wang, Yan and Shen, Wei and Gu, Qing and Luu, Anh Tuan and Ng, See-Kiong and Jiang, Zhiwei and others},
 journal = {arXiv preprint arXiv:2409.00509},
 title = {LongRecipe: Recipe for Efficient Long Context Generalization in Large Language Models},
 year = {2024}
}

@article{hu2024memserve,
 author = {Hu, Cunchen and Huang, Heyang and Hu, Junhao and Xu, Jiang and Chen, Xusheng and Xie, Tao and Wang, Chenxi and Wang, Sa and Bao, Yungang and Sun, Ninghui and others},
 journal = {arXiv preprint arXiv:2406.17565},
 title = {Memserve: Context caching for disaggregated llm serving with elastic memory pool},
 year = {2024}
}

@article{hu2025efficient,
 author = {Hu, Junhao and Huang, Wenrui and Wang, Weidong and Li, Zhenwen and Hu, Tiancheng and Liu, Zhixia and Chen, Xusheng and Xie, Tao and Shan, Yizhou},
 journal = {arXiv preprint arXiv:2502.11147},
 title = {Efficient Long-Decoding Inference with Reasoning-Aware Attention Sparsity},
 year = {2025}
}

@article{huagents,
 author = {Xueyu Hu and Tao Xiong and Biao Yi and Zishu Wei and Ruixuan Xiao and Yurun Chen and Jiasheng Ye and Meiling Tao and Xiangxin Zhou and Ziyu Zhao and Yuhuai Li and Shengze Xu and Shawn Wang and Xinchen Xu and Shuofei Qiao and Kun Kuang and Tieyong Zeng and Liang Wang and Jiwei Li and Yuchen Eleanor Jiang and Wangchunshu Zhou and Guoyin Wang and Keting Yin and Zhou Zhao and Hongxia Yang and Fan Wu and Shengyu Zhang and Fei Wu},
 doi = {10.20944/preprints202412.2294.v1},
 journal = {Preprints},
 month = {December},
 publisher = {Preprints},
 title = {OS Agents: A Survey on MLLM-Based Agents for General Computing Devices Use},
 url = {https://doi.org/10.20944/preprints202412.2294.v1},
 year = {2024}
}

@misc{hu2024agents,
  title={Os agents: A survey on mllm-based agents for general computing devices use},
  author={Hu, Xueyu and Xiong, Tao and Yi, Biao and Wei, Zishu and Xiao, Ruixuan and Chen, Yurun and Ye, Jiasheng and Tao, Meiling and Zhou, Xiangxin and Zhao, Ziyu and others},
  year={2024},
  publisher={Preprints}
}

@inproceedings{huang2018improving,
 author = {Huang, Jin and Zhao, Wayne Xin and Dou, Hongjian and Wen, Ji-Rong and Chang, Edward Y},
 booktitle = {The 41st international ACM SIGIR conference on research \& development in information retrieval},
 pages = {505--514},
 title = {Improving sequential recommendation with knowledge-enhanced memory networks},
 year = {2018}
}

@inproceedings{huang2021efficient,
 author = {Huang, Luyang and Cao, Shuyang and Parulian, Nikolaus and Ji, Heng and Wang, Lu},
 booktitle = {Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
 pages = {1419--1436},
 title = {Efficient Attentions for Long Document Summarization},
 year = {2021}
}

@inproceedings{huang2023benchmarking,
 author = {Huang, Qian and Vora, Jian and Liang, Percy and Leskovec, Jure},
 booktitle = {NeurIPS 2023 Foundation Models for Decision Making Workshop},
 title = {Benchmarking large language models as ai research agents},
 year = {2023}
}

@article{DBLP:journals/corr/abs-2310-03302,
  author       = {Qian Huang and
                  Jian Vora and
                  Percy Liang and
                  Jure Leskovec},
  title        = {Benchmarking Large Language Models As {AI} Research Agents},
  journal      = {CoRR},
  volume       = {abs/2310.03302},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2310.03302},
  doi          = {10.48550/ARXIV.2310.03302},
  eprinttype    = {arXiv},
  eprint       = {2310.03302},
  timestamp    = {Thu, 19 Oct 2023 13:12:52 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2310-03302.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{huangcompression,
  title={Compression Represents Intelligence Linearly},
  author={Huang, Yuzhen and Zhang, Jinghan and Shan, Zifei and He, Junxian},
  booktitle={First Conference on Language Modeling}
}

@misc{huang2024fewermoreboostingllm,
 archiveprefix = {arXiv},
 author = {Xijie Huang and Li Lyna Zhang and Kwang-Ting Cheng and Fan Yang and Mao Yang},
 eprint = {2312.08901},
 primaryclass = {cs.CL},
 title = {Fewer is More: Boosting LLM Reasoning with Reinforced Context Pruning},
 url = {https://arxiv.org/abs/2312.08901},
 year = {2024}
}

@article{huang2024o1,
 author = {Huang, Zhen and Zou, Haoyang and Li, Xuefeng and Liu, Yixiu and Zheng, Yuxiang and Chern, Ethan and Xia, Shijie and Qin, Yiwei and Yuan, Weizhe and Liu, Pengfei},
 journal = {arXiv preprint arXiv:2411.16489},
 title = {O1 Replication Journey--Part 2: Surpassing O1-preview through Simple Distillation, Big Progress or Bitter Lesson?},
 year = {2024}
}

@inproceedings{Huang2024OpenCoderTO,
 author = {Siming Huang and Tianhao Cheng and Jason Klein Liu and Jiaran Hao and Liuyihan Song and Yang Xu and J. Yang and J. H. Liu and Chenchen Zhang and Linzheng Chai and Ruifeng Yuan and Zhaoxiang Zhang and Jie Fu and Qian Liu and Ge Zhang and Zili Wang and Yuan Qi and Yinghui Xu and Wei Chu},
 title = {OpenCoder: The Open Cookbook for Top-Tier Code Large Language Models},
 url = {https://arxiv.org/pdf/2411.04905},
 year = {2024}
}

@article{huang2024slim,
 author = {Huang, Wei and Qin, Haotong and Liu, Yangdong and Li, Yawei and Liu, Xianglong and Benini, Luca and Magno, Michele and Qi, Xiaojuan},
 journal = {arXiv preprint arXiv:2405.14917},
 title = {SliM-LLM: Salience-Driven Mixed-Precision Quantization for Large Language Models},
 year = {2024}
}


@article{hui2024qwen2,
  author       = {Binyuan Hui and
                  Jian Yang and
                  Zeyu Cui and
                  Jiaxi Yang and
                  Dayiheng Liu and
                  Lei Zhang and
                  Tianyu Liu and
                  Jiajun Zhang and
                  Bowen Yu and
                  Kai Dang and
                  An Yang and
                  Rui Men and
                  Fei Huang and
                  Xingzhang Ren and
                  Xuancheng Ren and
                  Jingren Zhou and
                  Junyang Lin},
  title        = {Qwen2.5-Coder Technical Report},
  journal      = {CoRR},
  volume       = {abs/2409.12186},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2409.12186},
  doi          = {10.48550/ARXIV.2409.12186},
  eprinttype    = {arXiv},
  eprint       = {2409.12186},
  timestamp    = {Mon, 03 Feb 2025 09:03:00 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2409-12186.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{Qwen2VL,
  title={Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution},
  author={Wang, Peng and Bai, Shuai and Tan, Sinan and Wang, Shijie and Fan, Zhihao and Bai, Jinze and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and Fan, Yang and Dang, Kai and Du, Mengfei and Ren, Xuancheng and Men, Rui and Liu, Dayiheng and Zhou, Chang and Zhou, Jingren and Lin, Junyang},
  journal={arXiv preprint arXiv:2409.12191},
  year={2024}
}


@article{infinitransformer,
 author = {Tsendsuren Munkhdalai and
Manaal Faruqui and
Siddharth Gopal},
 journal = {CoRR},
 title = {Leave No Context Behind: Efficient Infinite Context Transformers with
Infini-attention},
 volume = {abs/2404.07143},
 year = {2024}
}

@misc{inflection2023impi,
 author = {Inflection},
 howpublished = {\url{https://inflection.ai/}},
 title = {I’m Pi, Your personal AI},
 URL = {https://inflection.ai/},
 year = {2023}
}

@misc{talkie,
 author = {Talkie Ai},
 title = {Talkie | AI-Native Character Community},
 URL = {https://www.talkie-ai.com/},
 year = {2024}
}

@misc{spicychat,
 author = {spicychat.ai},
 title = {SpicyChat},
 URL = {https://spicychat.ai/},
 year = {2024}
}

@inproceedings{inoue2020r4c,
 author = {Inoue, Naoya and Stenetorp, Pontus and Inui, Kentaro},
 booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
 pages = {6740--6750},
 title = {R4C: A Benchmark for Evaluating RC Systems to Get the Right Answer for the Right Reason},
 year = {2020}
}

@article{jacobs2023deepspeed,
 author = {Jacobs, Sam Ade and Tanaka, Masahiro and Zhang, Chengming and Zhang, Minjia and Song, Shuaiwen Leon and Rajbhandari, Samyam and He, Yuxiong},
 journal = {arXiv preprint arXiv:2309.14509},
 title = {Deepspeed ulysses: System optimizations for enabling training of extreme long sequence transformer models},
 year = {2023}
}

@article{jacot2018neural,
 author = {Jacot, Arthur and Gabriel, Franck and Hongler, Cl{\'e}ment},
 journal = {Advances in neural information processing systems},
 title = {Neural tangent kernel: Convergence and generalization in neural networks},
 volume = {31},
 year = {2018}
}

@article{jacovi2025facts,
 author = {Jacovi, Alon and Wang, Andrew and Alberti, Chris and Tao, Connie and Lipovetz, Jon and Olszewska, Kate and Haas, Lukas and Liu, Michelle and Keating, Nate and Bloniarz, Adam and others},
 journal = {arXiv preprint arXiv:2501.03200},
 title = {The FACTS Grounding Leaderboard: Benchmarking LLMs' Ability to Ground Responses to Long-Form Input},
 year = {2025}
}

@inproceedings{jang-2024-emnlp-mate,
 author = {Young Kyun Jang and
Junmo Kang and
Yong Jae Lee and
Donghyun Kim},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/emnlp/JangKLK24.bib},
 booktitle = {Findings of the Association for Computational Linguistics: {EMNLP}
2024, Miami, Florida, USA, November 12-16, 2024},
 editor = {Yaser Al{-}Onaizan and
Mohit Bansal and
Yun{-}Nung Chen},
 pages = {1659--1672},
 publisher = {Association for Computational Linguistics},
 timestamp = {Mon, 18 Nov 2024 09:05:59 +0100},
 title = {{MATE:} Meet At The Embedding - Connecting Images with Long Texts},
 url = {https://aclanthology.org/2024.findings-emnlp.90},
 year = {2024}
}

@misc{jasper_ai,
 author = {Jasper AI},
 howpublished = {\url{https://www.jasper.ai/}},
 title = {Jasper AI Homepage}
}

@article{jeong2024olaph,
 author = {Jeong, Minbyul and Hwang, Hyeon and Yoon, Chanwoong and Lee, Taewhoo and Kang, Jaewoo},
 journal = {arXiv preprint arXiv:2405.12701},
 title = {OLAPH: Improving Factuality in Biomedical Long-form Question Answering},
 year = {2024}
}

@article{jeung2024large,
 author = {Jeung, Wonje and Jeon, Dongjae and Yousefpour, Ashkan and Choi, Jonghyun},
 journal = {arXiv preprint arXiv:2410.17519},
 title = {Large Language Models Still Exhibit Bias in Long Text},
 year = {2024}
}

@article{jiang-2024-arxiv-manyshots,
 author = {Yixing Jiang and
Jeremy Irvin and
Ji Hun Wang and
Muhammad Ahmed Chaudhry and
Jonathan H. Chen and
Andrew Y. Ng},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/journals/corr/abs-2405-09798.bib},
 doi = {10.48550/ARXIV.2405.09798},
 eprint = {2405.09798},
 eprinttype = {arXiv},
 journal = {CoRR},
 timestamp = {Sun, 06 Oct 2024 21:25:09 +0200},
 title = {Many-Shot In-Context Learning in Multimodal Foundation Models},
 url = {https://doi.org/10.48550/arXiv.2405.09798},
 volume = {abs/2405.09798},
 year = {2024}
}

@misc{jiang2023llmlinguacompressingpromptsaccelerated,
 archiveprefix = {arXiv},
 author = {Huiqiang Jiang and Qianhui Wu and Chin-Yew Lin and Yuqing Yang and Lili Qiu},
 eprint = {2310.05736},
 primaryclass = {cs.CL},
 title = {LLMLingua: Compressing Prompts for Accelerated Inference of Large Language Models},
 url = {https://arxiv.org/abs/2310.05736},
 year = {2023}
}

@inproceedings{jiang-etal-2023-llmlingua,
    title = "{LLML}ingua: Compressing Prompts for Accelerated Inference of Large Language Models",
    author = "Jiang, Huiqiang  and
      Wu, Qianhui  and
      Lin, Chin-Yew  and
      Yang, Yuqing  and
      Qiu, Lili",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.825/",
    doi = "10.18653/v1/2023.emnlp-main.825",
    pages = "13358--13376",
    abstract = "Large language models (LLMs) have been applied in various applications due to their astonishing capabilities. With advancements in technologies such as chain-of-thought (CoT) prompting and in-context learning (ICL), the prompts fed to LLMs are becoming increasingly lengthy, even exceeding tens of thousands of tokens. To accelerate model inference and reduce cost, this paper presents LLMLingua, a coarse-to-fine prompt compression method that involves a budget controller to maintain semantic integrity under high compression ratios, a token-level iterative compression algorithm to better model the interdependence between compressed contents, and an instruction tuning based method for distribution alignment between language models. We conduct experiments and analysis over four datasets from different scenarios, i.e., GSM8K, BBH, ShareGPT, and Arxiv-March23; showing that the proposed approach yields state-of-the-art performance and allows for up to 20x compression with little performance loss."
}

@misc{jiang2023mistral7b,
 archiveprefix = {arXiv},
 author = {Albert Q. Jiang and Alexandre Sablayrolles and Arthur Mensch and Chris Bamford and Devendra Singh Chaplot and Diego de las Casas and Florian Bressand and Gianna Lengyel and Guillaume Lample and Lucile Saulnier and Lélio Renard Lavaud and Marie-Anne Lachaux and Pierre Stock and Teven Le Scao and Thibaut Lavril and Thomas Wang and Timothée Lacroix and William El Sayed},
 eprint = {2310.06825},
 primaryclass = {cs.CL},
 title = {Mistral 7B},
 url = {https://arxiv.org/abs/2310.06825},
 year = {2023}
}

@article{jiang2024efficient,
 author = {Jiang, Chaoyi and Gao, Lei and Zarch, Hossein Entezari and Annavaram, Murali},
 journal = {arXiv preprint arXiv:2411.17089},
 title = {Efficient llm inference with i/o-aware partial kv cache recomputation},
 year = {2024}
}

@misc{jiang2024longllmlinguaacceleratingenhancingllms,
 archiveprefix = {arXiv},
 author = {Huiqiang Jiang and Qianhui Wu and Xufang Luo and Dongsheng Li and Chin-Yew Lin and Yuqing Yang and Lili Qiu},
 eprint = {2310.06839},
 primaryclass = {cs.CL},
 title = {LongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios via Prompt Compression},
 url = {https://arxiv.org/abs/2310.06839},
 year = {2024}
}

@inproceedings{jiang-etal-2024-longllmlingua,
    title = "{L}ong{LLML}ingua: Accelerating and Enhancing {LLM}s in Long Context Scenarios via Prompt Compression",
    author = "Jiang, Huiqiang  and
      Wu, Qianhui  and
      Luo, Xufang  and
      Li, Dongsheng  and
      Lin, Chin-Yew  and
      Yang, Yuqing  and
      Qiu, Lili",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.91/",
    doi = "10.18653/v1/2024.acl-long.91",
    pages = "1658--1677",
    abstract = "In long context scenarios, large language models (LLMs) face three main challenges: higher computational cost, performance reduction, and position bias. Research indicates that LLM performance hinges on the density and position of key information in the input prompt. Inspired by these findings, we propose LongLLMLingua for prompt compression towards improving LLMs' perception of the key information to simultaneously address the three challenges. Our extensive evaluation across various long context scenarios demonstrates that LongLLMLingua not only enhances performance but also significantly reduces costs and latency. For instance, in the NaturalQuestions benchmark, LongLLMLingua boosts performance by up to 21.4{\%} with around 4x fewer tokens in GPT-3.5-Turbo, leading to substantial cost savings. It achieves a 94.0{\%} cost reduction in the LooGLE benchmark. Moreover, when compressing prompts of about 10k tokens at ratios of 2x-6x, LongLLMLingua can accelerate end-to-end latency by 1.4x-2.6x."
}

@article{jiang2024longrag,
 author = {Jiang, Ziyan and Ma, Xueguang and Chen, Wenhu},
 journal = {arXiv preprint arXiv:2406.15319},
 title = {Longrag: Enhancing retrieval-augmented generation with long-context llms},
 year = {2024}
}

@article{DBLP:journals/corr/abs-2406-15319,
  author       = {Ziyan Jiang and
                  Xueguang Ma and
                  Wenhu Chen},
  title        = {LongRAG: Enhancing Retrieval-Augmented Generation with Long-context
                  LLMs},
  journal      = {CoRR},
  volume       = {abs/2406.15319},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2406.15319},
  doi          = {10.48550/ARXIV.2406.15319},
  eprinttype    = {arXiv},
  eprint       = {2406.15319},
  timestamp    = {Tue, 16 Jul 2024 16:17:54 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2406-15319.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{jiang2024minference,
  title={Minference 1.0: Accelerating pre-filling for long-context llms via dynamic sparse attention},
  author={Jiang, Huiqiang and Li, Yucheng and Zhang, Chengruidong and Wu, Qianhui and Luo, Xufang and Ahn, Surin and Han, Zhenhua and Abdi, Amir and Li, Dongsheng and Lin, Chin-Yew and others},
  journal={Advances in Neural Information Processing Systems},
  volume={37},
  pages={52481--52515},
  year={2024}
}

@inproceedings{NEURIPS2024_5dfbe6f5,
 author = {Jiang, Huiqiang and LI, Yucheng and Zhang, Chengruidong and Wu, Qianhui and Luo, Xufang and Ahn, Surin and Han, Zhenhua and Abdi, Amir and Li, Dongsheng and Lin, Chin-Yew and Yang, Yuqing and Qiu, Lili},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {52481--52515},
 publisher = {Curran Associates, Inc.},
 title = {MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/5dfbe6f5671e82c76841ba687a8a9ecb-Paper-Conference.pdf},
 volume = {37},
 year = {2024}
}


@article{jiang2024survey,
  author       = {Juyong Jiang and
                  Fan Wang and
                  Jiasi Shen and
                  Sungju Kim and
                  Sunghun Kim},
  title        = {A Survey on Large Language Models for Code Generation},
  journal      = {CoRR},
  volume       = {abs/2406.00515},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2406.00515},
  doi          = {10.48550/ARXIV.2406.00515},
  eprinttype    = {arXiv},
  eprint       = {2406.00515},
  timestamp    = {Fri, 12 Jul 2024 08:13:49 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2406-00515.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{jimenez2023swe,
 author = {Jimenez, Carlos E and Yang, John and Wettig, Alexander and Yao, Shunyu and Pei, Kexin and Press, Ofir and Narasimhan, Karthik},
 journal = {arXiv preprint arXiv:2310.06770},
 title = {Swe-bench: Can language models resolve real-world github issues?},
 year = {2023}
}

@inproceedings{jimenez2024swebench,
  author       = {Carlos E. Jimenez and
                  John Yang and
                  Alexander Wettig and
                  Shunyu Yao and
                  Kexin Pei and
                  Ofir Press and
                  Karthik R. Narasimhan},
  title        = {SWE-bench: Can Language Models Resolve Real-world Github Issues?},
  booktitle    = {The Twelfth International Conference on Learning Representations,
                  {ICLR} 2024, Vienna, Austria, May 7-11, 2024},
  publisher    = {OpenReview.net},
  year         = {2024},
  url          = {https://openreview.net/forum?id=VTF8yNQM66},
  timestamp    = {Mon, 29 Jul 2024 17:17:48 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/JimenezYWYPPN24.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@article{jin2024comprehensive,
  author       = {Hanlei Jin and
                  Yang Zhang and
                  Dan Meng and
                  Jun Wang and
                  Jinghua Tan},
  title        = {A Comprehensive Survey on Process-Oriented Automatic Text Summarization
                  with Exploration of LLM-Based Methods},
  journal      = {CoRR},
  volume       = {abs/2403.02901},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2403.02901},
  doi          = {10.48550/ARXIV.2403.02901},
  eprinttype    = {arXiv},
  eprint       = {2403.02901},
  timestamp    = {Wed, 03 Apr 2024 15:23:01 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2403-02901.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{jin2024llm,
 author = {Jin, Hongye and Han, Xiaotian and Yang, Jingfeng and Jiang, Zhimeng and Liu, Zirui and Chang, Chia-Yuan and Chen, Huiyuan and Hu, Xia},
 journal = {arXiv preprint arXiv:2401.01325},
 title = {Llm maybe longlm: Self-extend llm context window without tuning},
 year = {2024}
}

@article{jin2024long,
 author = {Jin, Bowen and Yoon, Jinsung and Han, Jiawei and Arik, Sercan O},
 journal = {arXiv preprint arXiv:2410.05983},
 title = {Long-Context LLMs Meet RAG: Overcoming Challenges for Long Inputs in RAG},
 year = {2024}
}

@article{DBLP:journals/corr/abs-2410-05983,
  author       = {Bowen Jin and
                  Jinsung Yoon and
                  Jiawei Han and
                  Sercan {\"{O}}. Arik},
  title        = {Long-Context LLMs Meet {RAG:} Overcoming Challenges for Long Inputs
                  in {RAG}},
  journal      = {CoRR},
  volume       = {abs/2410.05983},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2410.05983},
  doi          = {10.48550/ARXIV.2410.05983},
  eprinttype    = {arXiv},
  eprint       = {2410.05983},
  timestamp    = {Mon, 18 Nov 2024 14:52:10 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2410-05983.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{jo2024understanding,
  author       = {Eunkyung Jo and
                  Yuin Jeong and
                  SoHyun Park and
                  Daniel A. Epstein and
                  Young{-}Ho Kim},
  editor       = {Florian 'Floyd' Mueller and
                  Penny Kyburz and
                  Julie R. Williamson and
                  Corina Sas and
                  Max L. Wilson and
                  Phoebe O. Toups Dugas and
                  Irina Shklovski},
  title        = {Understanding the Impact of Long-Term Memory on Self-Disclosure with
                  Large Language Model-Driven Chatbots for Public Health Intervention},
  booktitle    = {Proceedings of the {CHI} Conference on Human Factors in Computing
                  Systems, {CHI} 2024, Honolulu, HI, USA, May 11-16, 2024},
  pages        = {440:1--440:21},
  publisher    = {{ACM}},
  year         = {2024},
  url          = {https://doi.org/10.1145/3613904.3642420},
  doi          = {10.1145/3613904.3642420},
  timestamp    = {Sun, 19 Jan 2025 13:11:47 +0100},
  biburl       = {https://dblp.org/rec/conf/chi/JoJPEK24.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{joshi2017triviaqa,
 author = {Joshi, Mandar and Choi, Eunsol and Weld, Daniel S and Zettlemoyer, Luke},
 booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
 pages = {1601--1611},
 title = {TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension},
 year = {2017}
}

@article{joshi2024reaper,
 author = {Ashutosh Joshi and Sheikh Muhammad Sarwar and Samarth Varshney and Sreyashi Nag and Shrivats Agrawal and Juhi Naik},
 journal = {arXiv preprint arXiv: 2407.18553},
 title = {REAPER: Reasoning based Retrieval Planning for Complex RAG Systems},
 year = {2024}
}

@misc{jung2024familiarityawareevidencecompressionretrievalaugmented,
 archiveprefix = {arXiv},
 author = {Dongwon Jung and Qin Liu and Tenghao Huang and Ben Zhou and Muhao Chen},
 eprint = {2409.12468},
 primaryclass = {cs.CL},
 title = {Familiarity-Aware Evidence Compression for Retrieval-Augmented Generation},
 url = {https://arxiv.org/abs/2409.12468},
 year = {2024}
}

@article{Jung_2024,
 author = {Jung, Hoyoun and Kim, Kyung-Joong},
 doi = {10.1109/access.2024.3403426},
 issn = {2169-3536},
 journal = {IEEE Access},
 pages = {72578–72587},
 publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
 title = {Discrete Prompt Compression With Reinforcement Learning},
 url = {http://dx.doi.org/10.1109/ACCESS.2024.3403426},
 volume = {12},
 year = {2024}
}

@article{kalman1960new,
 author = {Kalman, Rudolph Emil},
 title = {A new approach to linear filtering and prediction problems},
 year = {1960}
}

@article{kalra2024hypa,
  author       = {Rishi Kalra and
                  Zekun Wu and
                  Ayesha Gulley and
                  Airlie Hilliard and
                  Xin Guan and
                  Adriano S. Koshiyama and
                  Philip C. Treleaven},
  title        = {HyPA-RAG: {A} Hybrid Parameter Adaptive Retrieval-Augmented Generation
                  System for {AI} Legal and Policy Applications},
  journal      = {CoRR},
  volume       = {abs/2409.09046},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2409.09046},
  doi          = {10.48550/ARXIV.2409.09046},
  eprinttype    = {arXiv},
  eprint       = {2409.09046},
  timestamp    = {Sat, 12 Oct 2024 00:13:26 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2409-09046.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{kang2024gearefficientkvcache,
 archiveprefix = {arXiv},
 author = {Hao Kang and Qingru Zhang and Souvik Kundu and Geonhwa Jeong and Zaoxing Liu and Tushar Krishna and Tuo Zhao},
 eprint = {2403.05527},
 primaryclass = {cs.LG},
 title = {GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless Generative Inference of LLM},
 url = {https://arxiv.org/abs/2403.05527},
 year = {2024}
}


@article{kapoor2024promises,
  author       = {Sayash Kapoor and
                  Peter Henderson and
                  Arvind Narayanan},
  title        = {Promises and pitfalls of artificial intelligence for legal applications},
  journal      = {CoRR},
  volume       = {abs/2402.01656},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2402.01656},
  doi          = {10.48550/ARXIV.2402.01656},
  eprinttype    = {arXiv},
  eprint       = {2402.01656},
  timestamp    = {Thu, 18 Jul 2024 15:01:22 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2402-01656.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{katharopoulos2020transformersrnnsfastautoregressive,
  author       = {Angelos Katharopoulos and
                  Apoorv Vyas and
                  Nikolaos Pappas and
                  Fran{\c{c}}ois Fleuret},
  title        = {Transformers are RNNs: Fast Autoregressive Transformers with Linear
                  Attention},
  booktitle    = {{ICML}},
  series       = {Proceedings of Machine Learning Research},
  volume       = {119},
  pages        = {5156--5165},
  publisher    = {{PMLR}},
  year         = {2020}
}

@article{kazemnejad2024impact,
 author = {Kazemnejad, Amirhossein and Padhi, Inkit and Natesan Ramamurthy, Karthikeyan and Das, Payel and Reddy, Siva},
 journal = {Advances in Neural Information Processing Systems},
 title = {The impact of positional encoding on length generalization in transformers},
 volume = {36},
 year = {2024}
}

@article{kim2023memory,
 author = {Kim, Jeonghoon and Lee, Jung Hyun and Kim, Sungdong and Park, Joonsuk and Yoo, Kang Min and Kwon, Se Jung and Lee, Dongsoo},
 journal = {Advances in Neural Information Processing Systems},
 pages = {36187--36207},
 title = {Memory-efficient fine-tuning of compressed large language models via sub-4-bit integer quantization},
 volume = {36},
 year = {2023}
}

@article{kim2023squeezellm,
 author = {Kim, Sehoon and Hooper, Coleman and Gholami, Amir and Dong, Zhen and Li, Xiuyu and Shen, Sheng and Mahoney, Michael W and Keutzer, Kurt},
 journal = {arXiv preprint arXiv:2306.07629},
 title = {Squeezellm: Dense-and-sparse quantization},
 year = {2023}
}

@article{knn-lm,
 author = {Urvashi Khandelwal and Omer Levy and Dan Jurafsky and Luke Zettlemoyer and M. Lewis},
 bibsource = {Semantic Scholar https://www.semanticscholar.org/paper/7be8c119dbe065c52125ee7716601751f3116844},
 journal = {International Conference on Learning Representations},
 title = {Generalization through Memorization: Nearest Neighbor Language Models},
 year = {2019}
}

@article{koksal2023longform,
 author = {K{\"o}ksal, Abdullatif and Schick, Timo and Korhonen, Anna and Sch{\"u}tze, Hinrich},
 journal = {arXiv preprint arXiv:2304.08460},
 title = {LongForm: Effective Instruction Tuning with Reverse Instructions},
 year = {2023}
}

@article{korthikanti2023reducing,
 author = {Korthikanti, Vijay Anand and Casper, Jared and Lym, Sangkug and McAfee, Lawrence and Andersch, Michael and Shoeybi, Mohammad and Catanzaro, Bryan},
 journal = {Proceedings of Machine Learning and Systems},
 pages = {341--353},
 title = {Reducing activation recomputation in large transformer models},
 volume = {5},
 year = {2023}
}

@article{kovcisky2018narrativeqa,
 author = {Ko{\v{c}}isk{\`y}, Tom{\'a}{\v{s}} and Schwarz, Jonathan and Blunsom, Phil and Dyer, Chris and Hermann, Karl Moritz and Melis, G{\'a}bor and Grefenstette, Edward},
 journal = {Transactions of the Association for Computational Linguistics},
 pages = {317--328},
 publisher = {MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…},
 title = {The narrativeqa reading comprehension challenge},
 volume = {6},
 year = {2018}
}

@article{krell2021efficient,
 author = {Krell, Mario Michael and Kosec, Matej and Perez, Sergio P and Fitzgibbon, Andrew},
 journal = {arXiv preprint arXiv:2107.02027},
 title = {Efficient sequence packing without cross-contamination: Accelerating large language models without impacting performance},
 year = {2021}
}

@article{krishna2021hurdles,
 author = {Krishna, Kalpesh and Roy, Aurko and Iyyer, Mohit},
 journal = {arXiv preprint arXiv:2103.06332},
 title = {Hurdles to progress in long-form question answering},
 year = {2021}
}

@article{kulkarni2020aquamuse,
 author = {Kulkarni, Sayali and Chammas, Sheide and Zhu, Wan and Sha, Fei and Ie, Eugene},
 journal = {arXiv preprint arXiv:2010.12694},
 title = {Aquamuse: Automatically generating datasets for query-based multi-document summarization},
 year = {2020}
}

@article{kumar2024longlamp,
 author = {Kumar, Ishita and Viswanathan, Snigdha and Yerra, Sushrita and Salemi, Alireza and Rossi, Ryan A and Dernoncourt, Franck and Deilamsalehy, Hanieh and Chen, Xiang and Zhang, Ruiyi and Agarwal, Shubham and others},
 journal = {arXiv preprint arXiv:2407.11016},
 title = {Longlamp: A benchmark for personalized long-form text generation},
 year = {2024}
}

@article{kundu2024enhancing,
 author = {Kundu, Achintya and Lee, Rhui Dih and Wynter, Laura and Ganti, Raghu Kiran and Mishra, Mayank},
 journal = {arXiv preprint arXiv:2407.09105},
 title = {Enhancing training efficiency using packing with flash attention},
 year = {2024}
}

@article{kuratov2024babilong,
 author = {Kuratov, Yuri and Bulatov, Aydar and Anokhin, Petr and Rodkin, Ivan and Sorokin, Dmitry and Sorokin, Artyom and Burtsev, Mikhail},
 journal = {arXiv preprint arXiv:2406.10149},
 title = {BABILong: Testing the Limits of LLMs with Long Context Reasoning-in-a-Haystack},
 year = {2024}
}

@article{kwan2023m4le,
 author = {Kwan, Wai-Chung and Zeng, Xingshan and Wang, Yufei and Sun, Yusen and Li, Liangyou and Shang, Lifeng and Liu, Qun and Wong, Kam-Fai},
 journal = {arXiv preprint arXiv:2310.19240},
 title = {M4le: A multi-ability multi-range multi-task multi-domain long-context evaluation benchmark for large language models},
 year = {2023}
}

@article{kwiatkowski2019natural,
 author = {Kwiatkowski, Tom and Palomaki, Jennimaria and Redfield, Olivia and Collins, Michael and Parikh, Ankur and Alberti, Chris and Epstein, Danielle and Polosukhin, Illia and Devlin, Jacob and Lee, Kenton and others},
 journal = {Transactions of the Association for Computational Linguistics},
 pages = {452--466},
 title = {Natural Questions: A Benchmark for Question Answering Research},
 volume = {7},
 year = {2019}
}

@inproceedings{kwon2023efficient,
  title={Efficient memory management for large language model serving with pagedattention},
  author={Kwon, Woosuk and Li, Zhuohan and Zhuang, Siyuan and Sheng, Ying and Zheng, Lianmin and Yu, Cody Hao and Gonzalez, Joseph and Zhang, Hao and Stoica, Ion},
  booktitle={Proceedings of the 29th Symposium on Operating Systems Principles},
  pages={611--626},
  year={2023}
}

@article{laban2024SummHay,
 author = {Laban, Philippe and Fabbri, Alexander R and Xiong, Caiming and Wu, Chien-Sheng},
 journal = {arXiv preprint arXiv:https://arxiv.org/pdf/2407.01370},
 title = {Summary of a Haystack: A Challenge to Long-Context LLMs and RAG Systems},
 year = {2024}
}

@inproceedings{lee-etal-2023-prompted,
  author       = {Gibbeum Lee and
                  Volker Hartmann and
                  Jongho Park and
                  Dimitris Papailiopoulos and
                  Kangwook Lee},
  editor       = {Anna Rogers and
                  Jordan L. Boyd{-}Graber and
                  Naoaki Okazaki},
  title        = {Prompted LLMs as Chatbot Modules for Long Open-domain Conversation},
  booktitle    = {Findings of the Association for Computational Linguistics: {ACL} 2023,
                  Toronto, Canada, July 9-14, 2023},
  pages        = {4536--4554},
  publisher    = {Association for Computational Linguistics},
  year         = {2023},
  url          = {https://doi.org/10.18653/v1/2023.findings-acl.277},
  doi          = {10.18653/V1/2023.FINDINGS-ACL.277},
  timestamp    = {Thu, 10 Aug 2023 12:36:03 +0200},
  biburl       = {https://dblp.org/rec/conf/acl/LeeHPP023.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{lee2023qasa,
 author = {Lee, Yoonjoo and Lee, Kyungjae and Park, Sunghyun and Hwang, Dasol and Kim, Jaehyeon and Lee, Hong-in and Lee, Moontae},
 booktitle = {International Conference on Machine Learning},
 organization = {PMLR},
 pages = {19036--19052},
 title = {Qasa: advanced question answering on scientific articles},
 year = {2023}
}

@article{lee2024can,
 author = {Lee, Jinhyuk and Chen, Anthony and Dai, Zhuyun and Dua, Dheeru and Sachan, Devendra Singh and Boratko, Michael and Luan, Yi and Arnold, S{\'e}bastien MR and Perot, Vincent and Dalmia, Siddharth and others},
 journal = {arXiv preprint arXiv:2406.13121},
 title = {Can Long-Context Language Models Subsume Retrieval, RAG, SQL, and More?},
 year = {2024}
}

@article{lee2024ethic,
 author = {Lee, Taewhoo and Yoon, Chanwoong and Jang, Kyochul and Lee, Donghyeon and Song, Minju and Kim, Hyunjae and Kang, Jaewoo},
 journal = {arXiv preprint arXiv:2410.16848},
 title = {ETHIC: Evaluating Large Language Models on Long-Context Tasks with High Information Coverage},
 year = {2024}
}

@inproceedings{lei2024s3eval,
 author = {Lei, Fangyu and Liu, Qian and Huang, Yiming and He, Shizhu and Zhao, Jun and Liu, Kang},
 booktitle = {Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)},
 pages = {1259--1286},
 title = {S3Eval: A Synthetic, Scalable, Systematic Evaluation Suite for Large Language Model},
 year = {2024}
}

@book{leskovec2020mining,
 author = {Leskovec, Jure and Rajaraman, Anand and Ullman, Jeffrey David},
 publisher = {Cambridge university press},
 title = {Mining of massive data sets},
 year = {2020}
}

@inproceedings{lester-etal-2021-power,
 abstract = {In this work, we explore {``}prompt tuning,{''} a simple yet effective mechanism for learning {``}soft prompts{''} to condition frozen language models to perform specific downstream tasks. Unlike the discrete text prompts used by GPT-3, soft prompts are learned through backpropagation and can be tuned to incorporate signals from any number of labeled examples. Our end-to-end learned approach outperforms GPT-3{'}s few-shot learning by a large margin. More remarkably, through ablations on model size using T5, we show that prompt tuning becomes more competitive with scale: as models exceed billions of parameters, our method {``}closes the gap{''} and matches the strong performance of model tuning (where all model weights are tuned). This finding is especially relevant because large models are costly to share and serve and the ability to reuse one frozen model for multiple downstream tasks can ease this burden. Our method can be seen as a simplification of the recently proposed {``}prefix tuning{''} of Li and Liang (2021) and we provide a comparison to this and other similar approaches. Finally, we show that conditioning a frozen model with soft prompts confers benefits in robustness to domain transfer and enables efficient {``}prompt ensembling.{''} We release code and model checkpoints to reproduce our experiments.},
 address = {Online and Punta Cana, Dominican Republic},
 author = {Lester, Brian  and
Al-Rfou, Rami  and
Constant, Noah},
 booktitle = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
 doi = {10.18653/v1/2021.emnlp-main.243},
 editor = {Moens, Marie-Francine  and
Huang, Xuanjing  and
Specia, Lucia  and
Yih, Scott Wen-tau},
 month = {November},
 pages = {3045--3059},
 publisher = {Association for Computational Linguistics},
 title = {The Power of Scale for Parameter-Efficient Prompt Tuning},
 url = {https://aclanthology.org/2021.emnlp-main.243},
 year = {2021}
}

@inproceedings{leviathan2023fast,
 author = {Leviathan, Yaniv and Kalman, Matan and Matias, Yossi},
 booktitle = {International Conference on Machine Learning},
 organization = {PMLR},
 pages = {19274--19286},
 title = {Fast inference from transformers via speculative decoding},
 year = {2023}
}

@misc{leviathan2024selectiveattentionimprovestransformer,
 archiveprefix = {arXiv},
 author = {Yaniv Leviathan and Matan Kalman and Yossi Matias},
 eprint = {2410.02703},
 primaryclass = {cs.CL},
 title = {Selective Attention Improves Transformer},
 url = {https://arxiv.org/abs/2410.02703},
 year = {2024}
}

@inproceedings{levine2022the,
 author = {Yoav Levine and Noam Wies and Daniel Jannai and Dan Navon and Yedid Hoshen and Amnon Shashua},
 booktitle = {International Conference on Learning Representations},
 title = {The Inductive Bias of In-Context Learning: Rethinking Pretraining Example Design},
 year = {2022}
}

@article{lewis2020retrieval,
 author = {Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K{\"u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt{\"a}schel, Tim and others},
 journal = {Advances in Neural Information Processing Systems},
 pages = {9459--9474},
 title = {Retrieval-augmented generation for knowledge-intensive nlp tasks},
 volume = {33},
 year = {2020}
}

@article{lewis2020retrievalaugmented,
 author = {Patrick Lewis and Ethan Perez and Aleksandra Piktus and Fabio Petroni and Vladimir Karpukhin and Naman Goyal and Heinrich Küttler and Mike Lewis and Wen-tau Yih and Tim Rocktäschel and Sebastian Riedel and Douwe Kiela},
 journal = {arXiv preprint arXiv: 2005.11401},
 title = {Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks},
 year = {2020}
}

@inproceedings{li-2023-icml-blip2,
 author = {Junnan Li and
Dongxu Li and
Silvio Savarese and
Steven C. H. Hoi},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/icml/0008LSH23.bib},
 booktitle = {International Conference on Machine Learning, {ICML} 2023, 23-29 July
2023, Honolulu, Hawaii, {USA}},
 editor = {Andreas Krause and
Emma Brunskill and
Kyunghyun Cho and
Barbara Engelhardt and
Sivan Sabato and
Jonathan Scarlett},
 pages = {19730--19742},
 publisher = {{PMLR}},
 series = {Proceedings of Machine Learning Research},
 timestamp = {Mon, 04 Dec 2023 11:29:49 +0100},
 title = {{BLIP-2:} Bootstrapping Language-Image Pre-training with Frozen Image
Encoders and Large Language Models},
 url = {https://proceedings.mlr.press/v202/li23q.html},
 volume = {202},
 year = {2023}
}

@article{li-2024-arxiv-llavaonevision,
 author = {Bo Li and
Yuanhan Zhang and
Dong Guo and
Renrui Zhang and
Feng Li and
Hao Zhang and
Kaichen Zhang and
Yanwei Li and
Ziwei Liu and
Chunyuan Li},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/journals/corr/abs-2408-03326.bib},
 doi = {10.48550/ARXIV.2408.03326},
 eprint = {2408.03326},
 eprinttype = {arXiv},
 journal = {CoRR},
 timestamp = {Thu, 12 Sep 2024 21:06:52 +0200},
 title = {LLaVA-OneVision: Easy Visual Task Transfer},
 url = {https://doi.org/10.48550/arXiv.2408.03326},
 volume = {abs/2408.03326},
 year = {2024}
}

@article{li-2025-arxiv-TPO,
 author = {Rui Li and
Xiaohan Wang and
Yuhui Zhang and
Zeyu Wang and
Serena Yeung-Levy},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/journals/corr/abs-2501-13919.bib},
 doi = {10.48550/ARXIV.2501.13919},
 eprint = {2501.13919},
 eprinttype = {arXiv},
 journal = {CoRR},
 title = {Temporal Preference Optimization for Long-Form Video Understanding},
 url = {https://doi.org/10.48550/arXiv.2501.13919},
 volume = {abs/2501.13919},
 year = {2025}
}

@article{li-2025-arxiv-VideoChatFlash,
 author = {Xinhao Li and
Yi Wang and
Jiashuo Yu and
Xiangyu Zeng and
Yuhan Zhu and
Haian Huang and
Jianfei Gao and
Kunchang Li and
Yinan He and
Chenting Wang and
Yu Qiao and
Yali Wang and
Limin Wang},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/journals/corr/abs-2501-00574.bib},
 doi = {10.48550/arXiv.2501.00574},
 eprint = {2501.00574},
 eprinttype = {arXiv},
 journal = {CoRR},
 timestamp = {Fri, 12 Jan 2025 13:00:00 +0000},
 title = {VideoChat-Flash: Hierarchical Compression for Long-Context Video Modeling},
 url = {https://arxiv.org/pdf/2501.00574},
 volume = {abs/2501.00574},
 year = {2025}
}

@article{DBLP:journals/corr/abs-2501-00574,
  author       = {Xinhao Li and
                  Yi Wang and
                  Jiashuo Yu and
                  Xiangyu Zeng and
                  Yuhan Zhu and
                  Haian Huang and
                  Jianfei Gao and
                  Kunchang Li and
                  Yinan He and
                  Chenting Wang and
                  Yu Qiao and
                  Yali Wang and
                  Limin Wang},
  title        = {VideoChat-Flash: Hierarchical Compression for Long-Context Video Modeling},
  journal      = {CoRR},
  volume       = {abs/2501.00574},
  year         = {2025},
  url          = {https://doi.org/10.48550/arXiv.2501.00574},
  doi          = {10.48550/ARXIV.2501.00574},
  eprinttype    = {arXiv},
  eprint       = {2501.00574},
  timestamp    = {Sun, 09 Feb 2025 10:52:29 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2501-00574.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{li-etal-2023-compressing,
 abstract = {Large language models (LLMs) achieved remarkable performance across various tasks. However, they face challenges in managing long documents and extended conversations, due to significantly increased computational requirements, both in memory and inference time, and potential context truncation when the input exceeds the LLM{'}s fixed context length. This paper proposes a method called \textit{Selective Context} that enhances the inference efficiency of LLMs by identifying and pruning redundancy in the input context to make the input more compact. We test our approach using common data sources requiring long context processing: arXiv papers, news articles, and long conversations, on tasks of summarisation, question answering, and response generation. Experimental results show that Selective Context significantly reduces memory cost and decreases generation latency while maintaining comparable performance compared to that achieved when full context is used. Specifically, we achieve a 50{\%} reduction in context cost, resulting in a 36{\%} reduction in inference memory usage and a 32{\%} reduction in inference time, while observing only a minor drop of .023 in BERTscore and .038 in faithfulness on four downstream applications, indicating that our method strikes a good balance between efficiency and performance.},
 address = {Singapore},
 author = {Li, Yucheng  and
Dong, Bo  and
Guerin, Frank  and
Lin, Chenghua},
 booktitle = {Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
 doi = {10.18653/v1/2023.emnlp-main.391},
 editor = {Bouamor, Houda  and
Pino, Juan  and
Bali, Kalika},
 month = {December},
 pages = {6342--6353},
 publisher = {Association for Computational Linguistics},
 title = {Compressing Context to Enhance Inference Efficiency of Large Language Models},
 url = {https://aclanthology.org/2023.emnlp-main.391},
 year = {2023}
}

@article{li2015diversity,
 author = {Li, Jiwei and Galley, Michel and Brockett, Chris and Gao, Jianfeng and Dolan, Bill},
 journal = {arXiv preprint arXiv:1510.03055},
 title = {A diversity-promoting objective function for neural conversation models},
 year = {2015}
}

@misc{li2023compressingcontextenhanceinference,
 archiveprefix = {arXiv},
 author = {Yucheng Li and Bo Dong and Chenghua Lin and Frank Guerin},
 eprint = {2310.06201},
 primaryclass = {cs.CL},
 title = {Compressing Context to Enhance Inference Efficiency of Large Language Models},
 url = {https://arxiv.org/abs/2310.06201},
 year = {2023}
}

@article{li2023e4srec,
 author = {Li, Xinhang and Chen, Chong and Zhao, Xiangyu and Zhang, Yong and Xing, Chunxiao},
 journal = {arXiv preprint arXiv:2312.02443},
 title = {E4srec: An elegant effective efficient extensible solution of large language models for sequential recommendation},
 year = {2023}
}

@article{li2023fptq,
 author = {Li, Qingyuan and Zhang, Yifan and Li, Liang and Yao, Peng and Zhang, Bo and Chu, Xiangxiang and Sun, Yerui and Du, Li and Xie, Yuchen},
 journal = {arXiv preprint arXiv:2308.15987},
 title = {Fptq: Fine-grained post-training quantization for large language models},
 year = {2023}
}

@article{Li2023FromQT,
 author = {Ming Li and Yong Zhang and Zhitao Li and Jiuhai Chen and Lichang Chen and Ning Cheng and Jianzong Wang and Tianyi Zhou and Jing Xiao},
 journal = {ArXiv},
 title = {From Quantity to Quality: Boosting LLM Performance with Self-Guided Data Selection for Instruction Tuning},
 volume = {abs/2308.12032},
 year = {2023}
}

@article{li2023functional,
 author = {Li, Shanda and You, Chong and Guruganesh, Guru and Ainslie, Joshua and Ontanon, Santiago and Zaheer, Manzil and Sanghai, Sumit and Yang, Yiming and Kumar, Sanjiv and Bhojanapalli, Srinadh},
 journal = {arXiv preprint arXiv:2310.04418},
 title = {Functional interpolation for relative positions improves long context transformers},
 year = {2023}
}

@inproceedings{li2023long,
 author = {Li, Dacheng and Shao, Rulin and Xie, Anze and Sheng, Ying and Zheng, Lianmin and Gonzalez, Joseph and Stoica, Ion and Ma, Xuezhe and Zhang, Hao},
 booktitle = {NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following},
 title = {How Long Can Context Length of Open-Source LLMs truly Promise?},
 year = {2023}
}

@article{li2023loogle,
 author = {Li, Jiaqi and Wang, Mengmeng and Zheng, Zilong and Zhang, Muhan},
 journal = {arXiv preprint arXiv:2311.04939},
 title = {LooGLE: Can Long-Context Language Models Understand Long Contexts?},
 year = {2023}
}

@article{Li20242DDPOSD,
 author = {Shilong Li and Yancheng He and Hui Huang and Xingyuan Bu and Jiaheng Liu and Hangyu Guo and Weixun Wang and Jihao Gu and Wenbo Su and Bo Zheng},
 journal = {ArXiv},
 title = {2D-DPO: Scaling Direct Preference Optimization with 2-Dimensional Supervision},
 volume = {abs/2410.19720},
 year = {2024}
}

@misc{li2024500xcompressorgeneralizedpromptcompression,
 archiveprefix = {arXiv},
 author = {Zongqian Li and Yixuan Su and Nigel Collier},
 eprint = {2408.03094},
 primaryclass = {cs.CL},
 title = {500xCompressor: Generalized Prompt Compression for Large Language Models},
 url = {https://arxiv.org/abs/2408.03094},
 year = {2024}
}

@article{li2024alr,
 author = {Li, Huayang and Verga, Pat and Sen, Priyanka and Yang, Bowen and Viswanathan, Vijay and Lewis, Patrick and Watanabe, Taro and Su, Yixuan},
 journal = {arXiv preprint arXiv:2410.03227},
 title = {ALR $\^{} 2$: A Retrieve-then-Reason Framework for Long-context Question Answering},
 year = {2024}
}

@article{li2024demystifying,
 author = {Li, Haoyang and Fu, Fangcheng and Lin, Sheng and Ge, Hao and Wang, Xuanyu and Niu, Jiawen and Jiang, Jie and Cui, Bin},
 journal = {arXiv preprint arXiv:2412.07894},
 title = {Demystifying Workload Imbalances in Large Transformer Model Training over Variable-length Sequences},
 year = {2024}
}

@inproceedings{li2024eagle,
  title={EAGLE: speculative sampling requires rethinking feature uncertainty},
  author={Li, Yuhui and Wei, Fangyun and Zhang, Chao and Zhang, Hongyang},
  booktitle={Proceedings of the 41st International Conference on Machine Learning},
  pages={28935--28948},
  year={2024}
}

@article{li2024graphreader,
 author = {Li, Shilong and He, Yancheng and Guo, Hangyu and Bu, Xingyuan and Bai, Ge and Liu, Jie and Liu, Jiaheng and Qu, Xingwei and Li, Yangguang and Ouyang, Wanli and others},
 journal = {arXiv preprint arXiv:2406.14550},
 title = {GraphReader: Building Graph-based Agent to Enhance Long-Context Abilities of Large Language Models},
 year = {2024}
}

@article{li2024long,
 author = {Li, Tianle and Zhang, Ge and Do, Quy Duc and Yue, Xiang and Chen, Wenhu},
 journal = {arXiv preprint arXiv:2404.02060},
 title = {Long-context llms struggle with long in-context learning},
 year = {2024}
}

@article{li2024making,
 author = {Li, Yanyang and Liang, Shuo and Lyu, Michael R and Wang, Liwei},
 journal = {arXiv preprint arXiv:2408.03246},
 title = {Making long-context language models better multi-hop reasoners},
 year = {2024}
}

@inproceedings{DBLP:conf/acl/LiLL024,
  author       = {Yanyang Li and
                  Shuo Liang and
                  Michael R. Lyu and
                  Liwei Wang},
  editor       = {Lun{-}Wei Ku and
                  Andre Martins and
                  Vivek Srikumar},
  title        = {Making Long-Context Language Models Better Multi-Hop Reasoners},
  booktitle    = {Proceedings of the 62nd Annual Meeting of the Association for Computational
                  Linguistics (Volume 1: Long Papers), {ACL} 2024, Bangkok, Thailand,
                  August 11-16, 2024},
  pages        = {2462--2475},
  publisher    = {Association for Computational Linguistics},
  year         = {2024},
  url          = {https://doi.org/10.18653/v1/2024.acl-long.135},
  doi          = {10.18653/V1/2024.ACL-LONG.135},
  timestamp    = {Sun, 19 Jan 2025 13:20:29 +0100},
  biburl       = {https://dblp.org/rec/conf/acl/LiLL024.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{li2024needlebench,
 author = {Li, Mo and Zhang, Songyang and Liu, Yunxin and Chen, Kai},
 journal = {arXiv preprint arXiv:2407.11963},
 title = {NeedleBench: Can LLMs Do Retrieval and Reasoning in 1 Million Context Window?},
 year = {2024}
}

@misc{li2024promptcompressionlargelanguage,
 archiveprefix = {arXiv},
 author = {Zongqian Li and Yinhong Liu and Yixuan Su and Nigel Collier},
 eprint = {2410.12388},
 primaryclass = {cs.CL},
 title = {Prompt Compression for Large Language Models: A Survey},
 url = {https://arxiv.org/abs/2410.12388},
 year = {2024}
}

@article{li2024retrieval,
  title={Retrieval Augmented Generation or Long-Context LLMs? A Comprehensive Study and Hybrid Approach},
  author={Li, Zhuowan and Li, Cheng and Zhang, Mingyang and Mei, Qiaozhu and Bendersky, Michael},
  journal={arXiv preprint arXiv:2407.16833},
  year={2024}
}

@article{li2024scbench,
 author = {Li, Yucheng and Jiang, Huiqiang and Wu, Qianhui and Luo, Xufang and Ahn, Surin and Zhang, Chengruidong and Abdi, Amir H and Li, Dongsheng and Gao, Jianfeng and Yang, Yuqing and others},
 journal = {arXiv preprint arXiv:2412.10319},
 title = {Scbench: A kv cache-centric analysis of long-context methods},
 year = {2024}
}

@inproceedings{li2024snapkvllmknowslooking,
  author       = {Yuhong Li and
                  Yingbing Huang and
                  Bowen Yang and
                  Bharat Venkitesh and
                  Acyr Locatelli and
                  Hanchen Ye and
                  Tianle Cai and
                  Patrick Lewis and
                  Deming Chen},
  title        = {SnapKV: {LLM} Knows What You are Looking for Before Generation},
  booktitle    = {NeurIPS},
  year         = {2024}
}

@article{li2025llms,
 author = {Li, Dacheng and Cao, Shiyi and Griggs, Tyler and Liu, Shu and Mo, Xiangxi and Patil, Shishir G and Zaharia, Matei and Gonzalez, Joseph E and Stoica, Ion},
 journal = {arXiv preprint arXiv:2502.07374},
 title = {LLMs Can Easily Learn to Reason from Demonstrations Structure, not content, is what matters!},
 year = {2025}
}

@article{li2025minimax,
 author = {Li, Aonian and Gong, Bangwei and Yang, Bo and Shan, Boji and Liu, Chang and Zhu, Cheng and Zhang, Chunhao and Guo, Congchao and Chen, Da and Li, Dong and others},
 journal = {arXiv preprint arXiv:2501.08313},
 title = {Minimax-01: Scaling foundation models with lightning attention},
 year = {2025}
}

@article{li2025small,
 author = {Li, Yuetai and Yue, Xiang and Xu, Zhangchen and Jiang, Fengqing and Niu, Luyao and Lin, Bill Yuchen and Ramasubramanian, Bhaskar and Poovendran, Radha},
 journal = {arXiv preprint arXiv:2502.12143},
 title = {Small Models Struggle to Learn from Strong Reasoners},
 year = {2025}
}

@article{liang-2024-arxiv-keyvideollm,
 author = {Hao Liang and
Jiapeng Li and
Tianyi Bai and
Xijie Huang and
Linzhuang Sun and
Zhengren Wang and
Conghui He and
Bin Cui and
Chong Chen and
Wentao Zhang},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/journals/corr/abs-2407-03104.bib},
 doi = {10.48550/ARXIV.2407.03104},
 eprint = {2407.03104},
 eprinttype = {arXiv},
 journal = {CoRR},
 timestamp = {Wed, 07 Aug 2024 21:29:39 +0200},
 title = {KeyVideoLLM: Towards Large-scale Video Keyframe Selection},
 url = {https://doi.org/10.48550/arXiv.2407.03104},
 volume = {abs/2407.03104},
 year = {2024}
}

@inproceedings{liang2023open,
 author = {Liang, Xiaobo and Tang, Zecheng and Li, Juntao and Zhang, Min},
 booktitle = {Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
 pages = {223--241},
 title = {Open-ended long text generation via masked language modeling},
 year = {2023}
}

@article{lieber2024jamba,
 author = {Lieber, Opher and Lenz, Barak and Bata, Hofit and Cohen, Gal and Osin, Jhonathan and Dalmedigos, Itay and Safahi, Erez and Meirom, Shaked and Belinkov, Yonatan and Shalev-Shwartz, Shai and others},
 journal = {arXiv preprint arXiv:2403.19887},
 title = {Jamba: A hybrid transformer-mamba language model},
 year = {2024}
}

@misc{lieber2024jambahybridtransformermambalanguage,
 archiveprefix = {arXiv},
 author = {Opher Lieber and Barak Lenz and Hofit Bata and Gal Cohen and Jhonathan Osin and Itay Dalmedigos and Erez Safahi and Shaked Meirom and Yonatan Belinkov and Shai Shalev-Shwartz and Omri Abend and Raz Alon and Tomer Asida and Amir Bergman and Roman Glozman and Michael Gokhman and Avashalom Manevich and Nir Ratner and Noam Rozen and Erez Shwartz and Mor Zusman and Yoav Shoham},
 eprint = {2403.19887},
 primaryclass = {cs.CL},
 title = {Jamba: A Hybrid Transformer-Mamba Language Model},
 url = {https://arxiv.org/abs/2403.19887},
 year = {2024}
}

@inproceedings{lin2004rouge,
 author = {Lin, Chin-Yew},
 booktitle = {Text summarization branches out},
 pages = {74--81},
 title = {Rouge: A package for automatic evaluation of summaries},
 year = {2004}
}

@article{lin2023can,
 author = {Lin, Jianghao and Dai, Xinyi and Xi, Yunjia and Liu, Weiwen and Chen, Bo and Zhang, Hao and Liu, Yong and Wu, Chuhan and Li, Xiangyang and Zhu, Chenxu and others},
 journal = {arXiv preprint arXiv:2306.05817},
 title = {How can recommender systems benefit from large language models: A survey},
 year = {2023}
}

@article{DBLP:journals/corr/abs-2306-05817,
  author       = {Jianghao Lin and
                  Xinyi Dai and
                  Yunjia Xi and
                  Weiwen Liu and
                  Bo Chen and
                  Xiangyang Li and
                  Chenxu Zhu and
                  Huifeng Guo and
                  Yong Yu and
                  Ruiming Tang and
                  Weinan Zhang},
  title        = {How Can Recommender Systems Benefit from Large Language Models: {A}
                  Survey},
  journal      = {CoRR},
  volume       = {abs/2306.05817},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2306.05817},
  doi          = {10.48550/ARXIV.2306.05817},
  eprinttype    = {arXiv},
  eprint       = {2306.05817},
  timestamp    = {Sun, 06 Oct 2024 21:23:53 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2306-05817.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{lin2024awq,
 author = {Lin, Ji and Tang, Jiaming and Tang, Haotian and Yang, Shang and Chen, Wei-Ming and Wang, Wei-Chen and Xiao, Guangxuan and Dang, Xingyu and Gan, Chuang and Han, Song},
 journal = {Proceedings of Machine Learning and Systems},
 pages = {87--100},
 title = {AWQ: Activation-aware Weight Quantization for On-Device LLM Compression and Acceleration},
 volume = {6},
 year = {2024}
}

@inproceedings{lin2024clickprompt,
 author = {Lin, Jianghao and Chen, Bo and Wang, Hangyu and Xi, Yunjia and Qu, Yanru and Dai, Xinyi and Zhang, Kangning and Tang, Ruiming and Yu, Yong and Zhang, Weinan},
 booktitle = {Proceedings of the ACM on Web Conference 2024},
 pages = {3319--3330},
 title = {ClickPrompt: CTR Models are Strong Prompt Generators for Adapting Language Models to CTR Prediction},
 year = {2024}
}


@inproceedings{lin2024rella,
  author       = {Jianghao Lin and
                  Rong Shan and
                  Chenxu Zhu and
                  Kounianhua Du and
                  Bo Chen and
                  Shigang Quan and
                  Ruiming Tang and
                  Yong Yu and
                  Weinan Zhang},
  editor       = {Tat{-}Seng Chua and
                  Chong{-}Wah Ngo and
                  Ravi Kumar and
                  Hady W. Lauw and
                  Roy Ka{-}Wei Lee},
  title        = {ReLLa: Retrieval-enhanced Large Language Models for Lifelong Sequential
                  Behavior Comprehension in Recommendation},
  booktitle    = {Proceedings of the {ACM} on Web Conference 2024, {WWW} 2024, Singapore,
                  May 13-17, 2024},
  pages        = {3497--3508},
  publisher    = {{ACM}},
  year         = {2024},
  url          = {https://doi.org/10.1145/3589334.3645467},
  doi          = {10.1145/3589334.3645467},
  timestamp    = {Sun, 19 Jan 2025 13:10:30 +0100},
  biburl       = {https://dblp.org/rec/conf/www/LinSZDCQTY024.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{lingle2024transformervqlineartimetransformersvector,
  author       = {Lucas D. Lingle},
  title        = {Transformer-VQ: Linear-Time Transformers via Vector Quantization},
  booktitle    = {{ICLR}},
  publisher    = {OpenReview.net},
  year         = {2024}
}
@misc{liskavets2024promptcompressioncontextawaresentence,
 archiveprefix = {arXiv},
 author = {Barys Liskavets and Maxim Ushakov and Shuvendu Roy and Mark Klibanov and Ali Etemad and Shane Luke},
 eprint = {2409.01227},
 primaryclass = {cs.CL},
 title = {Prompt Compression with Context-Aware Sentence Encoding for Fast and Improved LLM Inference},
 url = {https://arxiv.org/abs/2409.01227},
 year = {2024}
}

@article{liu-2024-arxiv-mustdrop,
 author = {Ting Liu and
Liangtao Shi and
Richang Hong and
Yue Hu and
Quanjun Yin and
Linfeng Zhang},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/journals/corr/abs-2411-10803.bib},
 doi = {10.48550/ARXIV.2411.10803},
 eprint = {2411.10803},
 eprinttype = {arXiv},
 journal = {CoRR},
 timestamp = {Wed, 01 Jan 2025 13:20:10 +0100},
 title = {Multi-Stage Vision Token Dropping: Towards Efficient Multimodal Large
Language Model},
 url = {https://doi.org/10.48550/arXiv.2411.10803},
 volume = {abs/2411.10803},
 year = {2024}
}

@inproceedings{liu-etal-2023-tcra,
 abstract = {Since ChatGPT released its API for public use, the number of applications built on top of commercial large language models (LLMs) increase exponentially. One popular usage of such models is leveraging its in-context learning ability and generating responses given user queries leveraging knowledge obtained by retrieval augmentation. One problem of deploying commercial retrieval-augmented LLMs is the cost due to the additionally retrieved context that largely increases the input token size of the LLMs. To mitigate this, we propose a token compression scheme that includes two methods: summarization compression and semantic compression. The first method applies a T5-based model that is fine-tuned by datasets generated using self-instruct containing samples with varying lengths and reduce token size by doing summarization. The second method further compresses the token size by removing words with lower impact on the semantic. In order to adequately evaluate the effectiveness of the proposed methods, we propose and utilize a dataset called Food-Recommendation DB (FRDB) focusing on food recommendation for women around pregnancy period or infants. Our summarization compression can reduce 65{\%} of the retrieval token size with further 0.3{\%} improvement on the accuracy; semantic compression provides a more flexible way to trade-off the token size with performance, for which we can reduce the token size by 20{\%} with only 1.6{\%} of accuracy drop.},
 address = {Singapore},
 author = {Liu, Junyi  and
Li, Liangzhi  and
Xiang, Tong  and
Wang, Bowen  and
Qian, Yiming},
 booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2023},
 doi = {10.18653/v1/2023.findings-emnlp.655},
 editor = {Bouamor, Houda  and
Pino, Juan  and
Bali, Kalika},
 month = {December},
 pages = {9796--9810},
 publisher = {Association for Computational Linguistics},
 title = {{TCRA}-{LLM}: Token Compression Retrieval Augmented Large Language Model for Inference Cost Reduction},
 url = {https://aclanthology.org/2023.findings-emnlp.655},
 year = {2023}
}

@inproceedings{liu-etal-2024-e2,
 address = {Bangkok, Thailand},
 author = {Liu, Jiaheng  and
ZhiqiBai, ZhiqiBai  and
Zhang, Yuanxing  and
Zhang, Chenchen  and
YuangZh, YuangZh  and
Zhang, Ge  and
JiakaiWang, JiakaiWang  and
Que, Haoran  and
Chen, Yukang  and
Su, Wenbo  and
Ge, Tiezheng  and
Fu, Jie  and
Chen, Wenhu  and
Zheng, Bo},
 booktitle = {Findings of the Association for Computational Linguistics: ACL 2024},
 doi = {10.18653/v1/2024.findings-acl.252},
 editor = {Ku, Lun-Wei  and
Martins, Andre  and
Srikumar, Vivek},
 month = {August},
 pages = {4243--4253},
 publisher = {Association for Computational Linguistics},
 title = {E2-{LLM}: Efficient and Extreme Length Extension of Large Language Models},
 url = {https://aclanthology.org/2024.findings-acl.252/},
 year = {2024}
}

@inproceedings{liu-etal-2024-longwanjuan,
 address = {Miami, Florida, USA},
 author = {Liu, Xiaoran  and
Lv, Kai  and
Guo, Qipeng  and
Yan, Hang  and
He, Conghui  and
Qiu, Xipeng  and
Lin, Dahua},
 booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2024},
 doi = {10.18653/v1/2024.findings-emnlp.327},
 editor = {Al-Onaizan, Yaser  and
Bansal, Mohit  and
Chen, Yun-Nung},
 month = {November},
 pages = {5709--5725},
 publisher = {Association for Computational Linguistics},
 title = {{L}ong{W}anjuan: Towards Systematic Measurement for Long Text Quality},
 url = {https://aclanthology.org/2024.findings-emnlp.327},
 year = {2024}
}

@article{liu2024lost,
  title={Lost in the middle: How language models use long contexts},
  author={Liu, Nelson F and Lin, Kevin and Hewitt, John and Paranjape, Ashwin and Bevilacqua, Michele and Petroni, Fabio and Liang, Percy},
  journal={Transactions of the Association for Computational Linguistics},
  volume={12},
  pages={157--173},
  year={2024},
  publisher={MIT Press One Broadway, 12th Floor, Cambridge, Massachusetts 02142, USA~…}
}

@article{liu2023repobench,
 author = {Liu, Tianyang and Xu, Canwen and McAuley, Julian},
 journal = {arXiv preprint arXiv:2306.03091},
 title = {Repobench: Benchmarking repository-level code auto-completion systems},
 year = {2023}
}

@article{liu2023ring,
 author = {Liu, Hao and Zaharia, Matei and Abbeel, Pieter},
 journal = {arXiv preprint arXiv:2310.01889},
 title = {Ring attention with blockwise transformers for near-infinite context},
 year = {2023}
}

@misc{liu2023scissorhandsexploitingpersistenceimportance,
 archiveprefix = {arXiv},
 author = {Zichang Liu and Aditya Desai and Fangshuo Liao and Weitao Wang and Victor Xie and Zhaozhuo Xu and Anastasios Kyrillidis and Anshumali Shrivastava},
 eprint = {2305.17118},
 primaryclass = {cs.LG},
 title = {Scissorhands: Exploiting the Persistence of Importance Hypothesis for LLM KV Cache Compression at Test Time},
 url = {https://arxiv.org/abs/2305.17118},
 year = {2023}
}

@inproceedings{NEURIPS2023_a452a7c6,
 author = {Liu, Zichang and Desai, Aditya and Liao, Fangshuo and Wang, Weitao and Xie, Victor and Xu, Zhaozhuo and Kyrillidis, Anastasios and Shrivastava, Anshumali},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {52342--52364},
 publisher = {Curran Associates, Inc.},
 title = {Scissorhands: Exploiting the Persistence of Importance Hypothesis for LLM KV Cache Compression at Test Time},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/a452a7c6c463e4ae8fbdc614c6e983e6-Paper-Conference.pdf},
 volume = {36},
 year = {2023}
}


@article{liu2024bridging,
 author = {Liu, Yanming and Peng, Xinyue and Cao, Jiannan and Bo, Shi and Shen, Yanxin and Zhang, Xuhong and Cheng, Sheng and Wang, Xun and Yin, Jianwei and Du, Tianyu},
 journal = {arXiv preprint arXiv:2410.01671},
 title = {Bridging context gaps: Leveraging coreference resolution for long contextual understanding},
 year = {2024}
}

@inproceedings{liu2024cachegen,
 author = {Liu, Yuhan and Li, Hanchen and Cheng, Yihua and Ray, Siddhant and Huang, Yuyang and Zhang, Qizheng and Du, Kuntai and Yao, Jiayi and Lu, Shan and Ananthanarayanan, Ganesh and others},
 booktitle = {Proceedings of the ACM SIGCOMM 2024 Conference},
 pages = {38--56},
 title = {Cachegen: Kv cache compression and streaming for fast large language model serving},
 year = {2024}
}



@article{liu2024deepseek,
  title={DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model},
  author={Liu, Aixin and Feng, Bei and Wang, Bin and Wang, Bingxuan and Liu, Bo and Zhao, Chenggang and Deng, Chengqi and Ruan, Chong and Dai, Damai and Guo, Daya and others},
  journal={CoRR},
  year={2024}
}

@article{liu2024longgenbench,
 author = {Liu, Xiang and Dong, Peijie and Hu, Xuming and Chu, Xiaowen},
 journal = {arXiv preprint arXiv:2410.04199},
 title = {Longgenbench: Long-context generation benchmark},
 year = {2024}
}


@misc{liu2024minicachekvcachecompression,
 archiveprefix = {arXiv},
 author = {Akide Liu and Jing Liu and Zizheng Pan and Yefei He and Gholamreza Haffari and Bohan Zhuang},
 eprint = {2405.14366},
 primaryclass = {cs.CL},
 title = {MiniCache: KV Cache Compression in Depth Dimension for Large Language Models},
 url = {https://arxiv.org/abs/2405.14366},
 year = {2024}
}

@inproceedings{Liu2025ThusSL,
  title={Thus Spake Long-Context Large Language Model},
  author={Xiaoran Liu and Ruixiao Li and Mianqiu Huang and Zhigeng Liu and Yuerong Song and Qipeng Guo and Siyang He and Qiqi Wang and Linlin Li and Qun Liu and Yaqian Zhou and Xuanjing Huang and Xipeng Qiu},
  year={2025},
  url={https://api.semanticscholar.org/CorpusID:276575712}
}

@article{Li2024PromptCF,
  title={Prompt Compression for Large Language Models: A Survey},
  author={Zongqian Li and Yinhong Liu and Yixuan Su and Nigel Collier},
  journal={ArXiv},
  year={2024},
  volume={abs/2410.12388},
  url={https://api.semanticscholar.org/CorpusID:273375474}
}

@article{DBLP:journals/corr/abs-2302-14502,
  author       = {Zican Dong and
                  Tianyi Tang and
                  Junyi Li and
                  Wayne Xin Zhao},
  title        = {A Survey on Long Text Modeling with Transformers},
  journal      = {CoRR},
  volume       = {abs/2302.14502},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2302.14502},
  doi          = {10.48550/ARXIV.2302.14502},
  eprinttype    = {arXiv},
  eprint       = {2302.14502},
  timestamp    = {Thu, 02 Mar 2023 10:23:33 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2302-14502.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{liu2024regmix,
 author = {Liu, Qian and Zheng, Xiaosen and Muennighoff, Niklas and Zeng, Guangtao and Dou, Longxu and Pang, Tianyu and Jiang, Jing and Lin, Min},
 journal = {arXiv preprint arXiv:2407.01492},
 title = {RegMix: Data Mixture as Regression for Language Model Pre-training},
 year = {2024}
}

@inproceedings{liu2024repobench,
 author = {Liu, Tianyang and Xu, Canwen and McAuley, Julian},
 booktitle = {The Twelfth International Conference on Learning Representations},
 title = {RepoBench: Benchmarking Repository-Level Code Auto-Completion Systems},
 year = {2024}
}

@article{liu2024repoqa,
 author = {Liu, Jiawei and Tian, Jia Le and Daita, Vijay and Wei, Yuxiang and Ding, Yifeng and Wang, Yuhan Katherine and Yang, Jun and Zhang, Lingming},
 journal = {arXiv preprint arXiv:2406.06025},
 title = {RepoQA: Evaluating Long Context Code Understanding},
 year = {2024}
}

@article{liu2023scaling,
  title={Scaling laws of rope-based extrapolation},
  author={Liu, Xiaoran and Yan, Hang and Zhang, Shuo and An, Chenxin and Qiu, Xipeng and Lin, Dahua},
  journal={arXiv preprint arXiv:2310.05209},
  year={2023}
}


@article{liu2024scissorhands,
 author = {Liu, Zichang and Desai, Aditya and Liao, Fangshuo and Wang, Weitao and Xie, Victor and Xu, Zhaozhuo and Kyrillidis, Anastasios and Shrivastava, Anshumali},
 journal = {Advances in Neural Information Processing Systems},
 title = {Scissorhands: Exploiting the persistence of importance hypothesis for llm kv cache compression at test time},
 volume = {36},
 year = {2024}
}

@article{liu2024stall+,
 author = {Liu, Junwei and Chen, Yixuan and Liu, Mingwei and Peng, Xin and Lou, Yiling},
 journal = {arXiv preprint arXiv:2406.10018},
 title = {STALL+: Boosting LLM-based Repository-level Code Completion with Static Analysis},
 year = {2024}
}

@article{liu2024understanding,
 author = {Liu, Yiheng and He, Hao and Han, Tianle and Zhang, Xu and Liu, Mengyuan and Tian, Jiaming and Zhang, Yutong and Wang, Jiaqi and Gao, Xiaohui and Zhong, Tianyang and others},
 journal = {arXiv preprint arXiv:2401.02038},
 title = {Understanding llms: A comprehensive overview from training to inference},
 year = {2024}
}

@inproceedings{liu2024unlocking,
  title={Unlocking Data-free Low-bit Quantization with Matrix Decomposition for KV Cache Compression},
  author={Liu, Peiyu and Gao, Ze-Feng and Zhao, Wayne Xin and Ma, Yipeng and Wang, Tao and Wen, Ji-Rong},
  booktitle={Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={2430--2440},
  year={2024}
}

@article{liu2024visualwebbench,
 author = {Liu, Junpeng and Song, Yifan and Lin, Bill Yuchen and Lam, Wai and Neubig, Graham and Li, Yuanzhi and Yue, Xiang},
 journal = {arXiv preprint arXiv:2404.05955},
 title = {VisualWebBench: How Far Have Multimodal LLMs Evolved in Web Page Understanding and Grounding?},
 year = {2024}
}

@article{DBLP:journals/corr/abs-2404-05955,
  author       = {Junpeng Liu and
                  Yifan Song and
                  Bill Yuchen Lin and
                  Wai Lam and
                  Graham Neubig and
                  Yuanzhi Li and
                  Xiang Yue},
  title        = {VisualWebBench: How Far Have Multimodal LLMs Evolved in Web Page Understanding
                  and Grounding?},
  journal      = {CoRR},
  volume       = {abs/2404.05955},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2404.05955},
  doi          = {10.48550/ARXIV.2404.05955},
  eprinttype    = {arXiv},
  eprint       = {2404.05955},
  timestamp    = {Mon, 03 Mar 2025 21:34:11 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2404-05955.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{liu2024what,
 author = {Wei Liu and Weihao Zeng and Keqing He and Yong Jiang and Junxian He},
 booktitle = {The Twelfth International Conference on Learning Representations},
 title = {What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning},
 year = {2024}
}

@article{liu2024world,
 author = {Liu, Hao and Yan, Wilson and Zaharia, Matei and Abbeel, Pieter},
 journal = {arXiv e-prints},
 pages = {arXiv--2402},
 title = {World model on million-length video and language with ringattention},
 year = {2024}
}

@article{liu2025bitdelta,
 author = {Liu, James and Xiao, Guangxuan and Li, Kai and Lee, Jason D and Han, Song and Dao, Tri and Cai, Tianle},
 journal = {Advances in Neural Information Processing Systems},
 pages = {13579--13600},
 title = {Bitdelta: Your fine-tune may only be worth one bit},
 volume = {37},
 year = {2025}
}

@article{longagent,
 author = {Jun Zhao and Can Zu and Hao Xu and Yi Lu and Wei He and Yiwen Ding and Tao Gui and Qi Zhang and Xuanjing Huang},
 journal = {arXiv preprint arXiv: 2402.11550},
 title = {LongAgent: Scaling Language Models to 128k Context through Multi-Agent Collaboration},
 year = {2024}
}

@inproceedings{longalign,
 address = {Miami, Florida, USA},
 author = {Bai, Yushi  and
Lv, Xin  and
Zhang, Jiajie  and
He, Yuze  and
Qi, Ji  and
Hou, Lei  and
Tang, Jie  and
Dong, Yuxiao  and
Li, Juanzi},
 booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2024},
 doi = {10.18653/v1/2024.findings-emnlp.74},
 editor = {Al-Onaizan, Yaser  and
Bansal, Mohit  and
Chen, Yun-Nung},
 month = {November},
 pages = {1376--1395},
 publisher = {Association for Computational Linguistics},
 title = {{L}ong{A}lign: A Recipe for Long Context Alignment of Large Language Models},
 url = {https://aclanthology.org/2024.findings-emnlp.74},
 year = {2024}
}

@misc{longdata,
 howpublished = {\url{https://huggingface.co/datasets/togethercomputer/Long-Data-Collections}},
 journal = {GitHub repository},
 publisher = {Huggingface},
 title = {Long-Data-Collections},
 year = {2023}
}

@inproceedings{longlora,
 author = {Yukang Chen and Shengju Qian and Haotian Tang and Xin Lai and Zhijian Liu and Song Han and Jiaya Jia},
 booktitle = {The International Conference on Learning Representations (ICLR)},
 title = {LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models},
 year = {2024}
}

@inproceedings{longmem,
  author       = {Weizhi Wang and
                  Li Dong and
                  Hao Cheng and
                  Xiaodong Liu and
                  Xifeng Yan and
                  Jianfeng Gao and
                  Furu Wei},
  editor       = {Alice Oh and
                  Tristan Naumann and
                  Amir Globerson and
                  Kate Saenko and
                  Moritz Hardt and
                  Sergey Levine},
  title        = {Augmenting Language Models with Long-Term Memory},
  booktitle    = {Advances in Neural Information Processing Systems 36: Annual Conference
                  on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans,
                  LA, USA, December 10 - 16, 2023},
  year         = {2023},
  url          = {http://papers.nips.cc/paper\_files/paper/2023/hash/ebd82705f44793b6f9ade5a669d0f0bf-Abstract-Conference.html},
  timestamp    = {Tue, 13 Aug 2024 08:01:40 +0200},
  biburl       = {https://dblp.org/rec/conf/nips/Wang0CLYGW23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{lozhkov2024starcoder2stackv2,
  author       = {Anton Lozhkov and
                  Raymond Li and
                  Loubna Ben Allal and
                  Federico Cassano and
                  Joel Lamy{-}Poirier and
                  Nouamane Tazi and
                  Ao Tang and
                  Dmytro Pykhtar and
                  Jiawei Liu and
                  Yuxiang Wei and
                  Tianyang Liu and
                  Max Tian and
                  Denis Kocetkov and
                  Arthur Zucker and
                  Younes Belkada and
                  Zijian Wang and
                  Qian Liu and
                  Dmitry Abulkhanov and
                  Indraneil Paul and
                  Zhuang Li and
                  Wen{-}Ding Li and
                  Megan Risdal and
                  Jia Li and
                  Jian Zhu and
                  Terry Yue Zhuo and
                  Evgenii Zheltonozhskii and
                  Nii Osae Osae Dade and
                  Wenhao Yu and
                  Lucas Krau{\ss} and
                  Naman Jain and
                  Yixuan Su and
                  Xuanli He and
                  Manan Dey and
                  Edoardo Abati and
                  Yekun Chai and
                  Niklas Muennighoff and
                  Xiangru Tang and
                  Muhtasham Oblokulov and
                  Christopher Akiki and
                  Marc Marone and
                  Chenghao Mou and
                  Mayank Mishra and
                  Alex Gu and
                  Binyuan Hui and
                  Tri Dao and
                  Armel Zebaze and
                  Olivier Dehaene and
                  Nicolas Patry and
                  Canwen Xu and
                  Julian J. McAuley and
                  Han Hu and
                  Torsten Scholak and
                  S{\'{e}}bastien Paquet and
                  Jennifer Robinson and
                  Carolyn Jane Anderson and
                  Nicolas Chapados and
                  et al.},
  title        = {StarCoder 2 and The Stack v2: The Next Generation},
  journal      = {CoRR},
  volume       = {abs/2402.19173},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2402.19173},
  doi          = {10.48550/ARXIV.2402.19173},
  eprinttype    = {arXiv},
  eprint       = {2402.19173},
  timestamp    = {Tue, 06 Aug 2024 08:17:53 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2402-19173.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{lu2024controlled,
 author = {Lu, Yi and Yan, Jing Nathan and Yang, Songlin and Chiu, Justin T and Ren, Siyu and Yuan, Fei and Zhao, Wenting and Wu, Zhiyong and Rush, Alexander M},
 journal = {arXiv preprint arXiv:2409.12181},
 title = {A controlled study on long context extension and generalization in llms},
 year = {2024}
}

@article{lu2024datasculpt,
 author = {Lu, Keer and Nie, Xiaonan and Liang, Zheng and Pan, Da and Zhang, Shusen and Zhao, Keshi and Chen, Weipeng and Zhou, Zenan and Dong, Guosheng and Cui, Bin and others},
 journal = {arXiv preprint arXiv:2409.00997},
 title = {DataSculpt: Crafting Data Landscapes for Long-Context LLMs through Multi-Objective Partitioning},
 year = {2024}
}

@inproceedings{lu2024longheadsmultiheadattentionsecretly4,
  author       = {Yi Lu and
                  Xin Zhou and
                  Wei He and
                  Jun Zhao and
                  Tao Ji and
                  Tao Gui and
                  Qi Zhang and
                  Xuanjing Huang},
  title        = {LongHeads: Multi-Head Attention is Secretly a Long Context Processor},
  booktitle    = {{EMNLP} (Findings)},
  pages        = {7136--7148},
  publisher    = {Association for Computational Linguistics},
  year         = {2024}
}

@article{lu2025moba,
 author = {Lu, Enzhe and Jiang, Zhejun and Liu, Jingyuan and Du, Yulun and Jiang, Tao and Hong, Chao and Liu, Shaowei and He, Weiran and Yuan, Enming and Wang, Yuzhi and others},
 journal = {arXiv preprint arXiv:2502.13189},
 title = {MoBA: Mixture of Block Attention for Long-Context LLMs},
 year = {2025}
}

@misc{lu2025mobamixtureblockattention,
 archiveprefix = {arXiv},
 author = {Enzhe Lu and Zhejun Jiang and Jingyuan Liu and Yulun Du and Tao Jiang and Chao Hong and Shaowei Liu and Weiran He and Enming Yuan and Yuzhi Wang and Zhiqi Huang and Huan Yuan and Suting Xu and Xinran Xu and Guokun Lai and Yanru Chen and Huabin Zheng and Junjie Yan and Jianlin Su and Yuxin Wu and Neo Y. Zhang and Zhilin Yang and Xinyu Zhou and Mingxing Zhang and Jiezhong Qiu},
 eprint = {2502.13189},
 primaryclass = {cs.LG},
 title = {MoBA: Mixture of Block Attention for Long-Context LLMs},
 url = {https://arxiv.org/abs/2502.13189},
 year = {2025}
}

@misc{lucas2024extraglobalattentiondesignation,
 archiveprefix = {arXiv},
 author = {Evan Lucas and Dylan Kangas and Timothy C Havens},
 eprint = {2410.08971},
 primaryclass = {cs.CL},
 title = {Extra Global Attention Designation Using Keyword Detection in Sparse Transformer Architectures},
 url = {https://arxiv.org/abs/2410.08971},
 year = {2024}
}

@article{luo2024bge,
 author = {Luo, Kun and Liu, Zheng and Xiao, Shitao and Liu, Kang},
 journal = {arXiv preprint arXiv:2402.11573},
 title = {BGE Landmark Embedding: A Chunking-Free Embedding Method For Retrieval Augmented Long-Context Large Language Models},
 year = {2024}
}

@article{luo2024repoagent,
 author = {Luo, Qinyu and Ye, Yining and Liang, Shihao and Zhang, Zhong and Qin, Yujia and Lu, Yaxi and Wu, Yesai and Cong, Xin and Lin, Yankai and Zhang, Yingli and others},
 journal = {arXiv preprint arXiv:2402.16667},
 title = {Repoagent: An llm-powered open-source framework for repository-level code documentation generation},
 year = {2024}
}


@inproceedings{lyu2023paradigm,
  author       = {Chenyang Lyu and
                  Zefeng Du and
                  Jitao Xu and
                  Yitao Duan and
                  Minghao Wu and
                  Teresa Lynn and
                  Alham Fikri Aji and
                  Derek F. Wong and
                  Longyue Wang},
  editor       = {Nicoletta Calzolari and
                  Min{-}Yen Kan and
                  V{\'{e}}ronique Hoste and
                  Alessandro Lenci and
                  Sakriani Sakti and
                  Nianwen Xue},
  title        = {A Paradigm Shift: The Future of Machine Translation Lies with Large
                  Language Models},
  booktitle    = {Proceedings of the 2024 Joint International Conference on Computational
                  Linguistics, Language Resources and Evaluation, {LREC/COLING} 2024,
                  20-25 May, 2024, Torino, Italy},
  pages        = {1339--1352},
  publisher    = {{ELRA} and {ICCL}},
  year         = {2024},
  url          = {https://aclanthology.org/2024.lrec-main.120},
  timestamp    = {Thu, 23 May 2024 16:47:05 +0200},
  biburl       = {https://dblp.org/rec/conf/coling/LyuD0DWLAWW24.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{ma-2024-arxiv-mmlongbenchdoc,
 author = {Yubo Ma and
Yuhang Zang and
Liangyu Chen and
Meiqi Chen and
Yizhu Jiao and
Xinze Li and
Xinyuan Lu and
Ziyu Liu and
Yan Ma and
Xiaoyi Dong and
Pan Zhang and
Liangming Pan and
Yu{-}Gang Jiang and
Jiaqi Wang and
Yixin Cao and
Aixin Sun},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/journals/corr/abs-2407-01523.bib},
 doi = {10.48550/ARXIV.2407.01523},
 eprint = {2407.01523},
 eprinttype = {arXiv},
 journal = {CoRR},
 timestamp = {Mon, 13 Jan 2025 12:07:44 +0100},
 title = {MMLongBench-Doc: Benchmarking Long-context Document Understanding
with Visualizations},
 url = {https://doi.org/10.48550/arXiv.2407.01523},
 volume = {abs/2407.01523},
 year = {2024}
}

@article{ma-2024-arxiv-vlora,
 author = {Feipeng Ma and
Hongwei Xue and
Guangting Wang and
Yizhou Zhou and
Fengyun Rao and
Shilin Yan and
Yueyi Zhang and
Siying Wu and
Mike Zheng Shou and
Xiaoyan Sun},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/journals/corr/abs-2405-20339.bib},
 doi = {10.48550/ARXIV.2405.20339},
 eprint = {2405.20339},
 eprinttype = {arXiv},
 journal = {CoRR},
 timestamp = {Mon, 24 Jun 2024 10:16:39 +0200},
 title = {Visual Perception by Large Language Model's Weights},
 url = {https://doi.org/10.48550/arXiv.2405.20339},
 volume = {abs/2405.20339},
 year = {2024}
}

@article{ma2024era,
 author = {Ma, Shuming and Wang, Hongyu and Ma, Lingxiao and Wang, Lei and Wang, Wenhui and Huang, Shaohan and Dong, Lifeng and Wang, Ruiping and Xue, Jilong and Wei, Furu},
 journal = {arXiv preprint arXiv:2402.17764},
 publisher = {arXivpreprint},
 title = {The era of 1-bit llms: All large language models are in 1.58 bits},
 volume = {1},
 year = {2024}
}

@inproceedings{ma2024fine,
 author = {Ma, Xueguang and Wang, Liang and Yang, Nan and Wei, Furu and Lin, Jimmy},
 booktitle = {Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval},
 pages = {2421--2425},
 title = {Fine-tuning llama for multi-stage text retrieval},
 year = {2024}
}

@misc{ma2024megalodonefficientllmpretraining,
 archiveprefix = {arXiv},
 author = {Xuezhe Ma and Xiaomeng Yang and Wenhan Xiong and Beidi Chen and Lili Yu and Hao Zhang and Jonathan May and Luke Zettlemoyer and Omer Levy and Chunting Zhou},
 eprint = {2404.08801},
 primaryclass = {cs.LG},
 title = {Megalodon: Efficient LLM Pretraining and Inference with Unlimited Context Length},
 url = {https://arxiv.org/abs/2404.08801},
 year = {2024}
}

@article{ma2025cot,
 author = {Ma, Xinyin and Wan, Guangnian and Yu, Runpeng and Fang, Gongfan and Wang, Xinchao},
 journal = {arXiv preprint arXiv:2502.09601},
 title = {CoT-Valve: Length-Compressible Chain-of-Thought Tuning},
 year = {2025}
}

@misc{maekawa2025holistic,
 author = {Seiji Maekawa\textsuperscript{*} and Hayate Iso\textsuperscript{*} and Nikita Bhutani},
 note = {\textsuperscript{*}These authors contributed equally to this work.},
 title = {Holistic Reasoning with Long-Context LMs: A Benchmark for Database Operations on Massive Textual Data},
 url = {https://arxiv.org/abs/2410.11996},
 year = {2024}
}

@article{malaviya2023expertqa,
 author = {Malaviya, Chaitanya and Lee, Subin and Chen, Sihao and Sieber, Elizabeth and Yatskar, Mark and Roth, Dan},
 journal = {arXiv preprint arXiv:2309.07852},
 title = {Expertqa: Expert-curated questions and attributed answers},
 year = {2023}
}

@article{malaviya2024dolomites,
 author = {Malaviya, Chaitanya and Agrawal, Priyanka and Ganchev, Kuzman and Srinivasan, Pranesh and Huot, Fantine and Berant, Jonathan and Yatskar, Mark and Das, Dipanjan and Lapata, Mirella and Alberti, Chris},
 journal = {arXiv preprint arXiv:2405.05938},
 title = {DOLOMITES: Domain-Specific Long-Form Methodical Tasks},
 year = {2024}
}

@article{masry2024longfin,
 author = {Masry, Ahmed and Hajian, Amir},
 journal = {arXiv preprint arXiv:2401.15050},
 title = {LongFin: A Multimodal Document Understanding Model for Long Financial Domain Documents},
 year = {2024}
}

@article{DBLP:journals/corr/abs-2401-15050,
  author       = {Ahmed Masry and
                  Amir Hajian},
  title        = {LongFin: {A} Multimodal Document Understanding Model for Long Financial
                  Domain Documents},
  journal      = {CoRR},
  volume       = {abs/2401.15050},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2401.15050},
  doi          = {10.48550/ARXIV.2401.15050},
  eprinttype    = {arXiv},
  eprint       = {2401.15050},
  timestamp    = {Tue, 06 Feb 2024 14:15:49 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2401-15050.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{Memformer,
 author = {Qingyang Wu and
Zhenzhong Lan and
Jing Gu and
Zhou Yu},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/journals/corr/abs-2010-06891.bib},
 eprint = {2010.06891},
 eprinttype = {arXiv},
 journal = {CoRR},
 timestamp = {Sun, 21 Jul 2024 18:16:17 +0200},
 title = {Memformer: The Memory-Augmented Transformer},
 url = {https://arxiv.org/abs/2010.06891},
 volume = {abs/2010.06891},
 year = {2020}
}

@inproceedings{memorizingtransformer,
  author       = {Yuhuai Wu and
                  Markus Norman Rabe and
                  DeLesley Hutchins and
                  Christian Szegedy},
  title        = {Memorizing Transformers},
  booktitle    = {{ICLR}},
  publisher    = {OpenReview.net},
  year         = {2022}
}

@inproceedings{memoryllm,
 author = {Yu Wang and Yifan Gao and Xiusi Chen and Haoming Jiang and Shiyang Li and Jingfeng Yang and Qingyu Yin and Zheng Li and Xian Li and Bing Yin and Jingbo Shang and Julian J. McAuley},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/icml/WangGCJLYYLLYSM24.bib},
 booktitle = {Forty-first International Conference on Machine Learning, {ICML} 2024, Vienna, Austria, July 21-27, 2024},
 publisher = {OpenReview.net},
 timestamp = {Mon, 30 Sep 2024 07:54:39 +0200},
 title = {{MEMORYLLM:} Towards Self-Updatable Large Language Models},
 url = {https://openreview.net/forum?id=p0lKWzdikQ},
 year = {2024}
}

@article{memprompt,
 author = {Aman Madaan and Niket Tandon and Peter Clark and Yiming Yang},
 journal = {arXiv preprint arXiv: 2201.06009},
 title = {Memory-assisted prompt editing to improve GPT-3 after deployment},
 year = {2022}
}

@article{memwalker,
 author = {Howard Chen and Ramakanth Pasunuru and Jason Weston and Asli Celikyilmaz},
 journal = {arXiv preprint arXiv: 2310.05029},
 title = {Walking Down the Memory Maze: Beyond Context Limit through Interactive Reading},
 year = {2023}
}

@article{men2024shortgpt,
 author = {Men, Xin and Xu, Mingyu and Zhang, Qingyu and Wang, Bingning and Lin, Hongyu and Lu, Yaojie and Han, Xianpei and Chen, Weipeng},
 journal = {arXiv preprint arXiv:2403.03853},
 title = {Shortgpt: Layers in large language models are more redundant than you expect},
 year = {2024}
}

@inproceedings{meng2022locating,
 author = {Meng, Kevin and Bau, David and Andonian, Alex and Belinkov, Yonatan},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {17359--17372},
 publisher = {Curran Associates, Inc.},
 title = {Locating and Editing Factual Associations in GPT},
 url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/6f1d43d5a82a37e89b0665b33bf3a182-Paper-Conference.pdf},
 volume = {35},
 year = {2022}
}

@article{Meng2024SimPOSP,
 author = {Yu Meng and Mengzhou Xia and Danqi Chen},
 journal = {ArXiv},
 title = {SimPO: Simple Preference Optimization with a Reference-Free Reward},
 volume = {abs/2405.14734},
 year = {2024}
}

@article{micikevicius2017mixed,
 author = {Micikevicius, Paulius and Narang, Sharan and Alben, Jonah and Diamos, Gregory and Elsen, Erich and Garcia, David and Ginsburg, Boris and Houston, Michael and Kuchaiev, Oleksii and Venkatesh, Ganesh and others},
 journal = {arXiv preprint arXiv:1710.03740},
 title = {Mixed precision training},
 year = {2017}
}

@misc{microsoft_blog_graphrag_2024,
 author = {Microsoft},
 howpublished = {\url{https://www.microsoft.com/en-us/research/blog/graphrag-unlocking-llm-discovery-on-narrative-private-data/}},
 title = {GraphRAG: Unlocking LLM Discovery on Narrative Private Data}
}

@misc{contextual-retrieval,
 author = {Anthropic},
 howpublished = {\url{https://www.anthropic.com/news/contextual-retrieval}},
 title = {Introducing Contextual Retrieval}
}


@inproceedings{miculicich2018document,
  author       = {Lesly Miculicich and
                  Dhananjay Ram and
                  Nikolaos Pappas and
                  James Henderson},
  editor       = {Ellen Riloff and
                  David Chiang and
                  Julia Hockenmaier and
                  Jun'ichi Tsujii},
  title        = {Document-Level Neural Machine Translation with Hierarchical Attention
                  Networks},
  booktitle    = {Proceedings of the 2018 Conference on Empirical Methods in Natural
                  Language Processing, Brussels, Belgium, October 31 - November 4, 2018},
  pages        = {2947--2954},
  publisher    = {Association for Computational Linguistics},
  year         = {2018},
  url          = {https://doi.org/10.18653/v1/d18-1325},
  doi          = {10.18653/V1/D18-1325},
  timestamp    = {Wed, 07 Dec 2022 23:12:22 +0100},
  biburl       = {https://dblp.org/rec/conf/emnlp/WerlenRPH18.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@article{min2023factscore,
 author = {Min, Sewon and Krishna, Kalpesh and Lyu, Xinxi and Lewis, Mike and Yih, Wen-tau and Koh, Pang Wei and Iyyer, Mohit and Zettlemoyer, Luke and Hajishirzi, Hannaneh},
 journal = {arXiv preprint arXiv:2305.14251},
 title = {Factscore: Fine-grained atomic evaluation of factual precision in long form text generation},
 year = {2023}
}

@misc{minimax2025minimax01scalingfoundationmodels,
 archiveprefix = {arXiv},
 author = {MiniMax and Aonian Li and Bangwei Gong and Bo Yang and Boji Shan and Chang Liu and Cheng Zhu and Chunhao Zhang and Congchao Guo and Da Chen and Dong Li and Enwei Jiao and Gengxin Li and Guojun Zhang and Haohai Sun and Houze Dong and Jiadai Zhu and Jiaqi Zhuang and Jiayuan Song and Jin Zhu and Jingtao Han and Jingyang Li and Junbin Xie and Junhao Xu and Junjie Yan and Kaishun Zhang and Kecheng Xiao and Kexi Kang and Le Han and Leyang Wang and Lianfei Yu and Liheng Feng and Lin Zheng and Linbo Chai and Long Xing and Meizhi Ju and Mingyuan Chi and Mozhi Zhang and Peikai Huang and Pengcheng Niu and Pengfei Li and Pengyu Zhao and Qi Yang and Qidi Xu and Qiexiang Wang and Qin Wang and Qiuhui Li and Ruitao Leng and Shengmin Shi and Shuqi Yu and Sichen Li and Songquan Zhu and Tao Huang and Tianrun Liang and Weigao Sun and Weixuan Sun and Weiyu Cheng and Wenkai Li and Xiangjun Song and Xiao Su and Xiaodong Han and Xinjie Zhang and Xinzhu Hou and Xu Min and Xun Zou and Xuyang Shen and Yan Gong and Yingjie Zhu and Yipeng Zhou and Yiran Zhong and Yongyi Hu and Yuanxiang Fan and Yue Yu and Yufeng Yang and Yuhao Li and Yunan Huang and Yunji Li and Yunpeng Huang and Yunzhi Xu and Yuxin Mao and Zehan Li and Zekang Li and Zewei Tao and Zewen Ying and Zhaoyang Cong and Zhen Qin and Zhenhua Fan and Zhihang Yu and Zhuo Jiang and Zijia Wu},
 eprint = {2501.08313},
 primaryclass = {cs.CL},
 title = {MiniMax-01: Scaling Foundation Models with Lightning Attention},
 url = {https://arxiv.org/abs/2501.08313},
 year = {2025}
}

@misc{mishra2024granitecodemodelsfamily,
 archiveprefix = {arXiv},
 author = {Mayank Mishra and Matt Stallone and Gaoyuan Zhang and Yikang Shen and Aditya Prasad and Adriana Meza Soria and Michele Merler and Parameswaran Selvam and Saptha Surendran and Shivdeep Singh and Manish Sethi and Xuan-Hong Dang and Pengyuan Li and Kun-Lung Wu and Syed Zawad and Andrew Coleman and Matthew White and Mark Lewis and Raju Pavuluri and Yan Koyfman and Boris Lublinsky and Maximilien de Bayser and Ibrahim Abdelaziz and Kinjal Basu and Mayank Agarwal and Yi Zhou and Chris Johnson and Aanchal Goyal and Hima Patel and Yousaf Shah and Petros Zerfos and Heiko Ludwig and Asim Munawar and Maxwell Crouse and Pavan Kapanipathi and Shweta Salaria and Bob Calio and Sophia Wen and Seetharami Seelam and Brian Belgodere and Carlos Fonseca and Amith Singhee and Nirmit Desai and David D. Cox and Ruchir Puri and Rameswar Panda},
 eprint = {2405.04324},
 primaryclass = {cs.AI},
 title = {Granite Code Models: A Family of Open Foundation Models for Code Intelligence},
 url = {https://arxiv.org/abs/2405.04324},
 year = {2024}
}

@article{DBLP:journals/corr/abs-2405-04324,
  author       = {Mayank Mishra and
                  Matt Stallone and
                  Gaoyuan Zhang and
                  Yikang Shen and
                  Aditya Prasad and
                  Adriana Meza Soria and
                  Michele Merler and
                  Parameswaran Selvam and
                  Saptha Surendran and
                  Shivdeep Singh and
                  Manish Sethi and
                  Xuan{-}Hong Dang and
                  Pengyuan Li and
                  Kun{-}Lung Wu and
                  Syed Zawad and
                  Andrew Coleman and
                  Matthew White and
                  Mark Lewis and
                  Raju Pavuluri and
                  Yan Koyfman and
                  Boris Lublinsky and
                  Maximilien de Bayser and
                  Ibrahim Abdelaziz and
                  Kinjal Basu and
                  Mayank Agarwal and
                  Yi Zhou and
                  Chris Johnson and
                  Aanchal Goyal and
                  Hima Patel and
                  S. Yousaf Shah and
                  Petros Zerfos and
                  Heiko Ludwig and
                  Asim Munawar and
                  Maxwell Crouse and
                  Pavan Kapanipathi and
                  Shweta Salaria and
                  Bob Calio and
                  Sophia Wen and
                  Seetharami Seelam and
                  Brian Belgodere and
                  Carlos A. Fonseca and
                  Amith Singhee and
                  Nirmit Desai and
                  David D. Cox and
                  Ruchir Puri and
                  Rameswar Panda},
  title        = {Granite Code Models: {A} Family of Open Foundation Models for Code
                  Intelligence},
  journal      = {CoRR},
  volume       = {abs/2405.04324},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2405.04324},
  doi          = {10.48550/ARXIV.2405.04324},
  eprinttype    = {arXiv},
  eprint       = {2405.04324},
  timestamp    = {Fri, 05 Jul 2024 11:05:45 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2405-04324.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{mo2024survey,
 author = {Fengran Mo and Kelong Mao and Ziliang Zhao and Hongjin Qian and Haonan Chen and Yiruo Cheng and Xiaoxi Li and Yutao Zhu and Zhicheng Dou and Jian-Yun Nie},
 journal = {arXiv preprint arXiv: 2410.15576},
 title = {A Survey of Conversational Search},
 year = {2024}
}

@article{modarressi2025nolima,
 author = {Modarressi, Ali and Deilamsalehy, Hanieh and Dernoncourt, Franck and Bui, Trung and Rossi, Ryan A and Yoon, Seunghyun and Sch{\"u}tze, Hinrich},
 journal = {arXiv preprint arXiv:2502.05167},
 title = {NoLiMa: Long-Context Evaluation Beyond Literal Matching},
 year = {2025}
}

@article{mohr2024multi,
 author = {Mohr, Isabelle and Krimmel, Markus and Sturua, Saba and Akram, Mohammad Kalim and Koukounas, Andreas and G{\"u}nther, Michael and Mastrapas, Georgios and Ravishankar, Vinit and Mart{\'\i}nez, Joan Fontanals and Wang, Feng and others},
 journal = {arXiv preprint arXiv:2402.17016},
 title = {Multi-Task Contrastive Learning for 8192-Token Bilingual Text Embeddings},
 year = {2024}
}

@article{mohtashami2023landmark,
 author = {Mohtashami, Amirkeivan and Jaggi, Martin},
 journal = {arXiv preprint arXiv:2305.16300},
 title = {Landmark Attention: Random-Access Infinite Context Length for Transformers},
 year = {2023}
}

@misc{moonshot2023kimi,
 author = {Moonshot AI},
 title = {Kimi Chat},
 URL = {https://kimi.moonshot.cn/},
 year = {2023}
}

@misc{claude3_7,
 author = {anthropic},
 title = {Claude 3.7 Sonnet},
 URL = {https://www.anthropic.com/news/claude-3-7-sonnet/},
 year = {2025}
}

@misc{mu2024learningcompresspromptsgist,
 archiveprefix = {arXiv},
 author = {Jesse Mu and Xiang Lisa Li and Noah Goodman},
 eprint = {2304.08467},
 primaryclass = {cs.CL},
 title = {Learning to Compress Prompts with Gist Tokens},
 url = {https://arxiv.org/abs/2304.08467},
 year = {2024}
}

@inproceedings{NEURIPS2023_3d77c6dc,
 author = {Mu, Jesse and Li, Xiang and Goodman, Noah},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {19327--19352},
 publisher = {Curran Associates, Inc.},
 title = {Learning to Compress Prompts with Gist Tokens},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/3d77c6dcc7f143aa2154e7f4d5e22d68-Paper-Conference.pdf},
 volume = {36},
 year = {2023}
}

@article{muennighoff2025s1,
 author = {Muennighoff, Niklas and Yang, Zitong and Shi, Weijia and Li, Xiang Lisa and Fei-Fei, Li and Hajishirzi, Hannaneh and Zettlemoyer, Luke and Liang, Percy and Cand{\`e}s, Emmanuel and Hashimoto, Tatsunori},
 journal = {arXiv preprint arXiv:2501.19393},
 title = {s1: Simple test-time scaling},
 year = {2025}
}

@misc{munkhdalai2024leavecontextbehindefficient,
 archiveprefix = {arXiv},
 author = {Tsendsuren Munkhdalai and Manaal Faruqui and Siddharth Gopal},
 eprint = {2404.07143},
 primaryclass = {cs.CL},
 title = {Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention},
 url = {https://arxiv.org/abs/2404.07143},
 year = {2024}
}

@article{Nagrani-2024-arxiv-Neptune,
 author = {Arsha Nagrani and
Mingda Zhang and
Ramin Mehran and
Rachel Hornung and
Nitesh Bharadwaj Gundavarapu and
Nilpa Jha and
Austin Myers and
Xingyi Zhou and
Boqing Gong and
Cordelia Schmid and
Mikhail Sirotenko and
Yukun Zhu and
Tobias Weyand},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/journals/corr/abs-2412-09582.bib},
 doi = {10.48550/ARXIV.2412.09582},
 eprint = {2412.09582},
 eprinttype = {arXiv},
 journal = {CoRR},
 timestamp = {Mon, 20 Jan 2025 22:09:51 +0100},
 title = {Neptune: The Long Orbit to Benchmarking Long Video Understanding},
 url = {https://doi.org/10.48550/arXiv.2412.09582},
 volume = {abs/2412.09582},
 year = {2024}
}

@misc{nakanishi2025softmax_flat_problem,
 archiveprefix = {arXiv},
 author = {Ken M. Nakanishi},
 eprint = {2501.19399},
 primaryclass = {cs.CL},
 title = {Scalable-Softmax Is Superior for Attention},
 url = {https://arxiv.org/abs/2501.19399},
 year = {2025}
}

@article{nallapati2016abstractive,
 author = {Nallapati, Ramesh and Zhou, Bowen and Gulcehre, Caglar and Xiang, Bing and others},
 journal = {arXiv preprint arXiv:1602.06023},
 title = {Abstractive text summarization using sequence-to-sequence rnns and beyond},
 year = {2016}
}

@misc{nanda2023mechanistic,
 author = {Neel Nanda},
 note = {Accessed: 2025-01-30},
 title = {Mechanistic Interpretability Quickstart Guide},
 url = {https://www.neelnanda.io/mechanistic-interpretability/quickstart},
 year = {2023}
}

@article{narrativeqa,
 address = {Cambridge, MA},
 author = {Ko{\v{c}}isk{\'y}, Tom{\'a}{\v{s}}  and
Schwarz, Jonathan  and
Blunsom, Phil  and
Dyer, Chris  and
Hermann, Karl Moritz  and
Melis, G{\'a}bor  and
Grefenstette, Edward},
 doi = {10.1162/tacl_a_00023},
 editor = {Lee, Lillian  and
Johnson, Mark  and
Toutanova, Kristina  and
Roark, Brian},
 journal = {Transactions of the Association for Computational Linguistics},
 pages = {317--328},
 publisher = {MIT Press},
 title = {The {N}arrative{QA} Reading Comprehension Challenge},
 url = {https://aclanthology.org/Q18-1023/},
 volume = {6},
 year = {2018}
}

@misc{needleinhaystack,
 author = {Kamradt, Greg},
 commit = {4f57d6a0e4c030202a07a60bc1bb1ed1544bf679},
 howpublished = {\url{https://github.com/gkamradt/LLMTest_NeedleInAHaystack}},
 journal = {GitHub repository},
 publisher = {GitHub},
 title = {Needle In A Haystack - Pressure Testing LLMs},
 year = {2023}
}

@article{nguyen2016ms,
 author = {Nguyen, Tri and Rosenberg, Mir and Song, Xia and Gao, Jianfeng and Tiwary, Saurabh and Majumder, Rangan and Deng, Li},
 title = {Ms marco: A human-generated machine reading comprehension dataset},
 year = {2016}
}

@article{nie2024survey,
  author       = {Yuqi Nie and
                  Yaxuan Kong and
                  Xiaowen Dong and
                  John M. Mulvey and
                  H. Vincent Poor and
                  Qingsong Wen and
                  Stefan Zohren},
  title        = {A Survey of Large Language Models for Financial Applications: Progress,
                  Prospects and Challenges},
  journal      = {CoRR},
  volume       = {abs/2406.11903},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2406.11903},
  doi          = {10.48550/ARXIV.2406.11903},
  eprinttype    = {arXiv},
  eprint       = {2406.11903},
  timestamp    = {Sun, 06 Oct 2024 21:25:18 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2406-11903.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{nocha-2024-karp-thai-et-al,
 archiveprefix = {arXiv},
 author = {Marzena Karpinska and Katherine Thai and Kyle Lo and Tanya Goyal and Mohit Iyyer},
 eprint = {https://arxiv.org/abs/2406.16264},
 primaryclass = {cs.CL},
 title = {One Thousand and One Pairs: A "novel" challenge for long-context language models},
 year = {2024}
}

@misc{ntk,
 author = {Bowen Peng and Jeffrey Quesnelle},
 howpublished = {\url{https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_scaled_rope_allows_llama_models_to_have}},
 title = { NTK-Aware Scaled RoPE allows LLaMA models to have extended (8k+) context size without any fine-tuning and minimal perplexity degradation.},
 year = {2023}
}

@article{min2020ambigqa,
  title={AmbigQA: Answering ambiguous open-domain questions},
  author={Min, Sewon and Michael, Julian and Hajishirzi, Hannaneh and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:2004.10645},
  year={2020}
}

@misc{NtkAlibi2023,
 author = { },
 month = {August},
 title = {NTK-ALiBi: Long Text Extrapolation of ALiBi Position Encoding through Interpolation},
 url = {https://github.com/keezen/ntk_alibi},
 year = {2023}
}

@article{nussbaum2024nomic,
 author = {Nussbaum, Zach and Morris, John X and Duderstadt, Brandon and Mulyar, Andriy},
 journal = {arXiv preprint arXiv:2402.01613},
 title = {Nomic embed: Training a reproducible long context text embedder},
 year = {2024}
}

@misc{nvidia_h200,
 author = {{NVIDIA Corporation}},
 title = {NVIDIA H200},
 url = {https://www.nvidia.com/en-us/data-center/h200/},
 year = {2023}
}


@misc{oai2025deepresearch,
 author = {Team, OpenAI},
 howpublished = {\url{https://openai.com/index/introducing-deep-research/}},
 title = {Introducing deep research},
 year = {2025}
}

@misc{oord2018neuraldiscreterepresentationlearning,
 archiveprefix = {arXiv},
 author = {Aaron van den Oord and Oriol Vinyals and Koray Kavukcuoglu},
 eprint = {1711.00937},
 primaryclass = {cs.LG},
 title = {Neural Discrete Representation Learning},
 url = {https://arxiv.org/abs/1711.00937},
 year = {2018}
}

@misc{openai2024embedding,
 author = {OpenAI},
 note = {Accessed: 2024-01-25},
 title = {New Embedding Models and API Updates},
 url = {https://openai.com/index/new-embedding-models-and-api-updates/},
 year = {2024}
}

@misc{openai2024memory,
 author = {OpenAI},
 note = {Accessed: 2024-02-13},
 title = {Memory and New Controls for ChatGPT},
 url = {https://openai.com/index/memory-and-new-controls-for-chatgpt},
 year = {2024}
}

@misc{openai_o1_2024,
 author = {OpenAI},
 howpublished = {\url{https://openai.com/index/learning-to-reason-with-llms/}},
 title = {Learning to Reason with Large Language Models},
 year = {2024}
}

@misc{oren2024transformersmultistaternns,
 archiveprefix = {arXiv},
 author = {Matanel Oren and Michael Hassid and Nir Yarden and Yossi Adi and Roy Schwartz},
 eprint = {2401.06104},
 primaryclass = {cs.CL},
 title = {Transformers are Multi-State RNNs},
 url = {https://arxiv.org/abs/2401.06104},
 year = {2024}
}

@inproceedings{orpo,
 address = {Miami, Florida, USA},
 author = {Hong, Jiwoo  and
Lee, Noah  and
Thorne, James},
 booktitle = {Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
 doi = {10.18653/v1/2024.emnlp-main.626},
 editor = {Al-Onaizan, Yaser  and
Bansal, Mohit  and
Chen, Yun-Nung},
 month = {November},
 pages = {11170--11189},
 publisher = {Association for Computational Linguistics},
 title = {{ORPO}: Monolithic Preference Optimization without Reference Model},
 url = {https://aclanthology.org/2024.emnlp-main.626},
 year = {2024}
}

@article{Ouyang2022TrainingLM,
 author = {Long Ouyang and Jeff Wu and Xu Jiang and Diogo Almeida and Carroll L. Wainwright and Pamela Mishkin and Chong Zhang and Sandhini Agarwal and Katarina Slama and Alex Ray and John Schulman and Jacob Hilton and Fraser Kelton and Luke E. Miller and Maddie Simens and Amanda Askell and Peter Welinder and Paul Francis Christiano and Jan Leike and Ryan J. Lowe},
 journal = {ArXiv},
 title = {Training language models to follow instructions with human feedback},
 url = {https://api.semanticscholar.org/CorpusID:246426909},
 volume = {abs/2203.02155},
 year = {2022}
}

@article{pagliardini2023fast,
 author = {Pagliardini, Matteo and Paliotta, Daniele and Jaggi, Martin and Fleuret, Fran{\c{c}}ois},
 journal = {Advances in Neural Information Processing Systems},
 pages = {59808--59831},
 title = {Fast attention over long sequences with dynamic sparse flash attention},
 volume = {36},
 year = {2023}
}

@article{pal2023giraffe,
 author = {Pal, Arka and Karkhanis, Deep and Roberts, Manley and Dooley, Samuel and Sundararajan, Arvind and Naidu, Siddartha},
 journal = {arXiv preprint arXiv:2308.10882},
 title = {Giraffe: Adventures in expanding context lengths in llms},
 year = {2023}
}

@inproceedings{pamqa,
 address = {Bangkok, Thailand},
 author = {He, Junqing  and
Pan, Kunhao  and
Dong, Xiaoqun  and
Song, Zhuoyang  and
LiuYiBo, LiuYiBo  and
Qianguosun, Qianguosun  and
Liang, Yuxin  and
Wang, Hao  and
Zhang, Enming  and
Zhang, Jiaxing},
 booktitle = {Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
 doi = {10.18653/v1/2024.acl-long.736},
 editor = {Ku, Lun-Wei  and
Martins, Andre  and
Srikumar, Vivek},
 month = {August},
 pages = {13628--13642},
 publisher = {Association for Computational Linguistics},
 title = {Never Lost in the Middle: Mastering Long-Context Question Answering with Position-Agnostic Decompositional Training},
 url = {https://aclanthology.org/2024.acl-long.736},
 year = {2024}
}

@misc{pan2024llmlingua2datadistillationefficient,
 archiveprefix = {arXiv},
 author = {Zhuoshi Pan and Qianhui Wu and Huiqiang Jiang and Menglin Xia and Xufang Luo and Jue Zhang and Qingwei Lin and Victor Rühle and Yuqing Yang and Chin-Yew Lin and H. Vicky Zhao and Lili Qiu and Dongmei Zhang},
 eprint = {2403.12968},
 primaryclass = {cs.CL},
 title = {LLMLingua-2: Data Distillation for Efficient and Faithful Task-Agnostic Prompt Compression},
 url = {https://arxiv.org/abs/2403.12968},
 year = {2024}
}

@inproceedings{pan-etal-2024-llmlingua,
    title = "{LLML}ingua-2: Data Distillation for Efficient and Faithful Task-Agnostic Prompt Compression",
    author = {Pan, Zhuoshi  and
      Wu, Qianhui  and
      Jiang, Huiqiang  and
      Xia, Menglin  and
      Luo, Xufang  and
      Zhang, Jue  and
      Lin, Qingwei  and
      R{\"u}hle, Victor  and
      Yang, Yuqing  and
      Lin, Chin-Yew  and
      Zhao, H. Vicky  and
      Qiu, Lili  and
      Zhang, Dongmei},
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2024",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-acl.57/",
    doi = "10.18653/v1/2024.findings-acl.57",
    pages = "963--981",
    abstract = "This paper focuses on task-agnostic prompt compression for better generalizability and efficiency. Considering the redundancy in natural language, existing approaches compress prompts by removing tokens or lexical units according to their information entropy obtained from a causal language model such as LLaMa-7B. The challenge is that information entropy may be a suboptimal compression metric: (i) it only leverages unidirectional context and may fail to capture all essential information needed for prompt compression; (ii) it is not aligned with the prompt compression objective.To address these issues, we propose a data distillation procedure to derive knowledge from an LLM to compress prompts without losing crucial information, and meantime, introduce an extractive text compression dataset. We formulate prompt compression as a token classification problem to guarantee the faithfulness of the compressed prompt to the original one, and use a Transformer encoder as the base architecture to capture all essential information for prompt compression from the full bidirectional context. Our approach leads to lower latency by explicitly learning the compression objective with smaller models such as XLM-RoBERTa-large and mBERT.We evaluate our method on both in-domain and out-of-domain datasets, including MeetingBank, LongBench, ZeroScrolls, GSM8K, and BBH. Despite its small size, our model shows significant performance gains over strong baselines and demonstrates robust generalization ability across different LLMs. Additionally, our model is 3x-6x faster than existing prompt compression methods, while accelerating the end-to-end latency by 1.6x-2.9x with compression ratios of 2x-5x."
}

@article{pang2021quality,
 author = {Pang, Richard Yuanzhe and Parrish, Alicia and Joshi, Nitish and Nangia, Nikita and Phang, Jason and Chen, Angelica and Padmakumar, Vishakh and Ma, Johnny and Thompson, Jana and He, He and others},
 journal = {arXiv preprint arXiv:2112.08608},
 title = {QuALITY: Question answering with long input texts, yes!},
 year = {2021}
}

@inproceedings{pang2022quality,
 author = {Pang, Richard Yuanzhe and Parrish, Alicia and Joshi, Nitish and Nangia, Nikita and Phang, Jason and Chen, Angelica and Padmakumar, Vishakh and Ma, Johnny and Thompson, Jana and He, He and others},
 booktitle = {Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
 pages = {5336--5358},
 title = {QuALITY: Question Answering with Long Input Texts, Yes!},
 year = {2022}
}

@inproceedings{papineni2002bleu,
 author = {Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
 booktitle = {Proceedings of the 40th annual meeting of the Association for Computational Linguistics},
 pages = {311--318},
 title = {Bleu: a method for automatic evaluation of machine translation},
 year = {2002}
}

@inproceedings{park2024improving,
 author = {Park, Daon and Egger, Bernhard},
 booktitle = {Proceedings of the 2024 International Conference on Parallel Architectures and Compilation Techniques},
 pages = {233--245},
 title = {Improving Throughput-oriented LLM Inference with CPU Computations},
 year = {2024}
}

@inproceedings{parkcan,
  author       = {Jongho Park and
                  Jaeseung Park and
                  Zheyang Xiong and
                  Nayoung Lee and
                  Jaewoong Cho and
                  Samet Oymak and
                  Kangwook Lee and
                  Dimitris Papailiopoulos},
  title        = {Can Mamba Learn How To Learn? {A} Comparative Study on In-Context
                  Learning Tasks},
  booktitle    = {{ICML}},
  publisher    = {OpenReview.net},
  year         = {2024}
}

@inproceedings{patel2024splitwise,
 author = {Patel, Pratyush and Choukse, Esha and Zhang, Chaojie and Shah, Aashaka and Goiri, {\'I}{\~n}igo and Maleki, Saeed and Bianchini, Ricardo},
 booktitle = {2024 ACM/IEEE 51st Annual International Symposium on Computer Architecture (ISCA)},
 organization = {IEEE},
 pages = {118--132},
 title = {Splitwise: Efficient generative llm inference using phase splitting},
 year = {2024}
}

@article{pearl,
 author = {Simeng Sun and Y. Liu and Shuo Wang and Chenguang Zhu and Mohit Iyyer},
 bibsource = {Semantic Scholar https://www.semanticscholar.org/paper/4ee96f0757e517928590a2300af5d40ba768a5a7},
 doi = {10.48550/arXiv.2305.14564},
 journal = {Conference of the European Chapter of the Association for Computational Linguistics},
 title = {PEARL: Prompting Large Language Models to Plan and Execute Actions Over Long Documents},
 year = {2023}
}

@article{peng2023fp8,
 author = {Peng, Houwen and Wu, Kan and Wei, Yixuan and Zhao, Guoshuai and Yang, Yuxiang and Liu, Ze and Xiong, Yifan and Yang, Ziyue and Ni, Bolin and Hu, Jingcheng and others},
 journal = {arXiv preprint arXiv:2310.18313},
 title = {Fp8-lm: Training fp8 large language models},
 year = {2023}
}

@misc{peng2023ntk,
 author = {Peng, Bowen and Quesnelle, Jeffrey},
 title = {Ntk-aware scaled rope allows llama models to have extended (8k+) context size without any fine-tuning and minimal perplexity degradation},
 year = {2023}
}

@inproceedings{peng2023rwkvreinventingrnnstransformer,
  author       = {Bo Peng and
                  Eric Alcaide and
                  Quentin Anthony and
                  Alon Albalak and
                  Samuel Arcadinho and
                  Stella Biderman and
                  Huanqi Cao and
                  Xin Cheng and
                  Michael Chung and
                  Leon Derczynski and
                  Xingjian Du and
                  Matteo Grella and
                  Kranthi Kiran GV and
                  Xuzheng He and
                  Haowen Hou and
                  Przemyslaw Kazienko and
                  Jan Kocon and
                  Jiaming Kong and
                  Bartlomiej Koptyra and
                  Hayden Lau and
                  Jiaju Lin and
                  Krishna Sri Ipsit Mantri and
                  Ferdinand Mom and
                  Atsushi Saito and
                  Guangyu Song and
                  Xiangru Tang and
                  Johan S. Wind and
                  Stanislaw Wozniak and
                  Zhenyuan Zhang and
                  Qinghua Zhou and
                  Jian Zhu and
                  Rui{-}Jie Zhu},
  title        = {{RWKV:} Reinventing RNNs for the Transformer Era},
  booktitle    = {{EMNLP} (Findings)},
  pages        = {14048--14077},
  publisher    = {Association for Computational Linguistics},
  year         = {2023}
}

@article{peng2023yarn,
 author = {Peng, Bowen and Quesnelle, Jeffrey and Fan, Honglu and Shippole, Enrico},
 journal = {arXiv preprint arXiv:2309.00071},
 title = {Yarn: Efficient context window extension of large language models},
 year = {2023}
}

@misc{peng2024eaglefinchrwkvmatrixvalued,
 archiveprefix = {arXiv},
 author = {Bo Peng and Daniel Goldstein and Quentin Anthony and Alon Albalak and Eric Alcaide and Stella Biderman and Eugene Cheah and Xingjian Du and Teddy Ferdinan and Haowen Hou and Przemysław Kazienko and Kranthi Kiran GV and Jan Kocoń and Bartłomiej Koptyra and Satyapriya Krishna and Ronald McClelland Jr. and Jiaju Lin and Niklas Muennighoff and Fares Obeid and Atsushi Saito and Guangyu Song and Haoqin Tu and Cahya Wirawan and Stanisław Woźniak and Ruichong Zhang and Bingchen Zhao and Qihang Zhao and Peng Zhou and Jian Zhu and Rui-Jie Zhu},
 eprint = {2404.05892},
 primaryclass = {cs.CL},
 title = {Eagle and Finch: RWKV with Matrix-Valued States and Dynamic Recurrence},
 url = {https://arxiv.org/abs/2404.05892},
 year = {2024}
}

@inproceedings{peng2024yarn,
 author = {Peng, Bowen and Quesnelle, Jeffrey and Fan, Honglu and Shippole, Enrico},
 booktitle = {The Twelfth International Conference on Learning Representations},
 title = {YaRN: Efficient Context Window Extension of Large Language Models},
 year = {2024}
}

@misc{perplexity_pages,
 author = {Perplexity Labs},
 note = {Accessed: 2024-05-30},
 title = {perplexity},
 url = {https://www.perplexity.ai}
}

@article{pham2024suri,
 author = {Pham, Chau Minh and Sun, Simeng and Iyyer, Mohit},
 journal = {arXiv preprint arXiv:2406.19371},
 title = {Suri: Multi-constraint instruction following for long-form text generation},
 year = {2024}
}

@article{phan2024examining,
 author = {Phan, Hung and Acharya, Anurag and Meyur, Rounak and Chaturvedi, Sarthak and Sharma, Shivam and Parker, Mike and Nally, Dan and Jannesari, Ali and Pazdernik, Karl and Halappanavar, Mahantesh and others},
 journal = {arXiv preprint arXiv:2407.07321},
 title = {Examining Long-Context Large Language Models for Environmental Review Document Comprehension},
 year = {2024}
}


@article{phan2024repohyper,
  author       = {Huy Nhat Phan and
                  Hoang Nhat Phan and
                  Tien N. Nguyen and
                  Nghi D. Q. Bui},
  title        = {RepoHyper: Better Context Retrieval Is All You Need for Repository-Level
                  Code Completion},
  journal      = {CoRR},
  volume       = {abs/2403.06095},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2403.06095},
  doi          = {10.48550/ARXIV.2403.06095},
  eprinttype    = {arXiv},
  eprint       = {2403.06095},
  timestamp    = {Thu, 12 Sep 2024 20:54:18 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2403-06095.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{phi,
 author = {Suriya Gunasekar and
Yi Zhang and
Jyoti Aneja and
Caio C{\'{e}}sar Teodoro Mendes and
Allie Del Giorno and
Sivakanth Gopi and
Mojan Javaheripi and
Piero Kauffmann and
Gustavo de Rosa and
Olli Saarikivi and
Adil Salim and
Shital Shah and
Harkirat Singh Behl and
Xin Wang and
S{\'{e}}bastien Bubeck and
Ronen Eldan and
Adam Tauman Kalai and
Yin Tat Lee and
Yuanzhi Li},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/journals/corr/abs-2306-11644.bib},
 doi = {10.48550/ARXIV.2306.11644},
 eprint = {2306.11644},
 eprinttype = {arXiv},
 journal = {CoRR},
 timestamp = {Mon, 28 Aug 2023 21:26:20 +0200},
 title = {Textbooks Are All You Need},
 url = {https://doi.org/10.48550/arXiv.2306.11644},
 volume = {abs/2306.11644},
 year = {2023}
}

@article{pilault2024block,
 author = {Pilault, Jonathan and Fathi, Mahan and Firat, Orhan and Pal, Chris and Bacon, Pierre-Luc and Goroshin, Ross},
 journal = {Advances in Neural Information Processing Systems},
 title = {Block-state transformers},
 volume = {36},
 year = {2024}
}

@misc{ping2025longdpounlockbetterlongform,
 archiveprefix = {arXiv},
 author = {Bowen Ping and Jiali Zeng and Fandong Meng and Shuo Wang and Jie Zhou and Shanghang Zhang},
 eprint = {2502.02095},
 primaryclass = {cs.CL},
 title = {LongDPO: Unlock Better Long-form Generation Abilities for LLMs via Critique-augmented Stepwise Information},
 url = {https://arxiv.org/abs/2502.02095},
 year = {2025}
}

@inproceedings{pmlr-v235-liu24bz,
 author = {Liu, Zirui and Yuan, Jiayi and Jin, Hongye and Zhong, Shaochen and Xu, Zhaozhuo and Braverman, Vladimir and Chen, Beidi and Hu, Xia},
 booktitle = {Proceedings of the 41st International Conference on Machine Learning},
 pages = {32332--32344},
 title = {{KIVI}: A Tuning-Free Asymmetric 2bit Quantization for {KV} Cache},
 year = {2024}
}

@article{pouransari2024dataset,
 author = {Pouransari, Hadi and Li, Chun-Liang and Chang, Jen-Hao Rick and Vasu, Pavan Kumar Anasosalu and Koc, Cem and Shankar, Vaishaal and Tuzel, Oncel},
 journal = {arXiv preprint arXiv:2405.13226},
 title = {Dataset Decomposition: Faster LLM Training with Variable Sequence Length Curriculum},
 year = {2024}
}

@article{pramanick-2024-arxiv-spiqa,
 author = {Shraman Pramanick and
Rama Chellappa and
Subhashini Venugopalan},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/journals/corr/abs-2407-09413.bib},
 doi = {10.48550/ARXIV.2407.09413},
 eprint = {2407.09413},
 eprinttype = {arXiv},
 journal = {CoRR},
 timestamp = {Thu, 15 Aug 2024 11:20:56 +0200},
 title = {{SPIQA:} {A} Dataset for Multimodal Question Answering on Scientific
Papers},
 url = {https://doi.org/10.48550/arXiv.2407.09413},
 volume = {abs/2407.09413},
 year = {2024}
}

@article{press2021train,
 author = {Press, Ofir and Smith, Noah A and Lewis, Mike},
 journal = {arXiv preprint arXiv:2108.12409},
 title = {Train short, test long: Attention with linear biases enables input length extrapolation},
 year = {2021}
}

@inproceedings{prolong,
 address = {Bangkok, Thailand},
 author = {Chen, Longze  and
Liu, Ziqiang  and
He, Wanwei  and
Zheng, Yinhe  and
Sun, Hao  and
Li, Yunshui  and
Luo, Run  and
Yang, Min},
 booktitle = {Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
 doi = {10.18653/v1/2024.acl-long.447},
 editor = {Ku, Lun-Wei  and
Martins, Andre  and
Srikumar, Vivek},
 month = {August},
 pages = {8222--8234},
 publisher = {Association for Computational Linguistics},
 title = {Long Context is Not Long at All: A Prospector of Long-Dependency Data for Large Language Models},
 url = {https://aclanthology.org/2024.acl-long.447},
 year = {2024}
}

@article{qi2023zero,
 author = {Qi, Penghui and Wan, Xinyi and Huang, Guangxing and Lin, Min},
 journal = {arXiv preprint arXiv:2401.10241},
 title = {Zero bubble pipeline parallelism},
 year = {2023}
}

@inproceedings{qi2024long2rag,
 author = {Qi, Zehan and Xu, Rongwu and Guo, Zhijiang and Wang, Cunxiang and Zhang, Hao and Xu, Wei},
 booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2024},
 pages = {4852--4872},
 title = {LONG2RAG: Evaluating Long-Context \& Long-Form Retrieval-Augmented Generation with Key Point Recall},
 year = {2024}
}

@inproceedings{qin2022cosformerrethinkingsoftmaxattention,
  author       = {Zhen Qin and
                  Weixuan Sun and
                  Hui Deng and
                  Dongxu Li and
                  Yunshen Wei and
                  Baohong Lv and
                  Junjie Yan and
                  Lingpeng Kong and
                  Yiran Zhong},
  title        = {cosFormer: Rethinking Softmax In Attention},
  booktitle    = {{ICLR}},
  publisher    = {OpenReview.net},
  year         = {2022}
}

@misc{qin2024lightningattention2freelunch,
 archiveprefix = {arXiv},
 author = {Zhen Qin and Weigao Sun and Dong Li and Xuyang Shen and Weixuan Sun and Yiran Zhong},
 eprint = {2401.04658},
 primaryclass = {cs.CL},
 title = {Lightning Attention-2: A Free Lunch for Handling Unlimited Sequence Lengths in Large Language Models},
 url = {https://arxiv.org/abs/2401.04658},
 year = {2024}
}

@article{qin2024mooncake,
 author = {Qin, Ruoyu and Li, Zheming and He, Weiran and Zhang, Mingxing and Wu, Yongwei and Zheng, Weimin and Xu, Xinran},
 journal = {arXiv preprint arXiv:2407.00079},
 title = {Mooncake: A kvcache-centric disaggregated architecture for llm serving},
 year = {2024}
}

@article{qin2024o1,
 author = {Qin, Yiwei and Li, Xuefeng and Zou, Haoyang and Liu, Yixiu and Xia, Shijie and Huang, Zhen and Ye, Yixin and Yuan, Weizhe and Liu, Hector and Li, Yuanzhi and others},
 journal = {arXiv preprint arXiv:2410.18982},
 title = {O1 Replication Journey: A Strategic Progress Report--Part 1},
 year = {2024}
}

@inproceedings{qin2024variouslengthsconstantspeed,
  author       = {Zhen Qin and
                  Weigao Sun and
                  Dong Li and
                  Xuyang Shen and
                  Weixuan Sun and
                  Yiran Zhong},
  title        = {Various Lengths, Constant Speed: Efficient Language Modeling with
                  Lightning Attention},
  booktitle    = {{ICML}},
  publisher    = {OpenReview.net},
  year         = {2024}
}

@article{qiu2024clongeval,
 author = {Qiu, Zexuan and Li, Jingjing and Huang, Shijue and Jiao, Xiaoqi and Zhong, Wanjun and King, Irwin},
 journal = {arXiv preprint arXiv:2403.03514},
 title = {Clongeval: A chinese benchmark for evaluating long-context large language models},
 year = {2024}
}

@article{quan2024language,
 author = {Quan, Shanghaoran and Tang, Tianyi and Yu, Bowen and Yang, An and Liu, Dayiheng and Gao, Bofei and Tu, Jianhong and Zhang, Yichang and Zhou, Jingren and Lin, Junyang},
 journal = {arXiv preprint arXiv:2410.23933},
 title = {Language Models can Self-Lengthen to Generate Long Texts},
 year = {2024}
}

@article{que2024hellobench,
 author = {Que, Haoran and Duan, Feiyu and He, Liqun and Mou, Yutao and Zhou, Wangchunshu and Liu, Jiaheng and Rong, Wenge and Wang, Zekun Moore and Yang, Jian and Zhang, Ge and others},
 journal = {arXiv preprint arXiv:2409.16191},
 title = {Hellobench: Evaluating long text generation capabilities of large language models},
 year = {2024}
}

@article{quest,
 author = {Chaochen Gao and Xing Wu and Qingfang Fu and Songlin Hu},
 journal = {ArXiv},
 title = {Quest: Query-centric Data Synthesis Approach for Long-context Scaling of Large Language Model},
 volume = {abs/2405.19846},
 year = {2024}
}

@misc{Qwen2.5-VL,
 author = {Qwen Team},
 month = {January},
 title = {Qwen2.5-VL},
 url = {https://qwenlm.github.io/blog/qwen2.5-vl/},
 year = {2025}
}

@article{radford2018improving,
 author = {Radford, Alec},
 title = {Improving language understanding by generative pre-training},
 year = {2018}
}

@article{Rafailov2023DirectPO,
 author = {Rafael Rafailov and Archit Sharma and Eric Mitchell and Stefano Ermon and Christopher D. Manning and Chelsea Finn},
 journal = {ArXiv},
 title = {Direct Preference Optimization: Your Language Model is Secretly a Reward Model},
 url = {https://api.semanticscholar.org/CorpusID:258959321},
 volume = {abs/2305.18290},
 year = {2023}
}

@article{raffel2020exploring,
 author = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
 journal = {Journal of machine learning research},
 number = {140},
 pages = {1--67},
 title = {Exploring the limits of transfer learning with a unified text-to-text transformer},
 volume = {21},
 year = {2020}
}

@inproceedings{rasley2020deepspeed,
 author = {Rasley, Jeff and Rajbhandari, Samyam and Ruwase, Olatunji and He, Yuxiong},
 booktitle = {Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
 pages = {3505--3506},
 title = {Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters},
 year = {2020}
}

@inproceedings{re3,
 author = {Kevin Yang and Yuandong Tian and Nanyun Peng and Dan Klein},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/emnlp/YangTPK22.bib},
 booktitle = {Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, {EMNLP} 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022},
 doi = {10.18653/V1/2022.EMNLP-MAIN.296},
 editor = {Yoav Goldberg and Zornitsa Kozareva and Yue Zhang},
 pages = {4393-4479},
 publisher = {Association for Computational Linguistics},
 timestamp = {Thu, 10 Aug 2023 12:35:40 +0200},
 title = {Re3: Generating Longer Stories With Recursive Reprompting and Revision},
 url = {https://doi.org/10.18653/v1/2022.emnlp-main.296},
 year = {2022}
}

@inproceedings{readagent,
 author = {Kuang{-}Huei Lee and Xinyun Chen and Hiroki Furuta and John F. Canny and Ian Fischer},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/icml/LeeCFCF24.bib},
 booktitle = {Forty-first International Conference on Machine Learning, {ICML} 2024, Vienna, Austria, July 21-27, 2024},
 publisher = {OpenReview.net},
 timestamp = {Mon, 02 Sep 2024 16:55:26 +0200},
 title = {A Human-Inspired Reading Agent with Gist Memory of Very Long Contexts},
 url = {https://openreview.net/forum?id=OTmcsyEO5G},
 year = {2024}
}

@inproceedings{realm,
 articleno = {368},
 author = {Guu, Kelvin and Lee, Kenton and Tung, Zora and Pasupat, Panupong and Chang, Ming-Wei},
 booktitle = {Proceedings of the 37th International Conference on Machine Learning},
 numpages = {10},
 publisher = {JMLR.org},
 series = {ICML'20},
 title = {REALM: retrieval-augmented language model pre-training},
 year = {2020}
}

@article{recomp,
 author = {Fangyuan Xu and Weijia Shi and Eunsol Choi},
 journal = {arXiv preprint arXiv: 2310.04408},
 title = {RECOMP: Improving Retrieval-Augmented LMs with Compression and Selective Augmentation},
 year = {2023}
}

@article{recurrentgemma,
 author = {Aleksandar Botev and
Soham De and
Samuel L. Smith and
Anushan Fernando and
George{-}Cristian Muraru and
Ruba Haroun and
Leonard Berrada and
Razvan Pascanu and
Pier Giuseppe Sessa and
Robert Dadashi and
L{\'{e}}onard Hussenot and
Johan Ferret and
Sertan Girgin and
Olivier Bachem and
Alek Andreev and
Kathleen Kenealy and
Thomas Mesnard and
Cassidy Hardin and
Surya Bhupatiraju and
Shreya Pathak and
Laurent Sifre and
Morgane Rivi{\`{e}}re and
Mihir Sanjay Kale and
Juliette Love and
Pouya Tafti and
Armand Joulin and
Noah Fiedel and
Evan Senter and
Yutian Chen and
Srivatsan Srinivasan and
Guillaume Desjardins and
David Budden and
Arnaud Doucet and
Sharad Vikram and
Adam Paszke and
Trevor Gale and
Sebastian Borgeaud and
Charlie Chen and
Andy Brock and
Antonia Paterson and
Jenny Brennan and
Meg Risdal and
Raj Gundluru and
Nesh Devanathan and
Paul Mooney and
Nilay Chauhan and
Phil Culliton and
Luiz GUStavo Martins and
Elisa Bandy and
David Huntsperger and
Glenn Cameron and
Arthur Zucker and
Tris Warkentin and
Ludovic Peran and
Minh Giang and
Zoubin Ghahramani and
Cl{\'{e}}ment Farabet and
Koray Kavukcuoglu and
Demis Hassabis and
Raia Hadsell and
Yee Whye Teh and
Nando de Frietas},
 journal = {CoRR},
 title = {RecurrentGemma: Moving Past Transformers for Efficient Open Language
Models},
 volume = {abs/2404.07839},
 year = {2024}
}

@article{recurrentgpt,
 author = {Wangchunshu Zhou and Yuchen Eleanor Jiang and Peng Cui and Tiannan Wang and Zhenxin Xiao and Yifan Hou and Ryan Cotterell and Mrinmaya Sachan},
 journal = {arXiv preprint arXiv: 2305.13304},
 title = {RecurrentGPT: Interactive Generation of (Arbitrarily) Long Text},
 year = {2023}
}

@inproceedings{reddy2024docfinqa,
  author       = {Varshini Reddy and
                  Rik Koncel{-}Kedziorski and
                  Viet Dac Lai and
                  Michael Krumdick and
                  Charles Lovering and
                  Chris Tanner},
  editor       = {Lun{-}Wei Ku and
                  Andre Martins and
                  Vivek Srikumar},
  title        = {DocFinQA: {A} Long-Context Financial Reasoning Dataset},
  booktitle    = {Proceedings of the 62nd Annual Meeting of the Association for Computational
                  Linguistics, {ACL} 2024 - Short Papers, Bangkok, Thailand, August
                  11-16, 2024},
  pages        = {445--458},
  publisher    = {Association for Computational Linguistics},
  year         = {2024},
  url          = {https://aclanthology.org/2024.acl-short.42},
  timestamp    = {Fri, 11 Oct 2024 22:05:01 +0200},
  biburl       = {https://dblp.org/rec/conf/acl/ReddyKLKLT24.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{reflexion,
 author = {Noah Shinn and Federico Cassano and Edward Berman and Ashwin Gopinath and Karthik Narasimhan and Shunyu Yao},
 journal = {arXiv preprint arXiv: 2303.11366},
 title = {Reflexion: Language Agents with Verbal Reinforcement Learning},
 year = {2023}
}

@article{rehg2024kv,
 author = {Rehg, Isaac},
 journal = {arXiv preprint arXiv:2410.00161},
 title = {KV-Compress: Paged KV-Cache Compression with Variable Compression Rates per Attention Head},
 year = {2024}
}

@article{Reid2024Gemini1U,
 author = {Gemini 1.5 Team},
 journal = {ArXiv},
 title = {Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context},
 volume = {abs/2403.05530},
 year = {2024}
}

@misc{ren2024efficacyevictionpolicykeyvalue,
 archiveprefix = {arXiv},
 author = {Siyu Ren and Kenny Q. Zhu},
 eprint = {2402.06262},
 primaryclass = {cs.CL},
 title = {On the Efficacy of Eviction Policy for Key-Value Constrained Generative Language Model Inference},
 url = {https://arxiv.org/abs/2402.06262},
 year = {2024}
}

@article{ren2024samba,
 author = {Ren, Liliang and Liu, Yang and Lu, Yadong and Shen, Yelong and Liang, Chen and Chen, Weizhu},
 journal = {arXiv preprint arXiv:2406.07522},
 title = {Samba: Simple hybrid state space models for efficient unlimited context language modeling},
 year = {2024}
}



@misc{ren2024sambasimplehybridstate,
 archiveprefix = {arXiv},
 author = {Liliang Ren and Yang Liu and Yadong Lu and Yelong Shen and Chen Liang and Weizhu Chen},
 eprint = {2406.07522},
 primaryclass = {cs.CL},
 title = {Samba: Simple Hybrid State Space Models for Efficient Unlimited Context Language Modeling},
 url = {https://arxiv.org/abs/2406.07522},
 year = {2024}
}

@inproceedings{replug,
 abstract = {We introduce REPLUG, a retrieval-augmented language modeling framework that treats the language model (LM) as a black box and augments it with a tuneable retrieval model. Unlike prior retrieval-augmented LMs that train language models with special cross-attention mechanisms to encode the retrieved text, REPLUG simply prepends retrieved documents to the input for the frozen black-box LM. This simple design can be easily applied to any existing language models. Furthermore, we show that the LM can be used to supervise the retrieval model, which can then find documents that help the LM make better predictions. Our experiments demonstrate that REPLUG with the tuned retriever significantly improves the performance of GPT-3 (175B) on language modeling by 6.3{\%}, as well as the performance of Codex on five-shot MMLU by 5.1{\%}. Code is publicly released at github.com/swj0419/REPLUG.},
 address = {Mexico City, Mexico},
 author = {Shi, Weijia and Min, Sewon and Yasunaga, Michihiro and Seo, Minjoon and James, Richard and Lewis, Mike and Zettlemoyer, Luke and Yih, Wen-tau},
 booktitle = {Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)},
 doi = {10.18653/v1/2024.naacl-long.463},
 editor = {Duh, Kevin and Gomez, Helena and Bethard, Steven},
 month = {jun},
 pages = {8371-8384},
 publisher = {Association for Computational Linguistics},
 title = {{REPLUG}: Retrieval-Augmented Black-Box Language Models},
 url = {https://aclanthology.org/2024.naacl-long.463/},
 year = {2024}
}



@article{retro,
 author = {Sebastian Borgeaud and A. Mensch and Jordan Hoffmann and Trevor Cai and Eliza Rutherford and Katie Millican and George van den Driessche and Jean-Baptiste Lespiau and Bogdan Damoc and Aidan Clark and Diego de Las Casas and Aurelia Guy and Jacob Menick and Roman Ring and T. Hennigan and Saffron Huang and Lorenzo Maggiore and Chris Jones and Albin Cassirer and Andy Brock and Michela Paganini and G. Irving and O. Vinyals and Simon Osindero and K. Simonyan and Jack W. Rae and Erich Elsen and L. Sifre},
 bibsource = {Semantic Scholar https://www.semanticscholar.org/paper/002c256d30d6be4b23d365a8de8ae0e67e4c9641},
 journal = {International Conference on Machine Learning},
 title = {Improving language models by retrieving from trillions of tokens},
 year = {2021}
}

@article{rmt,
 author = {Aydar Bulatov and
Yuri Kuratov and
Mikhail S. Burtsev},
 journal = {CoRR},
 title = {Scaling Transformer to 1M tokens and beyond with {RMT}},
 volume = {abs/2304.11062},
 year = {2023}
}

@article{roberts2024needle,
 author = {Roberts, Jonathan and Han, Kai and Albanie, Samuel},
 journal = {arXiv preprint arXiv:2411.05000},
 title = {Needle Threading: Can LLMs Follow Threads through Near-Million-Scale Haystacks?},
 year = {2024}
}

@inproceedings{ronningstad2024entity,
 author = {R{\o}nningstad, Egil and Klinger, Roman and Velldal, Erik and {\O}vrelid, Lilja},
 booktitle = {Proceedings of the 14th Workshop on Computational Approaches to Subjectivity, Sentiment, \& Social Media Analysis},
 pages = {84--96},
 title = {Entity-Level Sentiment: More than the Sum of Its Parts},
 year = {2024}
}

@article{rosenthal2024clapnq,
 author = {Rosenthal, Sara and Sil, Avirup and Florian, Radu and Roukos, Salim},
 journal = {arXiv preprint arXiv:2404.02103},
 title = {CLAPNQ: Cohesive Long-form Answers from Passages in Natural Questions for RAG systems},
 year = {2024}
}

@article{ruan2024defining,
 author = {Ruan, Jie and Wang, Wenqing and Wan, Xiaojun},
 journal = {arXiv preprint arXiv:2406.07935},
 title = {Defining and detecting vulnerability in human evaluation guidelines: A preliminary study towards reliable nlg evaluation},
 year = {2024}
}

@article{Ruoss-2024-arxiv-lmact,
 author = {Anian Ruoss and
Fabio Pardo and
Harris Chan and
Bonnie Li and
Volodymyr Mnih and
Tim Genewein},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/journals/corr/abs-2412-01441.bib},
 doi = {10.48550/ARXIV.2412.01441},
 eprint = {2412.01441},
 eprinttype = {arXiv},
 journal = {CoRR},
 timestamp = {Sun, 12 Jan 2025 00:08:02 +0100},
 title = {LMAct: {A} Benchmark for In-Context Imitation Learning with Long Multimodal
Demonstrations},
 url = {https://doi.org/10.48550/arXiv.2412.01441},
 volume = {abs/2412.01441},
 year = {2024}
}

@article{ruoss2023randomized,
 author = {Ruoss, Anian and Del{\'e}tang, Gr{\'e}goire and Genewein, Tim and Grau-Moya, Jordi and Csord{\'a}s, R{\'o}bert and Bennani, Mehdi and Legg, Shane and Veness, Joel},
 journal = {arXiv preprint arXiv:2305.16843},
 title = {Randomized positional encodings boost length generalization of transformers},
 year = {2023}
}


@inproceedings{saad2024benchmarking,
  author       = {Jon Saad{-}Falcon and
                  Daniel Y. Fu and
                  Simran Arora and
                  Neel Guha and
                  Christopher R{\'{e}}},
  title        = {Benchmarking and Building Long-Context Retrieval Models with LoCo
                  and {M2-BERT}},
  booktitle    = {Forty-first International Conference on Machine Learning, {ICML} 2024,
                  Vienna, Austria, July 21-27, 2024},
  publisher    = {OpenReview.net},
  year         = {2024},
  url          = {https://openreview.net/forum?id=HkCRgoGtt6},
  timestamp    = {Mon, 02 Sep 2024 16:55:26 +0200},
  biburl       = {https://dblp.org/rec/conf/icml/Saad-FalconFAGR24.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{saeidi2024triple,
 archiveprefix = {arXiv},
 author = {Amir Saeidi and Shivanshu Verma and Aswin RRV and Chitta Baral},
 eprint = {2405.16681},
 primaryclass = {cs.CL},
 title = {Triple Preference Optimization: Achieving Better Alignment with Less Data in a Single Step Optimization},
 year = {2024}
}

@article{sailor2report,
 author = {Longxu Dou and Qian Liu and Fan Zhou and Changyu Chen and Zili Wang and Ziqi Jin and Zichen Liu and Tongyao Zhu and Cunxiao Du and Penghui Yang and Haonan Wang and Jiaheng Liu and Yongchi Zhao and Xiachong Feng and Xin Mao and Man Tsung Yeung and Kunat Pipatanakul and Fajri Koto and Min Si Thu and Hynek Kydl{\'\i}{\v{c}}ek and Zeyi Liu and Qunshu Lin and Sittipong Sripaisarnmongkol and Kridtaphad Sae-Khow and Nirattisai Thongchim and Taechawat Konkaew and Narong Borijindargoon and Anh Dao and Matichon Maneegard and Phakphum Artkaew and Zheng-Xin Yong and Quan Nguyen and Wannaphong Phatthiyaphaibun and Hoang H. Tran and Mike Zhang and Shiqi Chen and Tianyu Pang and Chao Du and Xinyi Wan and Wei Lu and Min Lin},
 journal = {arXiv preprint arXiv:2502.12982},
 title = {Sailor2: Sailing in South-East Asia with Inclusive Multilingual LLM},
 year = {2025}
}

@article{Scao2022BLOOMA1,
 author = {Teven Le Scao and Angela Fan and Christopher Akiki and Ellie Pavlick and Suzana Ili'c and Daniel Hesslow and Roman Castagn'e and Alexandra Sasha Luccioni and François Yvon and Matthias Gall{\'e} and Jonathan Tow and Alexander M. Rush and Stella Biderman and Albert Webson and Pawan Sasanka Ammanamanchi and Thomas Wang and Beno{\^i}t Sagot and Niklas Muennighoff and Albert Villanova del Moral and Olatunji Ruwase and Rachel Bawden and Stas Bekman and Angelina McMillan-Major and Iz Beltagy and Huu Nguyen and Lucile Saulnier and Samson Tan and Pedro Ortiz Suarez and Victor Sanh and Hugo Laurenccon and Yacine Jernite and Julien Launay and Margaret Mitchell and Colin Raffel and Aaron Gokaslan and Adi Simhi and Aitor Soroa Etxabe and Alham Fikri Aji and Amit Alfassy and Anna Rogers and Ariel Kreisberg Nitzav and Canwen Xu and Chenghao Mou and Chris C. Emezue and Christopher Klamm and Colin Leong and Daniel Alexander van Strien and David Ifeoluwa Adelani and Dragomir R. Radev and Eduardo Gonz'alez Ponferrada and Efrat Levkovizh and Ethan Kim and Eyal Natan and Francesco De Toni and G{\'e}rard Dupont and Germ{\'a}n Kruszewski and Giada Pistilli and Hady ElSahar and Hamza Benyamina and Hieu Trung Tran and Ian Yu and Idris Abdulmumin and Isaac Johnson and Itziar Gonzalez-Dios and Javier de la Rosa and Jenny Chim and Jesse Dodge and Jian Zhu and Jonathan Chang and Jorg Frohberg and Josephine Tobing and Joydeep Bhattacharjee and Khalid Almubarak and Kimbo Chen and Kyle Lo and Leandro von Werra and Leon Weber and Long Phan and Loubna Ben Allal and Ludovic Tanguy and Manan Dey and Manuel Romero Mu{\~n}oz and Maraim Masoud and Mar{\'i}a Grandury and Mario vSavsko and Max Huang and Maximin Coavoux and Mayank Singh and Mike Tian-Jian Jiang and Minh Chien Vu and Moham-mad A. Jauhar and Mustafa Ghaleb and Nishant Subramani and Nora Kassner and Nurulaqilla Khamis and Olivier Nguyen and Omar Espejel and Ona de Gibert and Paulo Villegas and Peter Henderson and Pierre Colombo and Priscilla Amuok and Quentin Lhoest and Rheza Harliman and Rishi Bommasani and Roberto L'opez and Rui Ribeiro and Salomey Osei and Sampo Pyysalo and Sebastian Nagel and Shamik Bose and Shamsuddeen Hassan Muhammad and Shanya Sharma Sharma and S. Longpre and Somaieh Nikpoor and S. Silberberg and Suhas Pai and Sydney Zink and Tiago Timponi Torrent and Timo Schick and Tristan Thrush and Valentin Danchev and Vassilina Nikoulina and Veronika Laippala and Violette Lepercq and Vrinda Prabhu and Zaid Alyafeai and Zeerak Talat and Arun Raja and Benjamin Heinzerling and Chenglei Si and Elizabeth Salesky and Sabrina J. Mielke and Wilson Y. Lee and Abheesht Sharma and Andrea Santilli and Antoine Chaffin and Arnaud Stiegler and Debajyoti Datta and Eliza Szczechla and Gunjan Chhablani and Han Wang and Harshit Pandey and Hendrik Strobelt and Jason Alan Fries and Jos Rozen and Leo Gao and Lintang Sutawika and M Saiful Bari and Maged S. Al-Shaibani and Matteo Manica and Nihal V. Nayak and Ryan Teehan and Samuel Albanie and Sheng Shen and Srulik Ben-David and Stephen H. Bach and Taewoon Kim and Tali Bers and Thibault F{\'e}vry and Trishala Neeraj and Urmish Thakker and Vikas Raunak and Xiang Tang and Zheng-Xin Yong and Zhiqing Sun and Shaked Brody and Y Uri and Hadar Tojarieh and Adam Roberts and Hyung Won Chung and Jaesung Tae and Jason Phang and Ofir Press and Conglong Li and Deepak Narayanan and Hatim Bourfoune and Jared Casper and Jeff Rasley and Max Ryabinin and Mayank Mishra and Minjia Zhang and Mohammad Shoeybi and Myriam Peyrounette and Nicolas Patry and Nouamane Tazi and Omar Sanseviero and Patrick von Platen and Pierre Cornette and Pierre Franccois Lavall'ee and R{\'e}mi Lacroix and Samyam Rajbhandari and Sanchit Gandhi and Shaden Smith and St{\'e}phane Requena and Suraj Patil and Tim Dettmers and Ahmed Baruwa and Amanpreet Singh and Anastasia Cheveleva and Anne-Laure Ligozat and Arjun Subramonian and Aur'elie N'ev'eol and Charles Lovering and Daniel H Garrette and Deepak R. Tunuguntla and Ehud Reiter and Ekaterina Taktasheva and Ekaterina Voloshina and Eli Bogdanov and Genta Indra Winata and Hailey Schoelkopf and Jan-Christoph Kalo and Jekaterina Novikova and Jessica Zosa Forde and Xiangru Tang and Jungo Kasai and Ken Kawamura and Liam Hazan and Marine Carpuat and Miruna Clinciu and Najoung Kim and Newton Cheng and Oleg Serikov and Omer Antverg and Oskar van der Wal and Rui Zhang and Ruochen Zhang and Sebastian Gehrmann and Shachar Mirkin and S. Osher Pais and Tatiana Shavrina and Thomas Scialom and Tian Yun and Tomasz Limisiewicz and Verena Rieser and Vitaly Protasov and Vladislav Mikhailov and Yada Pruksachatkun and Yonatan Belinkov and Zachary Bamberger and Zdenˇek Kasner and Zdeněk Kasner and Amanda Pestana and Amir Feizpour and Ammar Khan and Amy Faranak and Ananda Santa Rosa Santos and Anthony Hevia and Antigona Unldreaj and Arash Aghagol and Arezoo Abdollahi and Aycha Tammour and Azadeh HajiHosseini and Bahareh Behroozi and Benjamin Ayoade Ajibade and Bharat Kumar Saxena and Carlos Mu{\~n}oz Ferrandis and Danish Contractor and David M. Lansky and Davis David and Douwe Kiela and Duong Anh Nguyen and Edward Tan and Emi Baylor and Ezinwanne Ozoani and Fatim Tahirah Mirza and Frankline Ononiwu and Habib Rezanejad and H.A. Jones and Indrani Bhattacharya and Irene Solaiman and Irina Sedenko and Isar Nejadgholi and Jan Passmore and Joshua Seltzer and Julio Bonis Sanz and Karen Fort and L{\'i}via Dutra and Mairon Samagaio and Maraim Elbadri and Margot Mieskes and Marissa Gerchick and Martha Akinlolu and Michael McKenna and Mike Qiu and Muhammed Ghauri and Mykola Burynok and Nafis Abrar and Nazneen Rajani and Nour Elkott and Nourhan Fahmy and Olanrewaju Samuel and Ran An and R. P. Kromann and Ryan Hao and Samira Alizadeh and Sarmad Shubber and Silas L. Wang and Sourav Roy and Sylvain Viguier and Thanh-Cong Le and Tobi Oyebade and Trieu Nguyen Hai Le and Yoyo Yang and Zach Nguyen and Abhinav Ramesh Kashyap and Alfredo Palasciano and Alison Callahan and Anima Shukla and Antonio Miranda-Escalada and Ayush Kumar Singh and Benjamin Beilharz and Bo Wang and Caio Matheus Fonseca de Brito and Chenxi Zhou and Chirag Jain and Chuxin Xu and Cl{\'e}mentine Fourrier and Daniel Le'on Perin'an and Daniel Molano and Dian Yu and Enrique Manjavacas and Fabio Barth and Florian Fuhrimann and Gabriel Altay and Giyaseddin Bayrak and Gully Burns and Helena U. Vrabec and Iman I.B. Bello and Isha Dash and Ji Soo Kang and John Giorgi and Jonas Golde and Jos{\'e} D. Posada and Karthi Sivaraman and Lokesh Bulchandani and Lu Liu and Luisa Shinzato and Madeleine Hahn de Bykhovetz and Maiko Takeuchi and Marc P{\`a}mies and Mar{\'i}a Andrea Castillo and Marianna Nezhurina and Mario Sanger and Matthias Samwald and Michael Cullan and Michael Weinberg and M Wolf and Mina Mihaljcic and Minna Liu and Moritz Freidank and Myungsun Kang and Natasha Seelam and Nathan Dahlberg and Nicholas Michio Broad and Nikolaus Muellner and Pascale Fung and Patricia Haller and Patrick Haller and Renata Eisenberg and Robert Martin and Rodrigo Canalli and Rosaline Su and Ruisi Su and Samuel Cahyawijaya and Samuele Garda and Shlok S Deshmukh and Shubhanshu Mishra and Sid Kiblawi and Simon Ott and Sinee Sang-aroonsiri and Srishti Kumar and Stefan Schweter and Sushil Pratap Bharati and Tanmay Laud and Th{\'e}o Gigant and Tomoya Kainuma and Wojciech Kusa and Yanis Labrak and Yashasvi Bajaj and Y. Venkatraman and Yifan Xu and Ying Xu and Yu Xu and Zhee Xao Tan and Zhongli Xie and Zifan Ye and Mathilde Bras and Younes Belkada and Thomas Wolf},
 journal = {ArXiv},
 title = {BLOOM: A 176B-Parameter Open-Access Multilingual Language Model},
 volume = {abs/2211.05100},
 year = {2022}
}

@article{schuurmans2023memory,
 author = {Dale Schuurmans},
 journal = {arXiv preprint arXiv: 2301.04589},
 pdf = {https://arxiv.org/pdf/2301.04589.pdf},
 title = {Memory Augmented Large Language Models are Computationally Universal},
 url = {https://arxiv.org/abs/2301.04589v1},
 year = {2023}
}

@inproceedings{segmentrecurrent,
 author = {Yinghan Long and
Sayeed Shafayet Chowdhury and
Kaushik Roy},
 booktitle = {{EMNLP} (Findings)},
 pages = {8325--8337},
 publisher = {Association for Computational Linguistics},
 title = {Segmented Recurrent Transformer: An Efficient Sequence-to-Sequence
Model},
 year = {2023}
}

@inproceedings{self-notes,
 author = {Lanchantin, Jack and Toshniwal, Shubham and Weston, Jason and szlam, arthur and Sukhbaatar, Sainbayar},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {11891-11911},
 pdf = {https://proceedings.neurips.cc/paper_files/paper/2023/file/274d0146144643ee2459a602123c60ff-Abstract-Conference.html},
 publisher = {Curran Associates, Inc.},
 title = {Learning to Reason and Memorize with Self-Notes},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/274d0146144643ee2459a602123c60ff-Paper-Conference.pdf},
 volume = {36},
 year = {2023}
}

@article{SemDeDup,
 author = {Amro Abbas and
Kushal Tirumala and
Daniel Simig and
Surya Ganguli and
Ari S. Morcos},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/journals/corr/abs-2303-09540.bib},
 doi = {10.48550/ARXIV.2303.09540},
 eprint = {2303.09540},
 eprinttype = {arXiv},
 journal = {CoRR},
 timestamp = {Tue, 21 Mar 2023 10:44:42 +0100},
 title = {SemDeDup: Data-efficient learning at web-scale through semantic deduplication},
 url = {https://doi.org/10.48550/arXiv.2303.09540},
 volume = {abs/2303.09540},
 year = {2023}
}

@article{setty2024improving,
 author = {Setty, Spurthi and Thakkar, Harsh and Lee, Alyssa and Chung, Eden and Vidra, Natan},
 journal = {arXiv preprint arXiv:2404.07221},
 title = {Improving retrieval for rag based question answering models on financial documents},
 year = {2024}
}

@article{shah2025flashattention,
 author = {Shah, Jay and Bikshandi, Ganesh and Zhang, Ying and Thakkar, Vijay and Ramani, Pradeep and Dao, Tri},
 journal = {Advances in Neural Information Processing Systems},
 pages = {68658--68685},
 title = {Flashattention-3: Fast and accurate attention with asynchrony and low-precision},
 volume = {37},
 year = {2025}
}

@article{shaham2022scrolls,
 author = {Shaham, Uri and Segal, Elad and Ivgi, Maor and Efrat, Avia and Yoran, Ori and Haviv, Adi and Gupta, Ankit and Xiong, Wenhan and Geva, Mor and Berant, Jonathan and others},
 journal = {arXiv preprint arXiv:2201.03533},
 title = {Scrolls: Standardized comparison over long language sequences},
 year = {2022}
}

@article{shaham2023zeroscrolls,
 author = {Shaham, Uri and Ivgi, Maor and Efrat, Avia and Berant, Jonathan and Levy, Omer},
 journal = {arXiv preprint arXiv:2305.14196},
 title = {Zeroscrolls: A zero-shot benchmark for long text understanding},
 year = {2023}
}

@misc{shandilya2024tacorltaskawareprompt,
 archiveprefix = {arXiv},
 author = {Shivam Shandilya and Menglin Xia and Supriyo Ghosh and Huiqiang Jiang and Jue Zhang and Qianhui Wu and Victor Rühle},
 eprint = {2409.13035},
 primaryclass = {cs.CL},
 title = {TACO-RL: Task Aware Prompt Compression Optimization with Reinforcement Learning},
 url = {https://arxiv.org/abs/2409.13035},
 year = {2024}
}

@article{shao2019long,
 author = {Shao, Zhihong and Huang, Minlie and Wen, Jiangtao and Xu, Wenfei and Zhu, Xiaoyan},
 journal = {arXiv preprint arXiv:1908.06605},
 title = {Long and diverse text generation with planning-based hierarchical variational model},
 year = {2019}
}

@article{shao2024long,
 author = {Shao, Bin and Yan, Jiawei},
 journal = {Nature Communications},
 number = {1},
 pages = {9392},
 publisher = {Nature Publishing Group UK London},
 title = {A long-context language model for deciphering and generating bacteriophage genomes},
 volume = {15},
 year = {2024}
}


@inproceedings{Sharma-2024-arxiv-LOCOVQA,
 author = {Aditya Sharma and
Michael Saxon and
William Yang Wang},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/conf/emnlp/SharmaSW24.bib},
 booktitle = {Findings of the Association for Computational Linguistics: {EMNLP}
2024, Miami, Florida, USA, November 12-16, 2024},
 editor = {Yaser Al{-}Onaizan and
Mohit Bansal and
Yun{-}Nung Chen},
 pages = {5429--5451},
 publisher = {Association for Computational Linguistics},
 timestamp = {Mon, 18 Nov 2024 09:05:59 +0100},
 title = {Losing Visual Needles in Image Haystacks: Vision Language Models are
Easily Distracted in Short and Long Contexts},
 url = {https://aclanthology.org/2024.findings-emnlp.312},
 year = {2024}
}

@inproceedings{sharma2019bigpatent,
 author = {Sharma, Eva and Li, Chen and Wang, Lu},
 booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
 pages = {2204--2213},
 title = {BIGPATENT: A Large-Scale Dataset for Abstractive and Coherent Summarization},
 year = {2019}
}

@article{shaw2018self,
 author = {Shaw, Peter and Uszkoreit, Jakob and Vaswani, Ashish},
 journal = {arXiv preprint arXiv:1803.02155},
 title = {Self-attention with relative position representations},
 year = {2018}
}

@misc{shazeer2019fasttransformerdecodingwritehead,
 archiveprefix = {arXiv},
 author = {Noam Shazeer},
 eprint = {1911.02150},
 primaryclass = {cs.NE},
 title = {Fast Transformer Decoding: One Write-Head is All You Need},
 url = {https://arxiv.org/abs/1911.02150},
 year = {2019}
}

@inproceedings{shen2021efficient,
 author = {Shen, Zhuoran and Zhang, Mingyuan and Zhao, Haiyu and Yi, Shuai and Li, Hongsheng},
 booktitle = {Proceedings of the IEEE/CVF winter conference on applications of computer vision},
 pages = {3531--3539},
 title = {Efficient attention: Attention with linear complexities},
 year = {2021}
}

@article{shen2022multi,
 author = {Shen, Zejiang and Lo, Kyle and Yu, Lauren and Dahlberg, Nathan and Schlanger, Margo and Downey, Doug},
 journal = {Advances in Neural Information Processing Systems},
 pages = {13158--13173},
 title = {Multi-lexsum: Real-world summaries of civil rights lawsuits at multiple granularities},
 volume = {35},
 year = {2022}
}

@article{shen2024efficient,
 author = {Shen, Haihao and Mellempudi, Naveen and He, Xin and Gao, Qun and Wang, Chang and Wang, Mengni},
 journal = {Proceedings of Machine Learning and Systems},
 pages = {483--498},
 title = {Efficient post-training quantization with fp8 formats},
 volume = {6},
 year = {2024}
}

@inproceedings{sheng2023flexgen,
 author = {Sheng, Ying and Zheng, Lianmin and Yuan, Binhang and Li, Zhuohan and Ryabinin, Max and Chen, Beidi and Liang, Percy and R{\'e}, Christopher and Stoica, Ion and Zhang, Ce},
 booktitle = {International Conference on Machine Learning},
 organization = {PMLR},
 pages = {31094--31116},
 title = {Flexgen: High-throughput generative inference of large language models with a single gpu},
 year = {2023}
}

@article{Shi2023InContextPL,
 author = {Weijia Shi and Sewon Min and Maria Lomeli and Chunting Zhou and Margaret Li and Victoria Lin and Noah A. Smith and Luke S. Zettlemoyer and Scott Yih and Mike Lewis},
 journal = {ArXiv},
 title = {In-Context Pretraining: Language Modeling Beyond Document Boundaries},
 volume = {abs/2310.10638},
 year = {2023}
}

@article{shi2024compressing,
 author = {Shi, Kaize and Sun, Xueyao and Li, Qing and Xu, Guandong},
 journal = {arXiv preprint arXiv:2405.03085},
 title = {Compressing Long Context for Enhancing RAG with AMR-based Concept Distillation},
 year = {2024}
}

@article{DBLP:journals/corr/abs-2405-03085,
  author       = {Kaize Shi and
                  Xueyao Sun and
                  Qing Li and
                  Guandong Xu},
  title        = {Compressing Long Context for Enhancing {RAG} with AMR-based Concept
                  Distillation},
  journal      = {CoRR},
  volume       = {abs/2405.03085},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2405.03085},
  doi          = {10.48550/ARXIV.2405.03085},
  eprinttype    = {arXiv},
  eprint       = {2405.03085},
  timestamp    = {Sun, 04 Aug 2024 19:45:43 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2405-03085.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{shoeybi2019megatron,
 author = {Shoeybi, Mohammad and Patwary, Mostofa and Puri, Raul and LeGresley, Patrick and Casper, Jared and Catanzaro, Bryan},
 journal = {arXiv preprint arXiv:1909.08053},
 title = {Megatron-lm: Training multi-billion parameter language models using model parallelism},
 year = {2019}
}

@inproceedings{Shrivastava2022RepositoryLevelPG,
  author       = {Disha Shrivastava and
                  Hugo Larochelle and
                  Daniel Tarlow},
  editor       = {Andreas Krause and
                  Emma Brunskill and
                  Kyunghyun Cho and
                  Barbara Engelhardt and
                  Sivan Sabato and
                  Jonathan Scarlett},
  title        = {Repository-Level Prompt Generation for Large Language Models of Code},
  booktitle    = {International Conference on Machine Learning, {ICML} 2023, 23-29 July
                  2023, Honolulu, Hawaii, {USA}},
  series       = {Proceedings of Machine Learning Research},
  volume       = {202},
  pages        = {31693--31715},
  publisher    = {{PMLR}},
  year         = {2023},
  url          = {https://proceedings.mlr.press/v202/shrivastava23a.html},
  timestamp    = {Mon, 28 Aug 2023 17:23:09 +0200},
  biburl       = {https://dblp.org/rec/conf/icml/ShrivastavaLT23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@article{shrivastava2023repofusion,
  author       = {Disha Shrivastava and
                  Denis Kocetkov and
                  Harm de Vries and
                  Dzmitry Bahdanau and
                  Torsten Scholak},
  title        = {RepoFusion: Training Code Models to Understand Your Repository},
  journal      = {CoRR},
  volume       = {abs/2306.10998},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2306.10998},
  doi          = {10.48550/ARXIV.2306.10998},
  eprinttype    = {arXiv},
  eprint       = {2306.10998},
  timestamp    = {Fri, 23 Jun 2023 15:19:11 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2306-10998.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{shrivastava2023repository,
 author = {Shrivastava, Disha and Larochelle, Hugo and Tarlow, Daniel},
 booktitle = {International Conference on Machine Learning},
 organization = {PMLR},
 pages = {31693--31715},
 title = {Repository-level prompt generation for large language models of code},
 year = {2023}
}

@misc{singhania2024lokilowrankkeysefficient,
 archiveprefix = {arXiv},
 author = {Prajwal Singhania and Siddharth Singh and Shwai He and Soheil Feizi and Abhinav Bhatele},
 eprint = {2406.02542},
 primaryclass = {cs.LG},
 title = {Loki: Low-rank Keys for Efficient Sparse Attention},
 url = {https://arxiv.org/abs/2406.02542},
 year = {2024}
}

@inproceedings{NEURIPS2024_1e027da6,
 author = {Singhania, Prajwal and Singh, Siddharth and He, Shwai and Feizi, Soheil and Bhatele, Abhinav},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {16692--16723},
 publisher = {Curran Associates, Inc.},
 title = {Loki: Low-rank Keys for Efficient Sparse Attention},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/1e027da6bec9ceb2ec37951ceeccae93-Paper-Conference.pdf},
 volume = {37},
 year = {2024}
}


@misc{slimpajama,
 author = {Soboleva, Daria and Al-Khateeb, Faisal and Myers, Robert and Steeves, Jacob R and Hestness, Joel and Dey, Nolan},
 month = {June},
 title = {{SlimPajama: A 627B token cleaned and deduplicated version of RedPajama}},
 url = {https://huggingface.co/datasets/cerebras/SlimPajama-627B},
 year = {2023}
}

@article{soh2024you,
 author = {Soh, Yun Joon and Huang, Hanxian and Tian, Yuandong and Zhao, Jishen},
 journal = {arXiv preprint arXiv:2409.13695},
 title = {You Only Use Reactive Attention Slice For Long Context Retrieval},
 year = {2024}
}

@article{sohafi2023dcars,
 author = {Sohafi-Bonab, Javad and Aghdam, Mehdi Hosseinzadeh and Majidzadeh, Kambiz},
 journal = {Applied Soft Computing},
 pages = {110416},
 publisher = {Elsevier},
 title = {DCARS: Deep context-aware recommendation system based on session latent context},
 volume = {143},
 year = {2023}
}

@article{song-2024-arxiv-milebench,
 author = {Dingjie Song and
Shunian Chen and
Guiming Hardy Chen and
Fei Yu and
Xiang Wan and
Benyou Wang},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/journals/corr/abs-2404-18532.bib},
 doi = {10.48550/ARXIV.2404.18532},
 eprint = {2404.18532},
 eprinttype = {arXiv},
 journal = {CoRR},
 timestamp = {Mon, 27 May 2024 14:55:45 +0200},
 title = {MileBench: Benchmarking MLLMs in Long Context},
 url = {https://doi.org/10.48550/arXiv.2404.18532},
 volume = {abs/2404.18532},
 year = {2024}
}

@misc{song2023zebraextendingcontextwindow,
 archiveprefix = {arXiv},
 author = {Kaiqiang Song and Xiaoyang Wang and Sangwoo Cho and Xiaoman Pan and Dong Yu},
 eprint = {2312.08618},
 primaryclass = {cs.CL},
 title = {Zebra: Extending Context Window with Layerwise Grouped Local-Global Attention},
 url = {https://arxiv.org/abs/2312.08618},
 year = {2023}
}

@article{song2024counting,
 author = {Song, Mingyang and Zheng, Mao and Luo, Xuan},
 journal = {Preprint},
 title = {Counting-stars: A multi-evidence, position-aware, and scalable benchmark for evaluating long-context large language models},
 year = {2024}
}

@article{spacking,
 author = {Konrad Staniszewski and Szymon Tworkowski and Yu Zhao and Sebastian Jaszczur and Henryk Michalewski and Lukasz Kuci'nski and Piotr Milo's},
 journal = {ArXiv},
 title = {Structured Packing in LLM Training Improves Long Context Utilization},
 volume = {abs/2312.17296},
 year = {2023}
}

@article{stallone2024scaling,
 author = {Stallone, Matt and Saxena, Vaibhav and Karlinsky, Leonid and McGinn, Bridget and Bula, Tim and Mishra, Mayank and Soria, Adriana Meza and Zhang, Gaoyuan and Prasad, Aditya and Shen, Yikang and others},
 journal = {arXiv preprint arXiv:2407.13739},
 title = {Scaling Granite Code Models to 128K Context},
 year = {2024}
}

@article{staniszewski2023structured,
 author = {Staniszewski, Konrad and Tworkowski, Szymon and Jaszczur, Sebastian and Zhao, Yu and Michalewski, Henryk and Kuci{\'n}ski, {\L}ukasz and Mi{\l}o{\'s}, Piotr},
 journal = {arXiv preprint arXiv:2312.17296},
 title = {Structured packing in llm training improves long context utilization},
 year = {2023}
}

@article{stelmakh2022asqa,
 author = {Stelmakh, Ivan and Luan, Yi and Dhingra, Bhuwan and Chang, Ming-Wei},
 journal = {arXiv preprint arXiv:2204.06092},
 title = {ASQA: Factoid questions meet long-form answers},
 year = {2022}
}

@article{su-kexuefm-2024-ropetie,
 author = {Su, Jianlin},
 bibsource = {dummy computer science bibliography, https://dummyurl.org},
 biburl = {https://dummyurl.org/rec/online/transformer.bib},
 eprint = {10040},
 eprinttype = {website},
 journal = {Online Resource},
 month = {Mar},
 timestamp = {Thu, 17 Oct 2024 12:28:14 +0200},
 title = {Transformer 17},
 url = {https://spaces.ac.cn/archives/10040},
 year = {2024}
}

@article{su2024roformer,
 author = {Su, Jianlin and Ahmed, Murtadha and Lu, Yu and Pan, Shengfeng and Bo, Wen and Liu, Yunfeng},
 journal = {Neurocomputing},
 pages = {127063},
 publisher = {Elsevier},
 title = {Roformer: Enhanced transformer with rotary position embedding},
 volume = {568},
 year = {2024}
}

@inproceedings{sun2021long,
 author = {Sun, Simeng and Krishna, Kalpesh and Mattarella-Micke, Andrew and Iyyer, Mohit},
 booktitle = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
 pages = {807--822},
 title = {Do Long-Range Language Models Actually Use Long-Range Context?},
 year = {2021}
}

@article{sun2022length,
 author = {Sun, Yutao and Dong, Li and Patra, Barun and Ma, Shuming and Huang, Shaohan and Benhaim, Alon and Chaudhary, Vishrav and Song, Xia and Wei, Furu},
 journal = {arXiv preprint arXiv:2212.10554},
 title = {A length-extrapolatable transformer},
 year = {2022}
}

@inproceedings{sun2023chatgpt,
 author = {Sun, Weiwei and Yan, Lingyong and Ma, Xinyu and Wang, Shuaiqiang and Ren, Pengjie and Chen, Zhumin and Yin, Dawei and Ren, Zhaochun},
 booktitle = {Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
 pages = {14918--14937},
 title = {Is ChatGPT Good at Search? Investigating Large Language Models as Re-Ranking Agents},
 year = {2023}
}

@inproceedings{DBLP:conf/emnlp/0001YMWRCYR23,
  author       = {Weiwei Sun and
                  Lingyong Yan and
                  Xinyu Ma and
                  Shuaiqiang Wang and
                  Pengjie Ren and
                  Zhumin Chen and
                  Dawei Yin and
                  Zhaochun Ren},
  editor       = {Houda Bouamor and
                  Juan Pino and
                  Kalika Bali},
  title        = {Is ChatGPT Good at Search? Investigating Large Language Models as
                  Re-Ranking Agents},
  booktitle    = {Proceedings of the 2023 Conference on Empirical Methods in Natural
                  Language Processing, {EMNLP} 2023, Singapore, December 6-10, 2023},
  pages        = {14918--14937},
  publisher    = {Association for Computational Linguistics},
  year         = {2023},
  url          = {https://doi.org/10.18653/v1/2023.emnlp-main.923},
  doi          = {10.18653/V1/2023.EMNLP-MAIN.923},
  timestamp    = {Mon, 03 Mar 2025 21:03:32 +0100},
  biburl       = {https://dblp.org/rec/conf/emnlp/0001YMWRCYR23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{sun2023retentivenetworksuccessortransformer,
 archiveprefix = {arXiv},
 author = {Yutao Sun and Li Dong and Shaohan Huang and Shuming Ma and Yuqing Xia and Jilong Xue and Jianyong Wang and Furu Wei},
 eprint = {2307.08621},
 primaryclass = {cs.CL},
 title = {Retentive Network: A Successor to Transformer for Large Language Models},
 url = {https://arxiv.org/abs/2307.08621},
 year = {2023}
}

@inproceedings{suntriforce,
  title={TriForce: Lossless Acceleration of Long Sequence Generation with Hierarchical Speculative Decoding},
  author={Sun, Hanshi and Chen, Zhuoming and Yang, Xinyu and Tian, Yuandong and Chen, Beidi},
  booktitle={First Conference on Language Modeling}
}

@article{Sun2024YouOC,
 author = {Yutao Sun and Li Dong and Yi Zhu and Shaohan Huang and Wenhui Wang and Shuming Ma and Quanlu Zhang and Jianyong Wang and Furu Wei},
 journal = {ArXiv},
 title = {You Only Cache Once: Decoder-Decoder Architectures for Language Models},
 url = {https://api.semanticscholar.org/CorpusID:269626143},
 volume = {abs/2405.05254},
 year = {2024}
}

@article{sun2025you,
 author = {Sun, Yutao and Dong, Li and Zhu, Yi and Huang, Shaohan and Wang, Wenhui and Ma, Shuming and Zhang, Quanlu and Wang, Jianyong and Wei, Furu},
 journal = {Advances in Neural Information Processing Systems},
 pages = {7339--7361},
 title = {You only cache once: Decoder-decoder architectures for language models},
 volume = {37},
 year = {2025}
}

@inproceedings{supergpqa,
 author = {M-A-P Team and Xinrun Du and Yifan Yao and Kaijing Ma and Bingli Wang and Tianyu Zheng and Kang Zhu and Minghao Liu and Yiming Liang and Xiaolong Jin and Zhen-Nan Wei and Chujie Zheng and Kaixin Deng and Shuyue Guo and Shian Jia and Sichao Jiang and Yiyan Liao and Rui Li and Qinrui Li and Sirun Li and Yizhi Li and Yunwen Li and Dehua Ma and Yuansheng Ni and Haoran Que and Qiyao Wang and Zhoufutu Wen and Si-Xuan Wu and Tianshun Xing and Ming Xu and Zhenzhu Yang and Ze Wang and Junting Zhou and Yu Bai and Xingyuan Bu and Chenglin Cai and Liang Chen and Yifan Chen and Chengtuo Cheng and Tianhao Cheng and Keyi Ding and Siming Huang and Yun-Jing Huang and Yaoru Li and Yizhe Li and Zhaoqun Li and Tianhao Liang and Chengdong Lin and Hongquan Lin and Yi-Hui Ma and Zhongyuan Peng and Zifan Peng and Qige Qi and Shi Qiu and Xingwei Qu and Yizhou Tan and Zili Wang and Chenqing Wang and Hao Wang and Yiya Wang and Yubo Wang and Jiajun Xu and Kexin Yang and Ru-Qing Yuan and Yuan-hao Yue and Tianyang Zhan and Chun Zhang and Jing-Yun Zhang and Xiyue Zhang and Xing Zhang and Yue Zhang and Yongchi Zhao and Xiangyu Zheng and Chenghua Zhong and Yang Gao and Zhoujun Li and Dayiheng Liu and Qian Liu and Tianyu Liu and Shiwen Ni and Junran Peng and Yujia Qin and Wenbo Su and Guoyin Wang and Shi Wang and Jian Yang and Min Yang and Meng Cao and Xiang Yue and Zhaoxiang Zhang and Wangchunshu Zhou and Jiaheng Liu and Qunshu Lin and Wenhao Huang and Ge Zhang},
 title = {SuperGPQA: Scaling LLM Evaluation across 285 Graduate Disciplines},
 year = {2025}
}

@article{t5,
 author = {Colin Raffel and
Noam Shazeer and
Adam Roberts and
Katherine Lee and
Sharan Narang and
Michael Matena and
Yanqi Zhou and
Wei Li and
Peter J. Liu},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/journals/jmlr/RaffelSRLNMZLL20.bib},
 journal = {J. Mach. Learn. Res.},
 pages = {140:1--140:67},
 timestamp = {Fri, 05 Feb 2021 15:43:41 +0100},
 title = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text
Transformer},
 url = {http://jmlr.org/papers/v21/20-074.html},
 volume = {21},
 year = {2020}
}

@article{talmor2018commonsenseqa,
 author = {Talmor, Alon and Herzig, Jonathan and Lourie, Nicholas and Berant, Jonathan},
 journal = {arXiv preprint arXiv:1811.00937},
 title = {Commonsenseqa: A question answering challenge targeting commonsense knowledge},
 year = {2018}
}

@article{tan2024proxyqa,
 author = {Tan, Haochen and Guo, Zhijiang and Shi, Zhan and Xu, Lu and Liu, Zhili and Feng, Yunlong and Li, Xiaoguang and Wang, Yasheng and Shang, Lifeng and Liu, Qun and others},
 journal = {arXiv preprint arXiv:2401.15042},
 title = {Proxyqa: An alternative framework for evaluating long-form text generation with large language models},
 year = {2024}
}

@inproceedings{tan2024towards,
 author = {Tan, Weihao and Ding, Ziluo and Zhang, Wentao and Li, Boyu and Zhou, Bohan and Yue, Junpeng and Xia, Haochong and Jiang, Jiechuan and Zheng, Longtao and Xu, Xinrun and others},
 booktitle = {ICLR 2024 Workshop on Large Language Model (LLM) Agents},
 title = {Towards general computer control: A multimodal agent for red dead redemption ii as a case study},
 year = {2024}
}

@article{DBLP:journals/corr/abs-2403-03186,
  author       = {Weihao Tan and
                  Ziluo Ding and
                  Wentao Zhang and
                  Boyu Li and
                  Bohan Zhou and
                  Junpeng Yue and
                  Haochong Xia and
                  Jiechuan Jiang and
                  Longtao Zheng and
                  Xinrun Xu and
                  Yifei Bi and
                  Pengjie Gu and
                  Xinrun Wang and
                  B{\"{o}}rje F. Karlsson and
                  Bo An and
                  Zongqing Lu},
  title        = {Towards General Computer Control: {A} Multimodal Agent for Red Dead
                  Redemption {II} as a Case Study},
  journal      = {CoRR},
  volume       = {abs/2403.03186},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2403.03186},
  doi          = {10.48550/ARXIV.2403.03186},
  eprinttype    = {arXiv},
  eprint       = {2403.03186},
  timestamp    = {Wed, 12 Feb 2025 09:05:08 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2403-03186.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{tang2024lciteeval,
 archiveprefix = {arXiv},
 author = {Zecheng Tang and Keyan Zhou and Juntao Li and Baibei Ji and Jianye Hou and Min Zhang},
 eprint = {2410.02115},
 primaryclass = {cs.CL},
 title = {L-CiteEval: Do Long-Context Models Truly Leverage Context for Responding?},
 year = {2024}
}

@article{tang2024logo,
 author = {Tang, Zecheng and Sun, Zechen and Li, Juntao and Zhu, Qiaoming and Zhang, Min},
 journal = {arXiv preprint arXiv:2410.18533},
 title = {LOGO--Long cOntext aliGnment via efficient preference Optimization},
 year = {2024}
}

@article{Tang2024LOGOL,
 author = {Zecheng Tang and Zechen Sun and Juntao Li and Qiaoming Zhu and Min Zhang},
 journal = {ArXiv},
 title = {LOGO - Long cOntext aliGnment via efficient preference Optimization},
 volume = {abs/2410.18533},
 year = {2024}
}

@misc{tang2024questqueryawaresparsityefficient,
 archiveprefix = {arXiv},
 author = {Jiaming Tang and Yilong Zhao and Kan Zhu and Guangxuan Xiao and Baris Kasikci and Song Han},
 eprint = {2406.10774},
 primaryclass = {cs.CL},
 title = {Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference},
 url = {https://arxiv.org/abs/2406.10774},
 year = {2024}
}


@InProceedings{pmlr-v235-tang24l,
  title = 	 {{QUEST}: Query-Aware Sparsity for Efficient Long-Context {LLM} Inference},
  author =       {Tang, Jiaming and Zhao, Yilong and Zhu, Kan and Xiao, Guangxuan and Kasikci, Baris and Han, Song},
  booktitle = 	 {Proceedings of the 41st International Conference on Machine Learning},
  pages = 	 {47901--47911},
  year = 	 {2024},
  editor = 	 {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},
  volume = 	 {235},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {21--27 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://raw.githubusercontent.com/mlresearch/v235/main/assets/tang24l/tang24l.pdf},
  url = 	 {https://proceedings.mlr.press/v235/tang24l.html},
  abstract = 	 {As the demand for long-context large language models (LLMs) increases, models with context windows of up to 128K or 1M tokens are becoming increasingly prevalent. However, long-context LLM inference is challenging since the inference speed decreases significantly as the sequence length grows. This slowdown is primarily caused by loading a large KV cache during self-attention. Previous works have shown that a small portion of critical tokens will dominate the attention outcomes. However, we observe the criticality of a token highly depends on the query. To this end, we propose Quest, a query-aware KV cache selection algorithm. Quest keeps track of the minimal and maximal Key values in KV cache pages and estimates the criticality of a given page using Query vectors. By only loading the Top-K critical KV cache pages for attention, Quest significantly speeds up self-attention without sacrificing accuracy. We show that Quest can achieve up to 2.23x self-attention speedup, which reduces inference latency by 7.03x while performing well on tasks with long dependencies with negligible accuracy loss. Code is available at https://github.com/mit-han-lab/quest.}
}


@article{tang2024razorattention,
  title={Razorattention: Efficient kv cache compression through retrieval heads},
  author={Tang, Hanlin and Lin, Yang and Lin, Jing and Han, Qingsen and Hong, Shikuan and Yao, Yiwu and Wang, Gongyi},
  journal={arXiv preprint arXiv:2407.15891},
  year={2024}
}
@inproceedings{
tang2025razorattention,
title={RazorAttention: Efficient {KV} Cache Compression Through Retrieval Heads},
author={Hanlin Tang and Yang Lin and Jing Lin and Qingsen Han and Danning Ke and Shikuan Hong and Yiwu Yao and Gongyi Wang},
booktitle={The Thirteenth International Conference on Learning Representations},
year={2025},
url={https://openreview.net/forum?id=tkiZQlL04w}
}
@article{tanner2015actions,
 author = {Tanner, Carmen and Br{\"u}gger, Adrian and van Schie, Susan and Lebherz, Carmen},
 journal = {Zeitschrift f{\"u}r Psychologie/Journal of Psychology},
 publisher = {Hogrefe Publishing},
 title = {Actions speak louder than words},
 year = {2015}
}

@misc{tay2020longrangearenabenchmark,
 archiveprefix = {arXiv},
 author = {Yi Tay and Mostafa Dehghani and Samira Abnar and Yikang Shen and Dara Bahri and Philip Pham and Jinfeng Rao and Liu Yang and Sebastian Ruder and Donald Metzler},
 eprint = {2011.04006},
 primaryclass = {cs.LG},
 title = {Long Range Arena: A Benchmark for Efficient Transformers},
 url = {https://arxiv.org/abs/2011.04006},
 year = {2020}
}


@article{team2024gemini,
  title={Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context},
  author={Team, Gemini and Georgiev, Petko and Lei, Ving Ian and Burnell, Ryan and Bai, Libin and Gulati, Anmol and Tanzer, Garrett and Vincent, Damien and Pan, Zhufeng and Wang, Shibo and others},
  journal={arXiv preprint arXiv:2403.05530},
  year={2024}
}

@article{team2024gemma,
 author = {Team, Gemma and Riviere, Morgane and Pathak, Shreya and Sessa, Pier Giuseppe and Hardin, Cassidy and Bhupatiraju, Surya and Hussenot, L{\'e}onard and Mesnard, Thomas and Shahriari, Bobak and Ram{\'e}, Alexandre and others},
 journal = {arXiv preprint arXiv:2408.00118},
 title = {Gemma 2: Improving open language models at a practical size},
 year = {2024}
}

@article{team2024jamba,
 author = {Team, Jamba and Lenz, Barak and Arazi, Alan and Bergman, Amir and Manevich, Avshalom and Peleg, Barak and Aviram, Ben and Almagor, Chen and Fridman, Clara and Padnos, Dan and others},
 journal = {arXiv preprint arXiv:2408.12570},
 title = {Jamba-1.5: Hybrid transformer-mamba models at scale},
 year = {2024}
}

@article{team2025kimi,
 author = {Team, Kimi and Du, Angang and Gao, Bofei and Xing, Bowei and Jiang, Changjiu and Chen, Cheng and Li, Cheng and Xiao, Chenjun and Du, Chenzhuang and Liao, Chonghua and others},
 journal = {arXiv preprint arXiv:2501.12599},
 title = {Kimi k1. 5: Scaling Reinforcement Learning with LLMs},
 year = {2025}
}

@article{tian2024distance,
 author = {Tian, Runchu and Li, Yanghao and Fu, Yuepeng and Deng, Siyang and Luo, Qinyu and Qian, Cheng and Wang, Shuo and Cong, Xin and Zhang, Zhong and Wu, Yesai and others},
 journal = {arXiv preprint arXiv:2410.14641},
 title = {Distance between Relevant Information Pieces Causes Bias in Long-Context LLMs},
 year = {2024}
}

@inproceedings{token-turing-machine,
 author = {Ryoo, Michael S. and Gopalakrishnan, Keerthana and Kahatapitiya, Kumara and Xiao, Ted and Rao, Kanishka and Stone, Austin and Lu, Yao and Ibarz, Julian and Arnab, Anurag},
 booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
 month = {June},
 pages = {19070-19081},
 title = {Token Turing Machines},
 year = {2023}
}

@article{toolformer,
 author = {Timo Schick and Jane Dwivedi-Yu and Roberto Dessì and Roberta Raileanu and M. Lomeli and Luke Zettlemoyer and Nicola Cancedda and Thomas Scialom},
 bibsource = {Semantic Scholar https://www.semanticscholar.org/paper/53d128ea815bcc0526856eb5a9c42cc977cb36a7},
 doi = {10.48550/arXiv.2302.04761},
 journal = {Neural Information Processing Systems},
 title = {Toolformer: Language Models Can Teach Themselves to Use Tools},
 year = {2023}
}

@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@inproceedings{towards-teachable-reasoning-systems,
 abstract = {Our goal is a teachable reasoning system for question-answering (QA), where a user can interact with faithful answer explanations, and correct its errors so that the system improves over time. Our approach is to augment a QA model with a dynamic memory of user feedback, containing user-supplied corrections toerroneous model beliefs that users identify during interaction. Retrievals from memory are used as additional context for QA, to help avoid previous mistakes in similar new situations - a novel application of memory-based continuous learning. With simulated feedback, we find that our system (called TeachMe) continually improves with time, and without model retraining, requiring feedback on only 25{\%} of training examples to reach within 1{\%} of the upper-bound (feedback on all examples). Similarly, in experiments with real users, we observe a similar trend, with performance improving by over 15{\%} on a hidden test set after teaching. This suggests new opportunities for using frozen language models in an interactive setting where users can inspect, debug, and correct the model{'}s beliefs, leading to improved system{'}s performance over time.},
 address = {Abu Dhabi, United Arab Emirates},
 author = {Dalvi Mishra, Bhavana and Tafjord, Oyvind and Clark, Peter},
 booktitle = {Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
 doi = {10.18653/v1/2022.emnlp-main.644},
 editor = {Goldberg, Yoav and Kozareva, Zornitsa and Zhang, Yue},
 month = {dec},
 pages = {9465-9480},
 pdf = {https://aclanthology.org/2022.emnlp-main.644.pdf},
 publisher = {Association for Computational Linguistics},
 title = {Towards Teachable Reasoning Systems: Using a Dynamic Memory of User Feedback for Continual System Improvement},
 url = {https://aclanthology.org/2022.emnlp-main.644},
 year = {2022}
}

@inproceedings{trams,
 author = {Haofei Yu and
Cunxiang Wang and
Yue Zhang and
Wei Bi},
 booktitle = {{EMNLP} (Findings)},
 pages = {4966--4972},
 publisher = {Association for Computational Linguistics},
 title = {{TRAMS:} Training-free Memory Selection for Long-range Language Modeling},
 year = {2023}
}

 
@inproceedings{TransformerXL,
  author       = {Zihang Dai and
                  Zhilin Yang and
                  Yiming Yang and
                  Jaime G. Carbonell and
                  Quoc Viet Le and
                  Ruslan Salakhutdinov},
  title        = {Transformer-XL: Attentive Language Models beyond a Fixed-Length Context},
  booktitle    = {{ACL} {(1)}},
  pages        = {2978--2988},
  publisher    = {Association for Computational Linguistics},
  year         = {2019}
}

@article{trivedi2022musique,
 author = {Trivedi, Harsh and Balasubramanian, Niranjan and Khot, Tushar and Sabharwal, Ashish},
 journal = {Transactions of the Association for Computational Linguistics},
 pages = {539--554},
 title = {MuSiQue: Multi-hop Questions via Single-hop Question Composition},
 volume = {10},
 year = {2022}
}

@article{tseng2024two,
 author = {Tseng, Yu-Min and Huang, Yu-Chao and Hsiao, Teng-Yun and Hsu, Yu-Ching and Foo, Jia-Yin and Huang, Chao-Wei and Chen, Yun-Nung},
 journal = {arXiv preprint arXiv:2406.01171},
 title = {Two tales of persona in llms: A survey of role-playing and personalization},
 year = {2024}
}

@inproceedings{
liu2024roleagent,
title={RoleAgent: Building, Interacting, and Benchmarking High-quality Role-Playing Agents from Scripts},
author={Jiaheng Liu and Zehao Ni and Haoran Que and Tao Sun and Noah Wang and Jian Yang and JiakaiWang and Hongcheng Guo and Z.Y. Peng and Ge Zhang and Jiayi Tian and Xingyuan Bu and Ke Xu and Wenge Rong and Junran Peng and Zhaoxiang Zhang},
booktitle={The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
year={2024},
url={https://openreview.net/forum?id=hORTHzt2cE}
}

@inproceedings{
ma2023query,
title={Query Rewriting in Retrieval-Augmented Large Language Models},
author={Xinbei Ma and Yeyun Gong and Pengcheng He and hai zhao and Nan Duan},
booktitle={The 2023 Conference on Empirical Methods in Natural Language Processing},
year={2023},
url={https://openreview.net/forum?id=gXq1cwkUZc}
}

@article{hyde,
  title   = {Precise Zero-Shot Dense Retrieval without Relevance Labels},
  author  = {Luyu Gao and Xueguang Ma and Jimmy Lin and Jamie Callan},
  year    = {2022},
  journal = {arXiv preprint arXiv: 2212.10496}
}

@article{wang2023query2doc0,
  title     = {Query2doc: Query Expansion with Large Language Models},
  author    = {Liang Wang and Nan Yang and Furu Wei},
  journal   = {Conference on Empirical Methods in Natural Language Processing},
  year      = {2023},
  doi       = {10.48550/arXiv.2303.07678},
  bibSource = {Semantic Scholar https://www.semanticscholar.org/paper/ccc772d88c231275f24c4fac9b28bbe0942e1107}
}

@article{warner2024smarter0,
  title   = {Smarter, Better, Faster, Longer: A Modern Bidirectional Encoder for Fast, Memory Efficient, and Long Context Finetuning and Inference},
  author  = {Benjamin Warner and Antoine Chaffin and Benjamin Clavié and Orion Weller and Oskar Hallström and Said Taghadouini and Alexis Gallagher and Raja Biswas and Faisal Ladhak and Tom Aarsen and Nathan Cooper and Griffin Adams and Jeremy Howard and Iacopo Poli},
  year    = {2024},
  journal = {arXiv preprint arXiv: 2412.13663},
  url     = {https://arxiv.org/abs/2412.13663},
  pdf     = {https://arxiv.org/pdf/2412.13663.pdf}
}
@misc{vannguyen2024taipanefficientexpressivestate,
 archiveprefix = {arXiv},
 author = {Chien Van Nguyen and Huy Huu Nguyen and Thang M. Pham and Ruiyi Zhang and Hanieh Deilamsalehy and Puneet Mathur and Ryan A. Rossi and Trung Bui and Viet Dac Lai and Franck Dernoncourt and Thien Huu Nguyen},
 eprint = {2410.18572},
 primaryclass = {cs.CL},
 title = {Taipan: Efficient and Expressive State Space Language Models with Selective Attention},
 url = {https://arxiv.org/abs/2410.18572},
 year = {2024}
}

@article{vasu-2024-arxiv-fastvlm,
 author = {Pavan Kumar Anasosalu Vasu and
Fartash Faghri and
Chun-Liang Li and
Cem Koc and
Nate True and
Albert Antony and
Gokul Santhanam and
James Gabriel and
Peter Grasch and
Oncel Tuzel},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/journals/corr/abs-2412.13303.bib},
 doi = {10.48550/ARXIV.2412.13303},
 eprint = {2412.13303},
 eprinttype = {arXiv},
 journal = {CoRR},
 timestamp = {Thu, 17 Oct 2024 12:28:14 +0200},
 title = {FastVLM: Efficient Vision Encoding for Vision Language Models},
 url = {https://doi.org/10.48550/arXiv.2412.13303},
 volume = {abs/2412.13303},
 year = {2024}
}

@inproceedings{vaswani2017attention,
  author       = {Ashish Vaswani and
                  Noam Shazeer and
                  Niki Parmar and
                  Jakob Uszkoreit and
                  Llion Jones and
                  Aidan N. Gomez and
                  Lukasz Kaiser and
                  Illia Polosukhin},
  title        = {Attention is All you Need},
  booktitle    = {{NIPS}},
  pages        = {5998--6008},
  year         = {2017}
}

@inproceedings{vaswani2023attentionneed,
  author       = {Ashish Vaswani and
                  Noam Shazeer and
                  Niki Parmar and
                  Jakob Uszkoreit and
                  Llion Jones and
                  Aidan N. Gomez and
                  Lukasz Kaiser and
                  Illia Polosukhin},
  title        = {Attention is All you Need},
  booktitle    = {{NIPS}},
  pages        = {5998--6008},
  year         = {2017}
}

@article{venkatraman2024collabstory,
 author = {Venkatraman, Saranya and Tripto, Nafis Irtiza and Lee, Dongwon},
 journal = {arXiv preprint arXiv:2406.12665},
 title = {CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis},
 year = {2024}
}

@article{vodrahalli2024michelangelo,
 author = {Vodrahalli, Kiran and Ontanon, Santiago and Tripuraneni, Nilesh and Xu, Kelvin and Jain, Sanil and Shivanna, Rakesh and Hui, Jeffrey and Dikkala, Nishanth and Kazemi, Mehran and Fatemi, Bahare and others},
 journal = {arXiv preprint arXiv:2409.12640},
 title = {Michelangelo: Long context evaluations beyond haystacks via latent structure queries},
 year = {2024}
}

@article{voelker2018improving,
  author       = {Aaron R. Voelker and
                  Chris Eliasmith},
  title        = {Improving Spiking Dynamical Networks: Accurate Delays, Higher-Order
                  Synapses, and Time Cells},
  journal      = {Neural Comput.},
  volume       = {30},
  number       = {3},
  year         = {2018}
}
@inproceedings{voita-etal-2024-neurons_func,
 address = {Bangkok, Thailand},
 author = {Voita, Elena  and
Ferrando, Javier  and
Nalmpantis, Christoforos},
 booktitle = {Findings of the Association for Computational Linguistics: ACL 2024},
 doi = {10.18653/v1/2024.findings-acl.75},
 editor = {Ku, Lun-Wei  and
Martins, Andre  and
Srikumar, Vivek},
 month = {August},
 pages = {1288--1301},
 publisher = {Association for Computational Linguistics},
 title = {Neurons in Large Language Models: Dead, N-gram, Positional},
 url = {https://aclanthology.org/2024.findings-acl.75/},
 year = {2024}
}

@article{voyager,
 author = {Guanzhi Wang and Yuqi Xie and Yunfan Jiang and Ajay Mandlekar and Chaowei Xiao and Yuke Zhu and Linxi Fan and Anima Anandkumar},
 journal = {arXiv preprint arXiv: 2305.16291},
 title = {Voyager: An Open-Ended Embodied Agent with Large Language Models},
 year = {2023}
}

@article{waleffe2024empirical,
 author = {Waleffe, Roger and Byeon, Wonmin and Riach, Duncan and Norick, Brandon and Korthikanti, Vijay and Dao, Tri and Gu, Albert and Hatamizadeh, Ali and Singh, Sudhakar and Narayanan, Deepak and others},
 journal = {arXiv preprint arXiv:2406.07887},
 title = {An Empirical Study of Mamba-based Language Models},
 year = {2024}
}

@misc{waleffe2024empiricalstudymambabasedlanguage,
 archiveprefix = {arXiv},
 author = {Roger Waleffe and Wonmin Byeon and Duncan Riach and Brandon Norick and Vijay Korthikanti and Tri Dao and Albert Gu and Ali Hatamizadeh and Sudhakar Singh and Deepak Narayanan and Garvit Kulshreshtha and Vartika Singh and Jared Casper and Jan Kautz and Mohammad Shoeybi and Bryan Catanzaro},
 eprint = {2406.07887},
 primaryclass = {cs.LG},
 title = {An Empirical Study of Mamba-based Language Models},
 url = {https://arxiv.org/abs/2406.07887},
 year = {2024}
}

@article{wang-2024-arxiv-LongLLaVA,
 author = {Xidong Wang and
Dingjie Song and
Shunian Chen and
Chen Zhang and
Benyou Wang},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/journals/corr/abs-2409-02889.bib},
 doi = {10.48550/ARXIV.2409.02889},
 eprint = {2409.02889},
 eprinttype = {arXiv},
 journal = {CoRR},
 timestamp = {Fri, 11 Oct 2024 07:31:53 +0200},
 title = {LongLLaVA: Scaling Multi-modal LLMs to 1000 Images Efficiently via
Hybrid Architecture},
 url = {https://doi.org/10.48550/arXiv.2409.02889},
 volume = {abs/2409.02889},
 year = {2024}
}

@article{wang-2024-arxiv-MMNeedle,
 author = {Hengyi Wang and
Haizhou Shi and
Shiwei Tan and
Weiyi Qin and
Wenyuan Wang and
Tunyu Zhang and
Akshay Nambi and
Tanuja Ganu and
Hao Wang},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/journals/corr/abs-2406-11230.bib},
 doi = {10.48550/ARXIV.2406.11230},
 eprint = {2406.11230},
 eprinttype = {arXiv},
 journal = {CoRR},
 timestamp = {Fri, 15 Nov 2024 10:53:31 +0100},
 title = {Multimodal Needle in a Haystack: Benchmarking Long-Context Capability
of Multimodal Large Language Models},
 url = {https://doi.org/10.48550/arXiv.2406.11230},
 volume = {abs/2406.11230},
 year = {2024}
}

@article{wang-2024-arxiv-Qwen2-VL,
 author = {Peng Wang and
Shuai Bai and
Sinan Tan and
Shijie Wang and
Zhihao Fan and
Jinze Bai and
Keqin Chen and
Xuejing Liu and
Jialin Wang and
Wenbin Ge and
Yang Fan and
Kai Dang and
Mengfei Du and
Xuancheng Ren and
Rui Men and
Dayiheng Liu and
Chang Zhou and
Jingren Zhou and
Junyang Lin},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/journals/corr/abs-2409-12191.bib},
 doi = {10.48550/ARXIV.2409.12191},
 eprint = {2409.12191},
 eprinttype = {arXiv},
 journal = {CoRR},
 timestamp = {Thu, 17 Oct 2024 12:28:14 +0200},
 title = {Qwen2-VL: Enhancing Vision-Language Model's Perception of the
World at Any Resolution},
 url = {https://doi.org/10.48550/arXiv.2409.12191},
 volume = {abs/2409.12191},
 year = {2024}
}

@article{wang-2024-arxiv-videotree,
 author = {Ziyang Wang and
Shoubin Yu and
Elias Stengel{-}Eskin and
Jaehong Yoon and
Feng Cheng and
Gedas Bertasius and
Mohit Bansal},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/journals/corr/abs-2405-19209.bib},
 doi = {10.48550/ARXIV.2405.19209},
 eprint = {2405.19209},
 eprinttype = {arXiv},
 journal = {CoRR},
 timestamp = {Fri, 21 Jun 2024 22:39:20 +0200},
 title = {VideoTree: Adaptive Tree-based Video Representation for {LLM} Reasoning
on Long Videos},
 url = {https://doi.org/10.48550/arXiv.2405.19209},
 volume = {abs/2405.19209},
 year = {2024}
}

@article{wang2019encoding,
 author = {Wang, Benyou and Zhao, Donghao and Lioma, Christina and Li, Qiuchi and Zhang, Peng and Simonsen, Jakob Grue},
 journal = {arXiv preprint arXiv:1912.12333},
 title = {Encoding word order in complex embeddings},
 year = {2019}
}

@misc{wang2020linformerselfattentionlinearcomplexity,
 archiveprefix = {arXiv},
 author = {Sinong Wang and Belinda Z. Li and Madian Khabsa and Han Fang and Hao Ma},
 eprint = {2006.04768},
 primaryclass = {cs.LG},
 title = {Linformer: Self-Attention with Linear Complexity},
 url = {https://arxiv.org/abs/2006.04768},
 year = {2020}
}

@inproceedings{wang2022squality,
 author = {Wang, Alex and Pang, Richard Yuanzhe and Chen, Angelica and Phang, Jason and Bowman, Samuel},
 booktitle = {Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
 pages = {1139--1156},
 title = {SQuALITY: Building a Long-Document Summarization Dataset the Hard Way},
 year = {2022}
}

@article{wang2023document,
 author = {Wang, Longyue and Lyu, Chenyang and Ji, Tianbo and Zhang, Zhirui and Yu, Dian and Shi, Shuming and Tu, Zhaopeng},
 journal = {arXiv preprint arXiv:2304.02210},
 title = {Document-level machine translation with large language models},
 year = {2023}
}

@inproceedings{DBLP:conf/emnlp/WangLJZY0T23,
  author       = {Longyue Wang and
                  Chenyang Lyu and
                  Tianbo Ji and
                  Zhirui Zhang and
                  Dian Yu and
                  Shuming Shi and
                  Zhaopeng Tu},
  editor       = {Houda Bouamor and
                  Juan Pino and
                  Kalika Bali},
  title        = {Document-Level Machine Translation with Large Language Models},
  booktitle    = {Proceedings of the 2023 Conference on Empirical Methods in Natural
                  Language Processing, {EMNLP} 2023, Singapore, December 6-10, 2023},
  pages        = {16646--16661},
  publisher    = {Association for Computational Linguistics},
  year         = {2023},
  url          = {https://doi.org/10.18653/v1/2023.emnlp-main.1036},
  doi          = {10.18653/V1/2023.EMNLP-MAIN.1036},
  timestamp    = {Sun, 06 Oct 2024 21:00:53 +0200},
  biburl       = {https://dblp.org/rec/conf/emnlp/WangLJZY0T23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{wang2023improving,
 author = {Wang, Liang and Yang, Nan and Huang, Xiaolong and Yang, Linjun and Majumder, Rangan and Wei, Furu},
 journal = {arXiv preprint arXiv:2401.00368},
 title = {Improving text embeddings with large language models},
 year = {2023}
}

@inproceedings{DBLP:conf/acl/WangYHYMW24,
  author       = {Liang Wang and
                  Nan Yang and
                  Xiaolong Huang and
                  Linjun Yang and
                  Rangan Majumder and
                  Furu Wei},
  editor       = {Lun{-}Wei Ku and
                  Andre Martins and
                  Vivek Srikumar},
  title        = {Improving Text Embeddings with Large Language Models},
  booktitle    = {Proceedings of the 62nd Annual Meeting of the Association for Computational
                  Linguistics (Volume 1: Long Papers), {ACL} 2024, Bangkok, Thailand,
                  August 11-16, 2024},
  pages        = {11897--11916},
  publisher    = {Association for Computational Linguistics},
  year         = {2024},
  url          = {https://doi.org/10.18653/v1/2024.acl-long.642},
  doi          = {10.18653/V1/2024.ACL-LONG.642},
  timestamp    = {Tue, 24 Sep 2024 10:55:48 +0200},
  biburl       = {https://dblp.org/rec/conf/acl/WangYHYMW24.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{wang2023rolellm,
 author = {Zekun Moore Wang and Zhongyuan Peng and Haoran Que and Jiaheng Liu and Wangchunshu Zhou and Yuhan Wu and Hongcheng Guo and Ruitong Gan and Zehao Ni and Man Zhang and Zhaoxiang Zhang and Wanli Ouyang and Ke Xu and Wenhu Chen and Jie Fu and Junran Peng},
 journal = {arXiv preprint arXiv: 2310.00746},
 title = {RoleLLM: Benchmarking, Eliciting, and Enhancing Role-Playing Abilities of Large Language Models},
 year = {2023}
}

@misc{wang2024adaleval,
 archiveprefix = {arXiv},
 author = {Chonghua Wang and Haodong Duan and Songyang Zhang and Dahua Lin and Kai Chen},
 eprint = {2404.06480},
 primaryclass = {cs.CL},
 title = {Ada-LEval: Evaluating long-context LLMs with length-adaptable benchmarks},
 year = {2024}
}


@inproceedings{wang2024benchmarking,
  author       = {Longyue Wang and
                  Zefeng Du and
                  Wenxiang Jiao and
                  Chenyang Lyu and
                  Jianhui Pang and
                  Leyang Cui and
                  Kaiqiang Song and
                  Derek F. Wong and
                  Shuming Shi and
                  Zhaopeng Tu},
  editor       = {Lun{-}Wei Ku and
                  Andre Martins and
                  Vivek Srikumar},
  title        = {Benchmarking and Improving Long-Text Translation with Large Language
                  Models},
  booktitle    = {Findings of the Association for Computational Linguistics, {ACL} 2024,
                  Bangkok, Thailand and virtual meeting, August 11-16, 2024},
  pages        = {7175--7187},
  publisher    = {Association for Computational Linguistics},
  year         = {2024},
  url          = {https://doi.org/10.18653/v1/2024.findings-acl.428},
  doi          = {10.18653/V1/2024.FINDINGS-ACL.428},
  timestamp    = {Tue, 24 Sep 2024 10:55:36 +0200},
  biburl       = {https://dblp.org/rec/conf/acl/WangDJLPCSW0T24.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{wang2024domino,
 author = {Wang, Guanhua and Zhang, Chengming and Shen, Zheyu and Li, Ang and Ruwase, Olatunji},
 journal = {arXiv preprint arXiv:2409.15241},
 title = {Domino: Eliminating Communication in LLM Training via Generic Tensor Slicing and Overlapping},
 year = {2024}
}

@article{wang2024drt,
 author = {Wang, Jiaan and Meng, Fandong and Liang, Yunlong and Zhou, Jie},
 journal = {arXiv preprint arXiv:2412.17498},
 title = {DRT-o1: Optimized Deep Reasoning Translation via Long Chain-of-Thought},
 year = {2024}
}

@inproceedings{wang2024large,
 author = {Wang, Liang and Yang, Nan and Huang, Xiaolong and Yang, Linjun and Majumder, Rangan and Wei, Furu},
 booktitle = {ACM SIGIR Forum},
 number = {2},
 organization = {ACM New York, NY, USA},
 pages = {1--16},
 title = {Large search model: Redefining search stack in the era of llms},
 volume = {57},
 year = {2024}
}

@inproceedings{wang2024leave,
 author = {Wang, Minzheng and Chen, Longze and Cheng, Fu and Liao, Shengyi and Zhang, Xinghua and Wu, Bingli and Yu, Haiyang and Xu, Nan and Zhang, Lei and Luo, Run and others},
 booktitle = {Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
 pages = {5627--5646},
 title = {Leave no document behind: Benchmarking long-context llms with extended multi-doc qa},
 year = {2024}
}


@inproceedings{wang2024limitssurveytechniquesextend,
  author       = {Xindi Wang and
                  Mahsa Salmani and
                  Parsa Omidi and
                  Xiangyu Ren and
                  Mehdi Rezagholizadeh and
                  Armaghan Eshaghi},
  title        = {Beyond the Limits: {A} Survey of Techniques to Extend the Context
                  Length in Large Language Models},
  booktitle    = {Proceedings of the Thirty-Third International Joint Conference on
                  Artificial Intelligence, {IJCAI} 2024, Jeju, South Korea, August 3-9,
                  2024},
  pages        = {8299--8307},
  publisher    = {ijcai.org},
  year         = {2024},
  url          = {https://www.ijcai.org/proceedings/2024/917},
  timestamp    = {Fri, 18 Oct 2024 20:55:27 +0200},
  biburl       = {https://dblp.org/rec/conf/ijcai/WangSORRE24.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{wang2024mathhay,
 author = {Wang, Lei and Dong, Shan and Xu, Yuhui and Dong, Hanze and Wang, Yalu and Saha, Amrita and Lim, Ee-Peng and Xiong, Caiming and Sahoo, Doyen},
 journal = {arXiv preprint arXiv:2410.04698},
 title = {Mathhay: An automated benchmark for long-context mathematical reasoning in llms},
 year = {2024}
}

@article{wang2024openhands,
 author = {Wang, Xingyao and Li, Boxuan and Song, Yufan and Xu, Frank F and Tang, Xiangru and Zhuge, Mingchen and Pan, Jiayi and Song, Yueqi and Li, Bowen and Singh, Jaskirat and others},
 journal = {arXiv preprint arXiv:2407.16741},
 title = {Openhands: An open platform for ai software developers as generalist agents},
 year = {2024}
}

@inproceedings{wang2024openhands_,
  title={Openhands: An open platform for ai software developers as generalist agents},
  author={Wang, Xingyao and Li, Boxuan and Song, Yufan and Xu, Frank F and Tang, Xiangru and Zhuge, Mingchen and Pan, Jiayi and Song, Yueqi and Li, Bowen and Singh, Jaskirat and others},
  booktitle={The Thirteenth International Conference on Learning Representations},
  year={2024}
}

@article{wang2024precision,
 author = {Wang, Haonan and Liu, Qian and Du, Chao and Zhu, Tongyao and Du, Cunxiao and Kawaguchi, Kenji and Pang, Tianyu},
 journal = {arXiv preprint arXiv:2411.13476},
 title = {When Precision Meets Position: BFloat16 Breaks Down RoPE in Long-Context Training},
 year = {2024}
}

@article{wang2024repotransbench,
 author = {Wang, Yanli and Wang, Yanlin and Wang, Suiquan and Guo, Daya and Chen, Jiachi and Grundy, John and Liu, Xilin and Ma, Yuchi and Mao, Mingzhi and Zhang, Hongyu and others},
 journal = {arXiv preprint arXiv:2412.17744},
 title = {RepoTransBench: A Real-World Benchmark for Repository-Level Code Translation},
 year = {2024}
}

@article{wang2024resonance,
 author = {Wang, Suyuchen and Kobyzev, Ivan and Lu, Peng and Rezagholizadeh, Mehdi and Liu, Bang},
 journal = {arXiv preprint arXiv:2403.00071},
 title = {Resonance rope: Improving context length generalization of large language models},
 year = {2024}
}

@article{wang2024study,
 author = {Wang, Ting and Yang, Chuan and Zou, Maoyang and Liang, Jiaying and Xiang, Dong and Yang, Wenjie and Wang, Hongyang and Li, Jia},
 journal = {Scientific Reports},
 number = {1},
 pages = {10140},
 publisher = {Nature Publishing Group UK London},
 title = {A study of extractive summarization of long documents incorporating local topic and hierarchical information},
 volume = {14},
 year = {2024}
}


@article{wang2024teaching,
 author = {Wang, Chong and Zhang, Jian and Feng, Yebo and Li, Tianlin and Sun, Weisong and Liu, Yang and Peng, Xin},
 journal = {arXiv preprint arXiv:2401.06391},
 title = {Teaching Code LLMs to Use Autocompletion Tools in Repository-Level Code Generation},
 year = {2024}
}

@misc{wang2024weaver,
 archiveprefix = {arXiv},
 author = {Tiannan Wang and Jiamin Chen and Qingrui Jia and Shuai Wang and Ruoyu Fang and Huilin Wang and Zhaowei Gao and Chunzhao Xie and Chuou Xu and Jihong Dai and Yibin Liu and Jialong Wu and Shengwei Ding and Long Li and Zhiwei Huang and Xinle Deng and Teng Yu and Gangan Ma and Han Xiao and Zixin Chen and Danjun Xiang and Yunxia Wang and Yuanyuan Zhu and Yi Xiao and Jing Wang and Yiru Wang and Siran Ding and Jiayang Huang and Jiayi Xu and Yilihamu Tayier and Zhenyu Hu and Yuan Gao and Chengfeng Zheng and Yueshu Ye and Yihang Li and Lei Wan and Xinyue Jiang and Yujie Wang and Siyu Cheng and Zhule Song and Xiangru Tang and Xiaohua Xu and Ningyu Zhang and Huajun Chen and Yuchen Eleanor Jiang and Wangchunshu Zhou},
 eprint = {2401.17268},
 primaryclass = {cs.CL},
 title = {Weaver: Foundation Models for Creative Writing},
 url = {https://arxiv.org/abs/2401.17268},
 year = {2024}
}

@article{wei2024long,
 author = {Wei, Jerry and Yang, Chengrun and Song, Xinying and Lu, Yifeng and Hu, Nathan and Tran, Dustin and Peng, Daiyi and Liu, Ruibo and Huang, Da and Du, Cosmo and others},
 journal = {arXiv preprint arXiv:2403.18802},
 title = {Long-form factuality in large language models},
 year = {2024}
}

@article{weston2015towards,
 author = {Weston, Jason and Bordes, Antoine and Chopra, Sumit and Rush, Alexander M and Van Merri{\"e}nboer, Bart and Joulin, Armand and Mikolov, Tomas},
 journal = {arXiv preprint arXiv:1502.05698},
 title = {Towards ai-complete question answering: A set of prerequisite toy tasks},
 year = {2015}
}

@misc{wingate2022promptcompressioncontrastiveconditioning,
 archiveprefix = {arXiv},
 author = {David Wingate and Mohammad Shoeybi and Taylor Sorensen},
 eprint = {2210.03162},
 primaryclass = {cs.CL},
 title = {Prompt Compression and Contrastive Conditioning for Controllability and Toxicity Reduction in Language Models},
 url = {https://arxiv.org/abs/2210.03162},
 year = {2022}
}

@inproceedings{wingate-etal-2022-prompt,
    title = "Prompt Compression and Contrastive Conditioning for Controllability and Toxicity Reduction in Language Models",
    author = "Wingate, David  and
      Shoeybi, Mohammad  and
      Sorensen, Taylor",
    editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2022",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-emnlp.412/",
    doi = "10.18653/v1/2022.findings-emnlp.412",
    pages = "5621--5634",
    abstract = "We explore the idea of compressing the prompts used to condition language models, and show that compressed prompts can retain a substantive amount of information about the original prompt. For severely compressed prompts, while fine-grained information is lost, abstract information and general sentiments can be retained with surprisingly few parameters, which can be useful in the context of decode-time algorithms for controllability and toxicity reduction. We find that some complex prompts can be effectively compressed into a single token to guide generation. We also show that compressed prompts are largely compositional, and can be constructed such that they can be used to control independent aspects of generated text."
}

@article{wiseman2017challenges,
 author = {Wiseman, Sam and Shieber, Stuart M and Rush, Alexander M},
 journal = {arXiv preprint arXiv:1707.08052},
 title = {Challenges in data-to-document generation},
 year = {2017}
}

@article{wu-2024-arxiv-longvideobench,
 author = {Haoning Wu and
Dongxu Li and
Bei Chen and
Junnan Li},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/journals/corr/abs-2407-15754.bib},
 doi = {10.48550/ARXIV.2407.15754},
 eprint = {2407.15754},
 eprinttype = {arXiv},
 journal = {CoRR},
 timestamp = {Mon, 19 Aug 2024 21:11:16 +0200},
 title = {LongVideoBench: {A} Benchmark for Long-context Interleaved Video-Language
Understanding},
 url = {https://doi.org/10.48550/arXiv.2407.15754},
 volume = {abs/2407.15754},
 year = {2024}
}

@inproceedings{wu2023vcsum,
 author = {Wu, Han and Zhan, Mingjie and Tan, Haochen and Hou, Zhaohui and Liang, Ding and Song, Linqi},
 booktitle = {Findings of the Association for Computational Linguistics: ACL 2023},
 pages = {6065--6079},
 title = {VCSUM: A Versatile Chinese Meeting Summarization Dataset},
 year = {2023}
}

@misc{wu2024layercondensedkvcacheefficient,
 archiveprefix = {arXiv},
 author = {Haoyi Wu and Kewei Tu},
 eprint = {2405.10637},
 primaryclass = {cs.CL},
 title = {Layer-Condensed KV Cache for Efficient Inference of Large Language Models},
 url = {https://arxiv.org/abs/2405.10637},
 year = {2024}
}

@article{wu2024lifbench,
 author = {Wu, Xiaodong and Wang, Minhao and Liu, Yichen and Shi, Xiaoming and Yan, He and Lu, Xiangju and Zhu, Junmin and Zhang, Wei},
 journal = {arXiv preprint arXiv:2411.07037},
 title = {LIFBench: Evaluating the Instruction Following Performance and Stability of Large Language Models in Long-Context Scenarios},
 year = {2024}
}

@article{wu2024long,
 author = {Wu, Wenhao and Wang, Yizhong and Fu, Yao and Yue, Xiang and Zhu, Dawei and Li, Sujian},
 journal = {arXiv preprint arXiv:2405.03939},
 title = {Long context alignment with short instructions and synthesized positions},
 year = {2024}
}

@article{wu2024longgenbench,
 author = {Wu, Yuhao and Hee, Ming Shan and Hu, Zhiqing and Lee, Roy Ka-Wei},
 journal = {arXiv preprint arXiv:2409.02076},
 title = {LongGenBench: Benchmarking Long-Form Generation in Long Context LLMs},
 year = {2024}
}

@article{wu2024longmemeval,
 author = {Di Wu and Hongwei Wang and Wenhao Yu and Yuwei Zhang and Kai-Wei Chang and Dong Yu},
 journal = {arXiv preprint arXiv:2410.10813},
 primaryclass = {cs.CL},
 title = {LongMemEval: Benchmarking Chat Assistants on Long-Term Interactive Memory},
 url = {https://arxiv.org/abs/2410.10813},
 year = {2024}
}

@article{wu2025longattn,
 author = {Wu, Longyun and Zhu, Dawei and Zhao, Guangxiang and Yu, Zhuocheng and Ran, Junfeng and Wong, Xiangyu and Sun, Lin and Li, Sujian},
 journal = {arXiv preprint arXiv:2502.16860},
 title = {LongAttn: Selecting Long-context Training Data via Token-level Attention},
 year = {2025}
}

@article{wu2025more,
 author = {Wu, Yuyang and Wang, Yifei and Du, Tianqi and Jegelka, Stefanie and Wang, Yisen},
 journal = {arXiv preprint arXiv:2502.07266},
 title = {When More is Less: Understanding Chain-of-Thought Length in LLMs},
 year = {2025}
}

@inproceedings{wu2025retrieval_head,
 author = {Wenhao Wu and Yizhong Wang and Guangxuan Xiao and Hao Peng and Yao Fu},
 booktitle = {The Thirteenth International Conference on Learning Representations},
 title = {Retrieval Head Mechanistically Explains Long-Context Factuality},
 url = {https://openreview.net/forum?id=EytBpUGB1Z},
 year = {2025}
}

@inproceedings{wuefficient,
 author = {Wu, Tong and Zhao, Yanpeng and Zheng, Zilong},
 booktitle = {The Thirty-eighth Annual Conference on Neural Information Processing Systems},
 title = {An Efficient Recipe for Long Context Extension via Middle-Focused Positional Encoding}
}

@article{xi2023rise,
 author = {Xi, Zhiheng and Chen, Wenxiang and Guo, Xin and He, Wei and Ding, Yiwen and Hong, Boyang and Zhang, Ming and Wang, Junzhe and Jin, Senjie and Zhou, Enyu and others},
 journal = {arXiv preprint arXiv:2309.07864},
 title = {The rise and potential of large language model based agents: A survey},
 year = {2023}
}

@article{DBLP:journals/corr/abs-2309-07864,
  author       = {Zhiheng Xi and
                  Wenxiang Chen and
                  Xin Guo and
                  Wei He and
                  Yiwen Ding and
                  Boyang Hong and
                  Ming Zhang and
                  Junzhe Wang and
                  Senjie Jin and
                  Enyu Zhou and
                  Rui Zheng and
                  Xiaoran Fan and
                  Xiao Wang and
                  Limao Xiong and
                  Yuhao Zhou and
                  Weiran Wang and
                  Changhao Jiang and
                  Yicheng Zou and
                  Xiangyang Liu and
                  Zhangyue Yin and
                  Shihan Dou and
                  Rongxiang Weng and
                  Wensen Cheng and
                  Qi Zhang and
                  Wenjuan Qin and
                  Yongyan Zheng and
                  Xipeng Qiu and
                  Xuanjing Huang and
                  Tao Gui},
  title        = {The Rise and Potential of Large Language Model Based Agents: {A} Survey},
  journal      = {CoRR},
  volume       = {abs/2309.07864},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2309.07864},
  doi          = {10.48550/ARXIV.2309.07864},
  eprinttype    = {arXiv},
  eprint       = {2309.07864},
  timestamp    = {Mon, 04 Nov 2024 22:21:24 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2309-07864.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{xi2024coat,
 author = {Xi, Haocheng and Cai, Han and Zhu, Ligeng and Lu, Yao and Keutzer, Kurt and Chen, Jianfei and Han, Song},
 journal = {arXiv preprint arXiv:2410.19313},
 title = {COAT: Compressing Optimizer states and Activation for Memory-Efficient FP8 Training},
 year = {2024}
}


@article{xia2024agentless,
  author       = {Chunqiu Steven Xia and
                  Yinlin Deng and
                  Soren Dunn and
                  Lingming Zhang},
  title        = {Agentless: Demystifying LLM-based Software Engineering Agents},
  journal      = {CoRR},
  volume       = {abs/2407.01489},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2407.01489},
  doi          = {10.48550/ARXIV.2407.01489},
  eprinttype    = {arXiv},
  eprint       = {2407.01489},
  timestamp    = {Fri, 09 Aug 2024 10:24:41 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2407-01489.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{xia2024less,
 author = {Xia, Mengzhou and Malladi, Sadhika and Gururangan, Suchin and Arora, Sanjeev and Chen, Danqi},
 booktitle = {International Conference on Machine Learning (ICML)},
 title = {{LESS}: Selecting Influential Data for Targeted Instruction Tuning},
 year = {2024}
}

@inproceedings{xia2024unlocking,
  title={Unlocking Efficiency in Large Language Model Inference: A Comprehensive Survey of Speculative Decoding},
  author={Xia, Heming and Yang, Zhe and Dong, Qingxiu and Wang, Peiyi and Li, Yongqi and Ge, Tao and Liu, Tianyu and Li, Wenjie and Sui, Zhifang},
  booktitle={ACL (Findings)},
  year={2024}
}
@article{xia2025tokenskip,
 author = {Xia, Heming and Li, Yongqi and Leong, Chak Tou and Wang, Wenjie and Li, Wenjie},
 journal = {arXiv preprint arXiv:2502.12067},
 title = {TokenSkip: Controllable Chain-of-Thought Compression in LLMs},
 year = {2025}
}

@inproceedings{xiao2023smoothquant,
 author = {Xiao, Guangxuan and Lin, Ji and Seznec, Mickael and Wu, Hao and Demouth, Julien and Han, Song},
 booktitle = {International Conference on Machine Learning},
 organization = {PMLR},
 pages = {38087--38099},
 title = {Smoothquant: Accurate and efficient post-training quantization for large language models},
 year = {2023}
}

@article{xiao2024duoattention,
 author = {Xiao, Guangxuan and Tang, Jiaming and Zuo, Jingwei and Guo, Junxian and Yang, Shang and Tang, Haotian and Fu, Yao and Han, Song},
 journal = {arXiv preprint arXiv:2410.10819},
 title = {Duoattention: Efficient long-context llm inference with retrieval and streaming heads},
 year = {2024}
}

@misc{xiao2024duoattentionefficientlongcontextllm,
 archiveprefix = {arXiv},
 author = {Guangxuan Xiao and Jiaming Tang and Jingwei Zuo and Junxian Guo and Shang Yang and Haotian Tang and Yao Fu and Song Han},
 eprint = {2410.10819},
 primaryclass = {cs.CL},
 title = {DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads},
 url = {https://arxiv.org/abs/2410.10819},
 year = {2024}
}


@inproceedings{xiao2024efficientstreaminglanguagemodels,
  author       = {Guangxuan Xiao and
                  Yuandong Tian and
                  Beidi Chen and
                  Song Han and
                  Mike Lewis},
  title        = {Efficient Streaming Language Models with Attention Sinks},
  booktitle    = {{ICLR}},
  publisher    = {OpenReview.net},
  year         = {2024}
}

@inproceedings{xie2024osworld,
  author       = {Tianbao Xie and
                  Danyang Zhang and
                  Jixuan Chen and
                  Xiaochuan Li and
                  Siheng Zhao and
                  Ruisheng Cao and
                  Toh Jing Hua and
                  Zhoujun Cheng and
                  Dongchan Shin and
                  Fangyu Lei and
                  Yitao Liu and
                  Yiheng Xu and
                  Shuyan Zhou and
                  Silvio Savarese and
                  Caiming Xiong and
                  Victor Zhong and
                  Tao Yu},
  editor       = {Amir Globersons and
                  Lester Mackey and
                  Danielle Belgrave and
                  Angela Fan and
                  Ulrich Paquet and
                  Jakub M. Tomczak and
                  Cheng Zhang},
  title        = {OSWorld: Benchmarking Multimodal Agents for Open-Ended Tasks in Real
                  Computer Environments},
  booktitle    = {Advances in Neural Information Processing Systems 38: Annual Conference
                  on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver,
                  BC, Canada, December 10 - 15, 2024},
  year         = {2024},
  url          = {http://papers.nips.cc/paper\_files/paper/2024/hash/5d413e48f84dc61244b6be550f1cd8f5-Abstract-Datasets\_and\_Benchmarks\_Track.html},
  timestamp    = {Thu, 13 Feb 2025 16:56:43 +0100},
  biburl       = {https://dblp.org/rec/conf/nips/XieZCLZCHCSLLXZ24.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{xiong-etal-2024-effective,
 address = {Mexico City, Mexico},
 author = {Xiong, Wenhan  and
Liu, Jingyu  and
Molybog, Igor  and
Zhang, Hejia  and
Bhargava, Prajjwal  and
Hou, Rui  and
Martin, Louis  and
Rungta, Rashi  and
Sankararaman, Karthik Abinav  and
Oguz, Barlas  and
Khabsa, Madian  and
Fang, Han  and
Mehdad, Yashar  and
Narang, Sharan  and
Malik, Kshitiz  and
Fan, Angela  and
Bhosale, Shruti  and
Edunov, Sergey  and
Lewis, Mike  and
Wang, Sinong  and
Ma, Hao},
 booktitle = {Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)},
 doi = {10.18653/v1/2024.naacl-long.260},
 editor = {Duh, Kevin  and
Gomez, Helena  and
Bethard, Steven},
 month = {June},
 pages = {4643--4663},
 publisher = {Association for Computational Linguistics},
 title = {Effective Long-Context Scaling of Foundation Models},
 url = {https://aclanthology.org/2024.naacl-long.260},
 year = {2024}
}


@inproceedings{xiong2021nystromformernystrombasedalgorithmapproximating,
  author       = {Yunyang Xiong and
                  Zhanpeng Zeng and
                  Rudrasis Chakraborty and
                  Mingxing Tan and
                  Glenn Fung and
                  Yin Li and
                  Vikas Singh},
  title        = {Nystr{\"{o}}mformer: {A} Nystr{\"{o}}m-based Algorithm for
                  Approximating Self-Attention},
  booktitle    = {{AAAI}},
  pages        = {14138--14148},
  publisher    = {{AAAI} Press},
  year         = {2021}
}

@article{xiong2023effective,
 author = {Xiong, Wenhan and Liu, Jingyu and Molybog, Igor and Zhang, Hejia and Bhargava, Prajjwal and Hou, Rui and Martin, Louis and Rungta, Rashi and Sankararaman, Karthik Abinav and Oguz, Barlas and others},
 journal = {arXiv preprint arXiv:2309.16039},
 title = {Effective long-context scaling of foundation models},
 year = {2023}
}

@article{xiong2024artificial,
 author = {Xiong, Zheyang and Papageorgiou, Vasilis and Lee, Kangwook and Papailiopoulos, Dimitris},
 journal = {arXiv preprint arXiv:2406.19292},
 title = {From artificial needles to real haystacks: Improving retrieval capabilities in llms by finetuning on synthetic data},
 year = {2024}
}

@article{xu2023critical,
 author = {Xu, Fangyuan and Song, Yixiao and Iyyer, Mohit and Choi, Eunsol},
 journal = {arXiv preprint arXiv:2305.18201},
 title = {A critical evaluation of evaluations for long-form question answering},
 year = {2023}
}

@article{Xu2024ChatQA2B,
 author = {Peng Xu and Wei Ping and Xianchao Wu and Zihan Liu and Mohammad Shoeybi and Bryan Catanzaro},
 journal = {ArXiv},
 title = {ChatQA 2: Bridging the Gap to Proprietary LLMs in Long Context and RAG Capabilities},
 url = {https://api.semanticscholar.org/CorpusID:271310244},
 volume = {abs/2407.14482},
 year = {2024}
}

@article{xu2024detectiveqa,
 author = {Xu, Zhe and Ye, Jiasheng and Liu, Xiangyang and Sun, Tianxiang and Liu, Xiaoran and Guo, Qipeng and Li, Linlin and Liu, Qun and Huang, Xuanjing and Qiu, Xipeng},
 journal = {arXiv preprint arXiv:2409.02465},
 title = {Detectiveqa: Evaluating long-context reasoning on detective novels},
 year = {2024}
}

@inproceedings{xu2024stress,
 author = {Xu, Xiaoyue and Ye, Qinyuan and Ren, Xiang},
 booktitle = {The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
 title = {Stress-Testing Long-Context Language Models with Lifelong ICL and Task Haystack},
 year = {2024}
}

@article{xu2024vtensor,
 author = {Xu, Jiale and Zhang, Rui and Guo, Cong and Hu, Weiming and Liu, Zihan and Wu, Feiyang and Feng, Yu and Sun, Shixuan and Shao, Changxu and Guo, Yuhong and others},
 journal = {arXiv preprint arXiv:2407.15309},
 title = {vtensor: Flexible virtual tensor management for efficient llm serving},
 year = {2024}
}

@article{xuanlei2024hetegen,
 author = {XUANLEI, ZHAO and Jia, Bin and Zhou, Haotian and Liu, Ziming and Cheng, Shenggan and You, Yang},
 journal = {Proceedings of Machine Learning and Systems},
 pages = {162--172},
 title = {Hetegen: Efficient heterogeneous parallel inference for large language models on resource-constrained devices},
 volume = {6},
 year = {2024}
}

@article{yang-2024-arxiv-vca,
 author = {Zeyuan Yang and
Delin Chen and
Xueyang Yu and
Maohao Shen and
Chuang Gan},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/journals/corr/abs-2412-10471.bib},
 doi = {10.48550/ARXIV.2412.10471},
 eprint = {2412.10471},
 eprinttype = {arXiv},
 journal = {CoRR},
 timestamp = {Thu, 17 Oct 2024 12:28:14 +0200},
 title = {VCA: Video Curious Agent for Long Video Understanding},
 url = {https://doi.org/10.48550/arXiv.2412.10471},
 volume = {abs/2412.10471},
 year = {2024}
}

@article{xue-2024-arxiv-LongVILA,
  author       = {Fuzhao Xue and
                  Yukang Chen and
                  Dacheng Li and
                  Qinghao Hu and
                  Ligeng Zhu and
                  Xiuyu Li and
                  Yunhao Fang and
                  Haotian Tang and
                  Shang Yang and
                  Zhijian Liu and
                  Ethan He and
                  Hongxu Yin and
                  Pavlo Molchanov and
                  Jan Kautz and
                  Linxi Fan and
                  Yuke Zhu and
                  Yao Lu and
                  Song Han},
  title        = {LongVILA: Scaling Long-Context Visual Language Models for Long Videos},
  journal      = {CoRR},
  volume       = {abs/2408.10188},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2408.10188},
  doi          = {10.48550/ARXIV.2408.10188},
  eprinttype    = {arXiv},
  eprint       = {2408.10188},
  timestamp    = {Mon, 14 Oct 2024 08:21:05 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2408-10188.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{yang-2024-arxiv-visionzip,
 author = {Senqiao Yang and
Yukang Chen and
Zhuotao Tian and
Chengyao Wang and
Jingyao Li and
Bei Yu and
Jiaya Jia},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/journals/corr/abs-2412-04467.bib},
 doi = {10.48550/ARXIV.2412.04467},
 eprint = {2412.04467},
 eprinttype = {arXiv},
 journal = {CoRR},
 timestamp = {Thu, 16 Jan 2025 07:43:48 +0100},
 title = {VisionZip: Longer is Better but Not Necessary in Vision Language Models},
 url = {https://doi.org/10.48550/arXiv.2412.04467},
 volume = {abs/2412.04467},
 year = {2024}
}

@inproceedings{yang2015wikiqa,
 author = {Yang, Yi and Yih, Wen-tau and Meek, Christopher},
 booktitle = {Proceedings of the 2015 conference on empirical methods in natural language processing},
 pages = {2013--2018},
 title = {Wikiqa: A challenge dataset for open-domain question answering},
 year = {2015}
}

@inproceedings{yang2018hotpotqa,
 author = {Yang, Zhilin and Qi, Peng and Zhang, Saizheng and Bengio, Yoshua and Cohen, William and Salakhutdinov, Ruslan and Manning, Christopher D},
 booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
 pages = {2369--2380},
 title = {HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering},
 year = {2018}
}

@article{Yang2023Baichuan2O,
 author = {Ai Ming Yang and Bin Xiao and Bingning Wang and Borong Zhang and Ce Bian and Chao Yin and Chenxu Lv and Da Pan and Dian Wang and Dong Yan and Fan Yang and Fei Deng and Feng Wang and Feng Liu and Guangwei Ai and Guosheng Dong and Hai Zhao and Hang Xu and Hao-Lun Sun and Hongda Zhang and Hui Liu and Jiaming Ji and Jian Xie and Juntao Dai and Kuncheng Fang and Lei Su and Liang Song and Lifeng Liu and Liyun Ru and Luyao Ma and Mang Wang and Mickel Liu and Mingan Lin and Nuolan Nie and Pei Guo and Ruiyang Sun and Zhang Tao and Tianpeng Li and Tianyu Li and Wei Cheng and Weipeng Chen and Xiangrong Zeng and Xiaochuan Wang and Xiaoxi Chen and Xin Men and Xin Yu and Xuehai Pan and Yan-Bin Shen and Yiding Wang and Yiyu Li and Youxin Jiang and Yuchen Gao and Yupeng Zhang and Zenan Zhou and Zhiying Wu},
 journal = {ArXiv},
 title = {Baichuan 2: Open Large-scale Language Models},
 volume = {abs/2309.10305},
 year = {2023}
}

@article{yang2023palr,
 author = {Yang, Fan and Chen, Zheng and Jiang, Ziyan and Cho, Eunah and Huang, Xiaojiang and Lu, Yanbin},
 journal = {arXiv preprint arXiv:2305.07622},
 title = {Palr: Personalization aware llms for recommendation},
 year = {2023}
}

@misc{yang2024pyramidinferpyramidkvcache,
 archiveprefix = {arXiv},
 author = {Dongjie Yang and XiaoDong Han and Yan Gao and Yao Hu and Shilin Zhang and Hai Zhao},
 eprint = {2405.12532},
 primaryclass = {cs.CL},
 title = {PyramidInfer: Pyramid KV Cache Compression for High-throughput LLM Inference},
 url = {https://arxiv.org/abs/2405.12532},
 year = {2024}
}

@inproceedings{yang-etal-2024-pyramidinfer,
    title = "{P}yramid{I}nfer: Pyramid {KV} Cache Compression for High-throughput {LLM} Inference",
    author = "Yang, Dongjie  and
      Han, Xiaodong  and
      Gao, Yan  and
      Hu, Yao  and
      Zhang, Shilin  and
      Zhao, Hai",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2024",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-acl.195/",
    doi = "10.18653/v1/2024.findings-acl.195",
    pages = "3258--3270",
    abstract = "Large Language Models (LLMs) have shown remarkable comprehension abilities but face challenges in GPU memory usage during inference, hindering their scalability for real-time applications like chatbots. To accelerate inference, we store computed keys and values (KV cache) in the GPU memory. Existing methods study the KV cache compression to reduce memory by pruning the pre-computed KV cache. However, they neglect the inter-layer dependency between layers and huge memory consumption in pre-computation. To explore these deficiencies, we find that the number of crucial keys and values that influence future generations decreases layer by layer and we can extract them by the consistency in attention weights. Based on the findings, we propose PyramidInfer, a method that compresses the KV cache by layer-wise retaining crucial context. PyramidInfer saves significant memory by computing fewer keys and values without sacrificing performance. Experimental results show PyramidInfer improves 2.2x throughput compared to Accelerate with over 54{\%} GPU memory reduction in KV cache."
}

@article{yang2024swe,
 author = {Yang, John and Jimenez, Carlos E and Wettig, Alexander and Lieret, Kilian and Yao, Shunyu and Narasimhan, Karthik and Press, Ofir},
 journal = {arXiv preprint arXiv:2405.15793},
 title = {Swe-agent: Agent-computer interfaces enable automated software engineering},
 year = {2024}
}

@inproceedings{DBLP:conf/nips/YangJWLYNP24,
  author       = {John Yang and
                  Carlos E. Jimenez and
                  Alexander Wettig and
                  Kilian Lieret and
                  Shunyu Yao and
                  Karthik Narasimhan and
                  Ofir Press},
  editor       = {Amir Globersons and
                  Lester Mackey and
                  Danielle Belgrave and
                  Angela Fan and
                  Ulrich Paquet and
                  Jakub M. Tomczak and
                  Cheng Zhang},
  title        = {SWE-agent: Agent-Computer Interfaces Enable Automated Software Engineering},
  booktitle    = {Advances in Neural Information Processing Systems 38: Annual Conference
                  on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver,
                  BC, Canada, December 10 - 15, 2024},
  year         = {2024},
  url          = {http://papers.nips.cc/paper\_files/paper/2024/hash/5a7c947568c1b1328ccc5230172e1e7c-Abstract-Conference.html},
  timestamp    = {Thu, 13 Feb 2025 16:56:43 +0100},
  biburl       = {https://dblp.org/rec/conf/nips/YangJWLYNP24.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{yang2024tidaldecodefastaccuratellm,
 archiveprefix = {arXiv},
 author = {Lijie Yang and Zhihao Zhang and Zhuofu Chen and Zikun Li and Zhihao Jia},
 eprint = {2410.05076},
 primaryclass = {cs.LG},
 title = {TidalDecode: Fast and Accurate LLM Decoding with Position Persistent Sparse Attention},
 url = {https://arxiv.org/abs/2410.05076},
 year = {2024}
}

@misc{yang2024tokenleftbehindreliable,
 archiveprefix = {arXiv},
 author = {June Yong Yang and Byeongwook Kim and Jeongin Bae and Beomseok Kwon and Gunho Park and Eunho Yang and Se Jung Kwon and Dongsoo Lee},
 eprint = {2402.18096},
 primaryclass = {cs.LG},
 title = {No Token Left Behind: Reliable KV Cache Compression via Importance-Aware Mixed Precision Quantization},
 url = {https://arxiv.org/abs/2402.18096},
 year = {2024}
}

@article{yang2025qwen2,
 author = {Yang, An and Yu, Bowen and Li, Chengyuan and Liu, Dayiheng and Huang, Fei and Huang, Haoyan and Jiang, Jiandong and Tu, Jianhong and Zhang, Jianwei and Zhou, Jingren and others},
 journal = {arXiv preprint arXiv:2501.15383},
 title = {Qwen2. 5-1M Technical Report},
 year = {2025}
}

@article{yang2025towards,
 author = {Yang, Wenkai and Ma, Shuming and Lin, Yankai and Wei, Furu},
 journal = {arXiv preprint arXiv:2502.18080},
 title = {Towards Thinking-Optimal Scaling of Test-Time Compute for LLM Reasoning},
 year = {2025}
}

@article{yao2022react,
 author = {Yao, Shunyu and Zhao, Jeffrey and Yu, Dian and Du, Nan and Shafran, Izhak and Narasimhan, Karthik and Cao, Yuan},
 journal = {arXiv preprint arXiv:2210.03629},
 title = {React: Synergizing reasoning and acting in language models},
 year = {2022}
}

@inproceedings{DBLP:conf/iclr/YaoZYDSN023,
  author       = {Shunyu Yao and
                  Jeffrey Zhao and
                  Dian Yu and
                  Nan Du and
                  Izhak Shafran and
                  Karthik R. Narasimhan and
                  Yuan Cao},
  title        = {ReAct: Synergizing Reasoning and Acting in Language Models},
  booktitle    = {The Eleventh International Conference on Learning Representations,
                  {ICLR} 2023, Kigali, Rwanda, May 1-5, 2023},
  publisher    = {OpenReview.net},
  year         = {2023},
  url          = {https://openreview.net/forum?id=WE\_vluYUL-X},
  timestamp    = {Wed, 24 Jul 2024 16:50:33 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/YaoZYDSN023.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{yao2022zeroquant,
 author = {Yao, Zhewei and Yazdani Aminabadi, Reza and Zhang, Minjia and Wu, Xiaoxia and Li, Conglong and He, Yuxiong},
 journal = {Advances in Neural Information Processing Systems},
 pages = {27168--27183},
 title = {Zeroquant: Efficient and affordable post-training quantization for large-scale transformers},
 volume = {35},
 year = {2022}
}

@article{ye-2024-arxiv-mPLUG-Owl3,
 author = {Jiabo Ye and
Haiyang Xu and
Haowei Liu and
Anwen Hu and
Ming Yan and
Qi Qian and
Ji Zhang and
Fei Huang and
Jingren Zhou},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/journals/corr/abs-2408-04840.bib},
 doi = {10.48550/ARXIV.2408.04840},
 eprint = {2408.04840},
 eprinttype = {arXiv},
 journal = {CoRR},
 timestamp = {Mon, 30 Sep 2024 07:54:36 +0200},
 title = {mPLUG-Owl3: Towards Long Image-Sequence Understanding in Multi-Modal
Large Language Models},
 url = {https://doi.org/10.48550/arXiv.2408.04840},
 volume = {abs/2408.04840},
 year = {2024}
}

@inproceedings{ye2024chunkattention,
  title={ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and Two-Phase Partition},
  author={Ye, Lu and Tao, Ze and Huang, Yong and Li, Yang},
  booktitle={Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={11608--11620},
  year={2024}
}

@article{ye2024data,
 author = {Ye, Jiasheng and Liu, Peiju and Sun, Tianxiang and Zhou, Yunhua and Zhan, Jun and Qiu, Xipeng},
 journal = {arXiv preprint arXiv:2403.16952},
 title = {Data Mixing Laws: Optimizing Data Mixtures by Predicting Language Modeling Performance},
 year = {2024}
}

@article{ye2025longproc,
 author = {Ye, Xi and Yin, Fangcong and He, Yinghui and Zhang, Joie and Yen, Howard and Gao, Tianyu and Durrett, Greg and Chen, Danqi},
 journal = {arXiv preprint arXiv:2501.05414},
 title = {LongProc: Benchmarking Long-Context Language Models on Long Procedural Generation},
 year = {2025}
}

@article{yen2024helmet,
 author = {Yen, Howard and Gao, Tianyu and Hou, Minmin and Ding, Ke and Fleischer, Daniel and Izsak, Peter and Wasserblat, Moshe and Chen, Danqi},
 journal = {arXiv preprint arXiv:2410.02694},
 title = {Helmet: How to evaluate long-context language models effectively and thoroughly},
 year = {2024}
}

@article{yeung2016recommender,
 author = {Yeung, Chi Ho},
 journal = {Journal of Statistical Mechanics: Theory and Experiment},
 number = {4},
 pages = {043401},
 publisher = {IOP Publishing},
 title = {Do recommender systems benefit users? a modeling approach},
 volume = {2016},
 year = {2016}
}

@article{yin-2024-arxiv-T2Vid,
 author = {Shukang Yin and
Chaoyou Fu and
Sirui Zhao and
Yunhang Shen and
Chunjiang Ge and
Yan Yang and
Zuwei Long and
Yuhan Dai and
Tong Xu and
Xing Sun and
Ran He and
Caifeng Shan and
Enhong Chen},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/journals/corr/abs-2411-19951.bib},
 doi = {10.48550/ARXIV.2411.19951},
 eprint = {2411.19951},
 eprinttype = {arXiv},
 journal = {CoRR},
 timestamp = {Fri, 03 Jan 2025 07:22:02 +0100},
 title = {T2Vid: Translating Long Text into Multi-Image is the Catalyst for
Video-LLMs},
 url = {https://doi.org/10.48550/arXiv.2411.19951},
 volume = {abs/2411.19951},
 year = {2024}
}

@article{DBLP:journals/corr/abs-2411-19951,
  author       = {Shukang Yin and
                  Chaoyou Fu and
                  Sirui Zhao and
                  Yunhang Shen and
                  Chunjiang Ge and
                  Yan Yang and
                  Zuwei Long and
                  Yuhan Dai and
                  Tong Xu and
                  Xing Sun and
                  Ran He and
                  Caifeng Shan and
                  Enhong Chen},
  title        = {T2Vid: Translating Long Text into Multi-Image is the Catalyst for
                  Video-LLMs},
  journal      = {CoRR},
  volume       = {abs/2411.19951},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2411.19951},
  doi          = {10.48550/ARXIV.2411.19951},
  eprinttype    = {arXiv},
  eprint       = {2411.19951},
  timestamp    = {Fri, 03 Jan 2025 07:22:02 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2411-19951.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{yoon2024compactcompressingretrieveddocuments,
 archiveprefix = {arXiv},
 author = {Chanwoong Yoon and Taewhoo Lee and Hyeon Hwang and Minbyul Jeong and Jaewoo Kang},
 eprint = {2407.09014},
 primaryclass = {cs.CL},
 title = {CompAct: Compressing Retrieved Documents Actively for Question Answering},
 url = {https://arxiv.org/abs/2407.09014},
 year = {2024}
}

@inproceedings{yoon-etal-2024-compact,
    title = "{C}omp{A}ct: Compressing Retrieved Documents Actively for Question Answering",
    author = "Yoon, Chanwoong  and
      Lee, Taewhoo  and
      Hwang, Hyeon  and
      Jeong, Minbyul  and
      Kang, Jaewoo",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.emnlp-main.1194/",
    doi = "10.18653/v1/2024.emnlp-main.1194",
    pages = "21424--21439",
    abstract = "Retrieval-augmented generation supports language models to strengthen their factual groundings by providing external contexts. However, language models often face challenges when given extensive information, diminishing their effectiveness in solving questions. Context compression tackles this issue by filtering out irrelevant information, but current methods still struggle in realistic scenarios where crucial information cannot be captured with a single-step approach. To overcome this limitation, we introduce CompAct, a novel framework that employs an active strategy to condense extensive documents without losing key information. Our experiments demonstrate that CompAct brings significant improvements in both performance and compression rate on multi-hop question-answering benchmarks. CompAct flexibly operates as a cost-efficient plug-in module with various off-the-shelf retrievers or readers, achieving exceptionally high compression rates (47x)."
}

@inproceedings{youonlycacheonce,
 author = {Yutao Sun and
Li Dong and
Yi Zhu and
Shaohan Huang and
Wenhui Wang and
Shuming Ma and
Quanlu Zhang and
Jianyong Wang and
Furu Wei},
 booktitle = {NeurIPS},
 title = {You Only Cache Once: Decoder-Decoder Architectures for Language Models},
 year = {2024}
}

@article{yu-2024-arxiv-framevoyager,
 author = {Sicheng Yu and
Chengkai Jin and
Huanyu Wang and
Zhenghao Chen and
Sheng Jin and
Zhongrong Zuo and
Xiaolei Xu and
Zhenbang Sun and
Bingni Zhang and
Jiawei Wu and
Hao Zhang and
Qianru Sun},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/journals/corr/abs-2410-03226.bib},
 doi = {10.48550/ARXIV.2410.03226},
 eprint = {2410.03226},
 eprinttype = {arXiv},
 journal = {CoRR},
 timestamp = {Mon, 11 Nov 2024 20:54:53 +0100},
 title = {Frame-Voyager: Learning to Query Frames for Video Large Language Models},
 url = {https://doi.org/10.48550/arXiv.2410.03226},
 volume = {abs/2410.03226},
 year = {2024}
}

@inproceedings{Yu2023TrainingWT,
 author = {Yijiong Yu and Yongfeng Huang and Zhixiao Qi and Zhe Zhou},
 title = {Training With"Paraphrasing the Original Text"Teaches LLM to Better Retrieve in Long-context Tasks},
 year = {2023}
}

@article{yu2024collage,
 author = {Yu, Tao and Gupta, Gaurav and Gopalswamy, Karthick and Mamidala, Amith and Zhou, Hao and Huynh, Jeffrey and Park, Youngsuk and Diamant, Ron and Deoras, Anoop and Huan, Luke},
 journal = {arXiv preprint arXiv:2405.03637},
 title = {Collage: Light-Weight Low-Precision Strategy for LLM Training},
 year = {2024}
}


@article{yu2024defense,
  author       = {Tan Yu and
                  Anbang Xu and
                  Rama Akkiraju},
  title        = {In Defense of {RAG} in the Era of Long-Context Language Models},
  journal      = {CoRR},
  volume       = {abs/2409.01666},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2409.01666},
  doi          = {10.48550/ARXIV.2409.01666},
  eprinttype    = {arXiv},
  eprint       = {2409.01666},
  timestamp    = {Sat, 05 Oct 2024 21:12:50 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2409-01666.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Chen2025LADMLT,
  title={LADM: Long-context Training Data Selection with Attention-based Dependency Measurement for LLMs},
  author={Jianghao Chen and Junhong Wu and Yangyifan Xu and Jiajun Zhang},
  year={2025},
  url={https://api.semanticscholar.org/CorpusID:276768135}
}

@inproceedings{Wu2025LongAttnSL,
  title={LongAttn: Selecting Long-context Training Data via Token-level Attention},
  author={Longyun Wu and Dawei Zhu and Guangxiang Zhao and Zhuocheng Yu and Junfeng Ran and Xiangyu Wong and Lin Sun and Sujian Li},
  year={2025},
  url={https://api.semanticscholar.org/CorpusID:276575561}
}

@inproceedings{yu2024twinpilots,
 author = {Yu, Chengye and Wang, Tianyu and Shao, Zili and Zhu, Linjie and Zhou, Xu and Jiang, Song},
 booktitle = {Proceedings of the 17th ACM International Systems and Storage Conference},
 pages = {91--103},
 title = {Twinpilots: A new computing paradigm for gpu-cpu parallel llm inference},
 year = {2024}
}

@inproceedings{yuan2022wordcraft,
 author = {Yuan, Ann and Coenen, Andy and Reif, Emily and Ippolito, Daphne},
 booktitle = {Proceedings of the 27th International Conference on Intelligent User Interfaces},
 pages = {841--852},
 title = {Wordcraft: story writing with large language models},
 year = {2022}
}

@misc{yuan2024lveval,
 archiveprefix = {arXiv},
 author = {Tao Yuan and Xuefei Ning and Dong Zhou and Zhijie Yang and Shiyao Li and Minghui Zhuang and Zheyue Tan and Zhuyu Yao and Dahua Lin and Boxun Li and Guohao Dai and Shengen Yan and Yu Wang},
 eprint = {2402.05136},
 primaryclass = {cs.CL},
 title = {LV-Eval: A Balanced Long-Context Benchmark with 5 Length Levels Up to 256K},
 year = {2024}
}

@article{yuan2025native,
 author = {Yuan, Jingyang and Gao, Huazuo and Dai, Damai and Luo, Junyu and Zhao, Liang and Zhang, Zhengyan and Xie, Zhenda and Wei, YX and Wang, Lean and Xiao, Zhiping and others},
 journal = {arXiv preprint arXiv:2502.11089},
 title = {Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention},
 year = {2025}
}

@misc{yuan2025nativesparseattentionhardwarealigned,
 archiveprefix = {arXiv},
 author = {Jingyang Yuan and Huazuo Gao and Damai Dai and Junyu Luo and Liang Zhao and Zhengyan Zhang and Zhenda Xie and Y. X. Wei and Lean Wang and Zhiping Xiao and Yuqing Wang and Chong Ruan and Ming Zhang and Wenfeng Liang and Wangding Zeng},
 eprint = {2502.11089},
 primaryclass = {cs.CL},
 title = {Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention},
 url = {https://arxiv.org/abs/2502.11089},
 year = {2025}
}

@misc{yuan2025remambaequipmambaeffective,
 archiveprefix = {arXiv},
 author = {Danlong Yuan and Jiahao Liu and Bei Li and Huishuai Zhang and Jingang Wang and Xunliang Cai and Dongyan Zhao},
 eprint = {2408.15496},
 primaryclass = {cs.CL},
 title = {ReMamba: Equip Mamba with Effective Long-Sequence Modeling},
 url = {https://arxiv.org/abs/2408.15496},
 year = {2025}
}

@article{yue2024wkvquant,
 author = {Yue, Yuxuan and Yuan, Zhihang and Duanmu, Haojie and Zhou, Sifan and Wu, Jianlong and Nie, Liqiang},
 journal = {arXiv preprint arXiv:2402.12065},
 title = {Wkvquant: Quantizing weight and key/value cache for large language models gains more},
 year = {2024}
}


@inproceedings{zaheer2020big,
  author       = {Manzil Zaheer and
                  Guru Guruganesh and
                  Kumar Avinava Dubey and
                  Joshua Ainslie and
                  Chris Alberti and
                  Santiago Onta{\~{n}}{\'{o}}n and
                  Philip Pham and
                  Anirudh Ravula and
                  Qifan Wang and
                  Li Yang and
                  Amr Ahmed},
  editor       = {Hugo Larochelle and
                  Marc'Aurelio Ranzato and
                  Raia Hadsell and
                  Maria{-}Florina Balcan and
                  Hsuan{-}Tien Lin},
  title        = {Big Bird: Transformers for Longer Sequences},
  booktitle    = {Advances in Neural Information Processing Systems 33: Annual Conference
                  on Neural Information Processing Systems 2020, NeurIPS 2020, December
                  6-12, 2020, virtual},
  year         = {2020},
  url          = {https://proceedings.neurips.cc/paper/2020/hash/c8512d142a2d849725f31a9a7a361ab9-Abstract.html},
  timestamp    = {Mon, 16 Oct 2023 13:50:56 +0200},
  biburl       = {https://dblp.org/rec/conf/nips/ZaheerGDAAOPRWY20.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{zeng2022socratic,
 author = {Andy Zeng and Adrian S. Wong and Stefan Welker and K. Choromanski and F. Tombari and Aveek Purohit and M. Ryoo and Vikas Sindhwani and Johnny Lee and Vincent Vanhoucke and Peter R. Florence},
 bibsource = {Semantic Scholar https://www.semanticscholar.org/paper/ada81a4de88a6ce474df2e2446ad11fea480616e},
 doi = {10.48550/arXiv.2204.00598},
 journal = {International Conference on Learning Representations},
 title = {Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language},
 year = {2022}
}

@article{zeng2025revisiting,
 author = {Zeng, Zhiyuan and Cheng, Qinyuan and Yin, Zhangyue and Zhou, Yunhua and Qiu, Xipeng},
 journal = {arXiv preprint arXiv:2502.12215},
 title = {Revisiting the Test-Time Scaling of o1-like Models: Do they Truly Possess Test-Time Scaling Capabilities?},
 year = {2025}
}

@article{zhang-2024-arxiv-intenlmxcomposer25,
 author = {Pan Zhang and
Xiaoyi Dong and
Yuhang Zang and
Yuhang Cao and
Rui Qian and
Lin Chen and
Qipeng Guo and
Haodong Duan and
Bin Wang and
Linke Ouyang and
Songyang Zhang and
Wenwei Zhang and
Yining Li and
Yang Gao and
Peng Sun and
Xinyue Zhang and
Wei Li and
Jingwen Li and
Wenhai Wang and
Hang Yan and
Conghui He and
Xingcheng Zhang and
Kai Chen and
Jifeng Dai and
Yu Qiao and
Dahua Lin and
Jiaqi Wang},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/journals/corr/abs-2407-03320.bib},
 doi = {10.48550/ARXIV.2407.03320},
 eprint = {2407.03320},
 eprinttype = {arXiv},
 journal = {CoRR},
 timestamp = {Thu, 09 Jan 2025 18:54:58 +0100},
 title = {InternLM-XComposer-2.5: {A} Versatile Large Vision Language Model
Supporting Long-Contextual Input and Output},
 url = {https://doi.org/10.48550/arXiv.2407.03320},
 volume = {abs/2407.03320},
 year = {2024}
}

@inproceedings{zhang-etal-2024-draft,
 author = {Zhang, Jun  and
Wang, Jue  and
Li, Huan  and
Shou, Lidan  and
Chen, Ke  and
Chen, Gang  and
Mehrotra, Sharad},
 booktitle = {Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
 editor = {Ku, Lun-Wei  and
Martins, Andre  and
Srikumar, Vivek},
 month = {August},
 title = {Draft{\&} Verify: Lossless Large Language Model Acceleration via Self-Speculative Decoding},
 year = {2024}
}

@inproceedings{zhang2018improving,
  author       = {Jiacheng Zhang and
                  Huanbo Luan and
                  Maosong Sun and
                  Feifei Zhai and
                  Jingfang Xu and
                  Min Zhang and
                  Yang Liu},
  editor       = {Ellen Riloff and
                  David Chiang and
                  Julia Hockenmaier and
                  Jun'ichi Tsujii},
  title        = {Improving the Transformer Translation Model with Document-Level Context},
  booktitle    = {Proceedings of the 2018 Conference on Empirical Methods in Natural
                  Language Processing, Brussels, Belgium, October 31 - November 4, 2018},
  pages        = {533--542},
  publisher    = {Association for Computational Linguistics},
  year         = {2018},
  url          = {https://doi.org/10.18653/v1/d18-1049},
  doi          = {10.18653/V1/D18-1049},
  timestamp    = {Fri, 01 Sep 2023 13:50:18 +0200},
  biburl       = {https://dblp.org/rec/conf/emnlp/ZhangLSZXZL18.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@article{zhang2019bertscore,
 author = {Zhang, Tianyi and Kishore, Varsha and Wu, Felix and Weinberger, Kilian Q and Artzi, Yoav},
 journal = {arXiv preprint arXiv:1904.09675},
 title = {Bertscore: Evaluating text generation with bert},
 year = {2019}
}


@inproceedings{zhang2019hibertdocumentlevelpretraining,
  author       = {Xingxing Zhang and
                  Furu Wei and
                  Ming Zhou},
  editor       = {Anna Korhonen and
                  David R. Traum and
                  Llu{\'{\i}}s M{\`{a}}rquez},
  title        = {{HIBERT:} Document Level Pre-training of Hierarchical Bidirectional
                  Transformers for Document Summarization},
  booktitle    = {Proceedings of the 57th Conference of the Association for Computational
                  Linguistics, {ACL} 2019, Florence, Italy, July 28- August 2, 2019,
                  Volume 1: Long Papers},
  pages        = {5059--5069},
  publisher    = {Association for Computational Linguistics},
  year         = {2019},
  url          = {https://doi.org/10.18653/v1/p19-1499},
  doi          = {10.18653/V1/P19-1499},
  timestamp    = {Thu, 14 Nov 2024 09:42:52 +0100},
  biburl       = {https://dblp.org/rec/conf/acl/ZhangWZ19.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{zhang2020pegasus,
  author       = {Jingqing Zhang and
                  Yao Zhao and
                  Mohammad Saleh and
                  Peter J. Liu},
  title        = {{PEGASUS:} Pre-training with Extracted Gap-sentences for Abstractive
                  Summarization},
  booktitle    = {Proceedings of the 37th International Conference on Machine Learning,
                  {ICML} 2020, 13-18 July 2020, Virtual Event},
  series       = {Proceedings of Machine Learning Research},
  volume       = {119},
  pages        = {11328--11339},
  publisher    = {{PMLR}},
  year         = {2020},
  url          = {http://proceedings.mlr.press/v119/zhang20ae.html},
  timestamp    = {Tue, 15 Dec 2020 17:40:19 +0100},
  biburl       = {https://dblp.org/rec/conf/icml/ZhangZSL20.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{zhang2022opt_model_pretrained,
 archiveprefix = {arXiv},
 author = {Susan Zhang and Stephen Roller and Naman Goyal and Mikel Artetxe and Moya Chen and Shuohui Chen and Christopher Dewan and Mona Diab and Xian Li and Xi Victoria Lin and Todor Mihaylov and Myle Ott and Sam Shleifer and Kurt Shuster and Daniel Simig and Punit Singh Koura and Anjali Sridhar and Tianlu Wang and Luke Zettlemoyer},
 eprint = {2205.01068},
 primaryclass = {cs.CL},
 title = {OPT: Open Pre-trained Transformer Language Models},
 url = {https://arxiv.org/abs/2205.01068},
 year = {2022}
}

@article{zhang2023collm,
 author = {Zhang, Yang and Feng, Fuli and Zhang, Jizhi and Bao, Keqin and Wang, Qifan and He, Xiangnan},
 journal = {arXiv preprint arXiv:2310.19488},
 title = {Collm: Integrating collaborative embeddings into large language models for recommendation},
 year = {2023}
}

@misc{zhang2023h2oheavyhitteroracleefficient,
 archiveprefix = {arXiv},
 author = {Zhenyu Zhang and Ying Sheng and Tianyi Zhou and Tianlong Chen and Lianmin Zheng and Ruisi Cai and Zhao Song and Yuandong Tian and Christopher Ré and Clark Barrett and Zhangyang Wang and Beidi Chen},
 eprint = {2306.14048},
 primaryclass = {cs.LG},
 title = {H$_2$O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models},
 url = {https://arxiv.org/abs/2306.14048},
 year = {2023}
}

@inproceedings{NEURIPS2023_6ceefa7b,
 author = {Zhang, Zhenyu and Sheng, Ying and Zhou, Tianyi and Chen, Tianlong and Zheng, Lianmin and Cai, Ruisi and Song, Zhao and Tian, Yuandong and R\'{e}, Christopher and Barrett, Clark and Wang, Zhangyang "Atlas" and Chen, Beidi},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {34661--34710},
 publisher = {Curran Associates, Inc.},
 title = {H2O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/6ceefa7b15572587b78ecfcebb2827f8-Paper-Conference.pdf},
 volume = {36},
 year = {2023}
}



@article{zhang2023marathon,
 author = {Zhang, Lei and Li, Yunshui and Liu, Ziqiang and Liu, Junhao and Chen, Longze and Luo, Run and Yang, Min and others},
 journal = {arXiv preprint arXiv:2312.09542},
 title = {Marathon: A race through the realm of long context with large language models},
 year = {2023}
}



@inproceedings{zhang2023repocoder,
  author       = {Fengji Zhang and
                  Bei Chen and
                  Yue Zhang and
                  Jacky Keung and
                  Jin Liu and
                  Daoguang Zan and
                  Yi Mao and
                  Jian{-}Guang Lou and
                  Weizhu Chen},
  editor       = {Houda Bouamor and
                  Juan Pino and
                  Kalika Bali},
  title        = {RepoCoder: Repository-Level Code Completion Through Iterative Retrieval
                  and Generation},
  booktitle    = {Proceedings of the 2023 Conference on Empirical Methods in Natural
                  Language Processing, {EMNLP} 2023, Singapore, December 6-10, 2023},
  pages        = {2471--2484},
  publisher    = {Association for Computational Linguistics},
  year         = {2023},
  url          = {https://doi.org/10.18653/v1/2023.emnlp-main.151},
  doi          = {10.18653/V1/2023.EMNLP-MAIN.151},
  timestamp    = {Tue, 13 Aug 2024 08:01:16 +0200},
  biburl       = {https://dblp.org/rec/conf/emnlp/ZhangCZKLZMLC23.bib}
}

@misc{zhang2024adacompextractivecontextcompression,
 archiveprefix = {arXiv},
 author = {Qianchi Zhang and Hainan Zhang and Liang Pang and Hongwei Zheng and Zhiming Zheng},
 eprint = {2409.01579},
 primaryclass = {cs.CL},
 title = {AdaComp: Extractive Context Compression with Adaptive Predictor for Retrieval-Augmented Large Language Models},
 url = {https://arxiv.org/abs/2409.01579},
 year = {2024}
}

@article{zhang2024analyzing,
 author = {Zhang, Zhihan and Cao, Yixin and Ye, Chenchen and Ma, Yunshan and Liao, Lizi and Chua, Tat-Seng},
 journal = {arXiv preprint arXiv:2406.02472},
 title = {Analyzing Temporal Complex Events with Large Language Models? A Benchmark towards Temporal, Long Context Understanding},
 year = {2024}
}

@inproceedings{zhang2024autocoderover,
 author = {Zhang, Yuntong and Ruan, Haifeng and Fan, Zhiyu and Roychoudhury, Abhik},
 booktitle = {Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis},
 pages = {1592--1604},
 title = {Autocoderover: Autonomous program improvement},
 year = {2024}
}

@inproceedings{DBLP:conf/issta/0002RFR24,
  author       = {Yuntong Zhang and
                  Haifeng Ruan and
                  Zhiyu Fan and
                  Abhik Roychoudhury},
  editor       = {Maria Christakis and
                  Michael Pradel},
  title        = {AutoCodeRover: Autonomous Program Improvement},
  booktitle    = {Proceedings of the 33rd {ACM} {SIGSOFT} International Symposium on
                  Software Testing and Analysis, {ISSTA} 2024, Vienna, Austria, September
                  16-20, 2024},
  pages        = {1592--1604},
  publisher    = {{ACM}},
  year         = {2024},
  url          = {https://doi.org/10.1145/3650212.3680384},
  doi          = {10.1145/3650212.3680384},
  timestamp    = {Sun, 19 Jan 2025 13:25:48 +0100},
  biburl       = {https://dblp.org/rec/conf/issta/0002RFR24.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{zhang2024bench,
 author = {Zhang, Xinrong and Chen, Yingfa and Hu, Shengding and Xu, Zihang and Chen, Junhao and Hao, Moo and Han, Xu and Thai, Zhen and Wang, Shuo and Liu, Zhiyuan and others},
 booktitle = {Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
 pages = {15262--15277},
 title = {Extending long context evaluation beyond 100k tokens},
 year = {2024}
}

@article{zhang2024found,
 author = {Zhang, Zhenyu and Chen, Runjin and Liu, Shiwei and Yao, Zhewei and Ruwase, Olatunji and Chen, Beidi and Wu, Xiaoxia and Wang, Zhangyang},
 journal = {arXiv preprint arXiv:2403.04797},
 title = {Found in the middle: How language models use long contexts better via plug-and-play positional encoding},
 year = {2024}
}

@article{zhang2024hirope,
 author = {Zhang, Kechi and Li, Ge and Zhang, Huangzhao and Jin, Zhi},
 journal = {arXiv preprint arXiv:2403.19115},
 title = {HiRoPE: Length Extrapolation for Code Models Using Hierarchical Position},
 year = {2024}
}

@article{zhang2024kv,
  title={Kv cache is 1 bit per channel: Efficient large language model inference with coupled quantization},
  author={Zhang, Tianyi and Yi, Jonah and Xu, Zhaozhuo and Shrivastava, Anshumali},
  journal={Advances in Neural Information Processing Systems},
  volume={37},
  pages={3304--3331},
  year={2024}
}

@article{zhang2024longcite,
 author = {Zhang, Jiajie and Bai, Yushi and Lv, Xin and Gu, Wanjun and Liu, Danqing and Zou, Minhao and Cao, Shulin and Hou, Lei and Dong, Yuxiao and Feng, Ling and others},
 journal = {arXiv preprint arXiv:2409.02897},
 title = {Longcite: Enabling llms to generate fine-grained citations in long-context qa},
 year = {2024}
}

@misc{zhang2024longcontextcompressionactivation,
 archiveprefix = {arXiv},
 author = {Peitian Zhang and Zheng Liu and Shitao Xiao and Ninglu Shao and Qiwei Ye and Zhicheng Dou},
 eprint = {2401.03462},
 primaryclass = {cs.CL},
 title = {Long Context Compression with Activation Beacon},
 url = {https://arxiv.org/abs/2401.03462},
 year = {2024}
}

@inproceedings{
zhang2025long,
title={Long Context Compression with Activation Beacon},
author={Peitian Zhang and Zheng Liu and Shitao Xiao and Ninglu Shao and Qiwei Ye and Zhicheng Dou},
booktitle={The Thirteenth International Conference on Learning Representations},
year={2025},
url={https://openreview.net/forum?id=1eQT9OzfNQ}
}

@article{zhang2024longreward,
 author = {Jiajie Zhang and Zhongni Hou and Xin Lv and Shulin Cao and Zhenyu Hou and Yilin Niu and Lei Hou and Yuxiao Dong and Ling Feng and Juanzi Li},
 journal = {arXiv preprint arXiv:2410.21252},
 title = {LongReward: Improving Long-context Large Language Models
with AI Feedback},
 year = {2024}
}



@article{zhang2024mapneo,
 author = {Zhang, Ge and Qu, Scott and Liu, Jiaheng and Zhang, Chenchen and Lin, Chenghua and Yu, Chou Leuang and Pan, Danny and Cheng, Esther and Liu, Jie and Lin, Qunshu and others},
 journal = {arXiv preprint arXiv:2405.19327},
 title = {Map-neo: Highly capable and transparent bilingual large language model series},
 year = {2024}
}

@article{zhang2024o1,
 author = {Zhang, Yuxiang and Wu, Shangxi and Yang, Yuqi and Shu, Jiangming and Xiao, Jinlin and Kong, Chao and Sang, Jitao},
 journal = {arXiv preprint arXiv:2412.00154},
 title = {o1-Coder: an o1 Replication for Coding},
 year = {2024}
}

@article{zhang2024spar,
 author = {Zhang, Chiyu and Sun, Yifei and Chen, Jun and Lei, Jie and Abdul-Mageed, Muhammad and Wang, Sinong and Jin, Rong and Park, Sem and Yao, Ning and Long, Bo},
 journal = {arXiv preprint arXiv:2402.10555},
 title = {SPAR: Personalized Content-Based Recommendation via Long Engagement Attention},
 year = {2024}
}

@article{DBLP:journals/corr/abs-2402-10555,
  author       = {Chiyu Zhang and
                  Yifei Sun and
                  Jun Chen and
                  Jie Lei and
                  Muhammad Abdul{-}Mageed and
                  Sinong Wang and
                  Rong Jin and
                  Sem Park and
                  Ning Yao and
                  Bo Long},
  title        = {{SPAR:} Personalized Content-Based Recommendation via Long Engagement
                  Attention},
  journal      = {CoRR},
  volume       = {abs/2402.10555},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2402.10555},
  doi          = {10.48550/ARXIV.2402.10555},
  eprinttype    = {arXiv},
  eprint       = {2402.10555},
  timestamp    = {Fri, 25 Oct 2024 08:04:18 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2402-10555.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{zhang2024tablellm,
 author = {Zhang, Xiaokang and Zhang, Jing and Ma, Zeyao and Li, Yang and Zhang, Bohan and Li, Guanlin and Yao, Zijun and Xu, Kangli and Zhou, Jinchang and Zhang-Li, Daniel and others},
 journal = {arXiv preprint arXiv:2403.19318},
 title = {TableLLM: Enabling Tabular Data Manipulation by LLMs in Real Office Usage Scenarios},
 year = {2024}
}

@misc{zhang2024tokenslowerprecisionoptimal,
 archiveprefix = {arXiv},
 author = {Jiebin Zhang and Dawei Zhu and Yifan Song and Wenhao Wu and Chuqiao Kuang and Xiaoguang Li and Lifeng Shang and Qun Liu and Sujian Li},
 eprint = {2412.12706},
 primaryclass = {cs.CL},
 title = {More Tokens, Lower Precision: Towards the Optimal Token-Precision Trade-off in KV Cache Compression},
 url = {https://arxiv.org/abs/2412.12706},
 year = {2024}
}

@inproceedings{Zhang2025CodeCriticBenchAH,
 author = {Alexander Zhang and Marcus Dong and Jiaheng Liu and Wei Zhang and Yejie Wang and Jian Yang and Ge Zhang and Tianyu Liu and Zhongyuan Peng and Yingshui Tan and Yuanxing Zhang and Zhexu Wang and Weixun Wang and Yancheng He and Ken Deng and Wangchunshu Zhou and Wenhao Huang and Zhaoxiang Zhang},
 title = {CodeCriticBench: A Holistic Code Critique Benchmark for Large Language Models},
 year = {2025}
}

@article{zhang2025lightthinker,
 author = {Zhang, Jintian and Zhu, Yuqi and Sun, Mengshu and Luo, Yujie and Qiao, Shuofei and Du, Lun and Zheng, Da and Chen, Huajun and Zhang, Ningyu},
 journal = {arXiv preprint arXiv:2502.15589},
 title = {LightThinker: Thinking Step-by-Step Compression},
 year = {2025}
}

@misc{zhang2025lighttransferlongcontextllmsecretly,
 archiveprefix = {arXiv},
 author = {Xuan Zhang and Fengzhuo Zhang and Cunxiao Du and Chao Du and Tianyu Pang and Wei Gao and Min Lin},
 eprint = {2410.13846},
 primaryclass = {cs.CL},
 title = {LightTransfer: Your Long-Context LLM is Secretly a Hybrid Model with Effortless Adaptation},
 url = {https://arxiv.org/abs/2410.13846},
 year = {2025}
}

@inproceedings{
zhang2025lighttransfer,
title={LightTransfer: Your Long-Context {LLM} is Secretly a Hybrid Model with Effortless Adaptation},
author={Xuan Zhang and Fengzhuo Zhang and Cunxiao Du and Chao Du and Tianyu Pang and Wei Gao and Min Lin},
booktitle={Workshop on Reasoning and Planning for Large Language Models},
year={2025},
url={https://openreview.net/forum?id=DfgfGTfObm}
}

@article{zhao-2024-arxiv-OmChat,
 author = {Tiancheng Zhao and
Qianqian Zhang and
Kyusong Lee and
Peng Liu and
Lu Zhang and
Chunxin Fang and
Jiajia Liao and
Kelei Jiang and
Yibo Ma and
Ruochen Xu},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/journals/corr/abs-2407-04923.bib},
 doi = {10.48550/ARXIV.2407.04923},
 eprint = {2407.04923},
 eprinttype = {arXiv},
 journal = {CoRR},
 timestamp = {Mon, 12 Aug 2024 20:53:44 +0200},
 title = {OmChat: {A} Recipe to Train Multimodal Language Models with Strong
Long Context and Video Understanding},
 url = {https://doi.org/10.48550/arXiv.2407.04923},
 volume = {abs/2407.04923},
 year = {2024}
}

@article{zhao-2024-arxiv-videonian,
 author = {Zijia Zhao and
Haoyu Lu and
Yuqi Huo and
Yifan Du and
Tongtian Yue and
Longteng Guo and
Bingning Wang and
Weipeng Chen and
Jing Liu},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/journals/corr/abs-2406-09367.bib},
 doi = {10.48550/ARXIV.2406.09367},
 eprint = {2406.09367},
 eprinttype = {arXiv},
 journal = {CoRR},
 timestamp = {Tue, 09 Jul 2024 17:23:21 +0200},
 title = {Needle In {A} Video Haystack: {A} Scalable Synthetic Framework for
Benchmarking Video MLLMs},
 url = {https://doi.org/10.48550/arXiv.2406.09367},
 volume = {abs/2406.09367},
 year = {2024}
}

@article{zhao2023goldminer,
 author = {Zhao, Hanyu and Yang, Zhi and Cheng, Yu and Tian, Chao and Ren, Shiru and Xiao, Wencong and Yuan, Man and Chen, Langshi and Liu, Kaibo and Zhang, Yang and others},
 journal = {Proceedings of the ACM on Management of Data},
 number = {2},
 pages = {1--25},
 publisher = {ACM New York, NY, USA},
 title = {Goldminer: Elastic scaling of training data pre-processing pipelines for deep learning},
 volume = {1},
 year = {2023}
}

@article{zhao2023pytorch,
 author = {Zhao, Yanli and Gu, Andrew and Varma, Rohan and Luo, Liang and Huang, Chien-Chin and Xu, Min and Wright, Less and Shojanazeri, Hamid and Ott, Myle and Shleifer, Sam and others},
 journal = {arXiv preprint arXiv:2304.11277},
 title = {Pytorch fsdp: experiences on scaling fully sharded data parallel},
 year = {2023}
}

@article{zhao2023survey,
 author = {Zhao, Wayne Xin and Zhou, Kun and Li, Junyi and Tang, Tianyi and Wang, Xiaolei and Hou, Yupeng and Min, Yingqian and Zhang, Beichen and Zhang, Junjie and Dong, Zican and others},
 journal = {arXiv preprint arXiv:2303.18223},
 title = {A survey of large language models},
 year = {2023}
}

@article{zhao2024longrag,
 author = {Zhao, Qingfei and Wang, Ruobing and Cen, Yukuo and Zha, Daren and Tan, Shicheng and Dong, Yuxiao and Tang, Jie},
 journal = {arXiv preprint arXiv:2410.18050},
 title = {LongRAG: A Dual-Perspective Retrieval-Augmented Generation Paradigm for Long-Context Question Answering},
 year = {2024}
}

@article{zhao2024longskywork,
 author = {Zhao, Liang and Wei, Tianwen and Zeng, Liang and Cheng, Cheng and Yang, Liu and Cheng, Peng and Wang, Lijie and Li, Chenxia and Wu, Xuejie and Zhu, Bo and others},
 journal = {arXiv preprint arXiv:2406.00605},
 title = {Longskywork: A training recipe for efficiently extending context length in large language models},
 year = {2024}
}

@article{zhao2024marco,
 author = {Zhao, Yu and Yin, Huifeng and Zeng, Bo and Wang, Hao and Shi, Tianqi and Lyu, Chenyang and Wang, Longyue and Luo, Weihua and Zhang, Kaifu},
 journal = {arXiv preprint arXiv:2411.14405},
 title = {Marco-o1: Towards Open Reasoning Models for Open-Ended Solutions},
 year = {2024}
}

@article{zhao2024wildhallucinations,
 author = {Zhao, Wenting and Goyal, Tanya and Chiu, Yu Ying and Jiang, Liwei and Newman, Benjamin and Ravichander, Abhilasha and Chandu, Khyathi and Bras, Ronan Le and Cardie, Claire and Deng, Yuntian and others},
 journal = {arXiv preprint arXiv:2407.17468},
 title = {Wildhallucinations: Evaluating long-form factuality in llms with real-world entity queries},
 year = {2024}
}

@article{zhao2024atom,
  title={Atom: Low-bit quantization for efficient and accurate llm serving},
  author={Zhao, Yilong and Lin, Chien-Yu and Zhu, Kan and Ye, Zihao and Chen, Lequn and Zheng, Size and Ceze, Luis and Krishnamurthy, Arvind and Chen, Tianqi and Kasikci, Baris},
  journal={Proceedings of Machine Learning and Systems},
  volume={6},
  pages={196--209},
  year={2024}
}

@article{zheng2023efficiently,
 author = {Zheng, Lianmin and Yin, Liangsheng and Xie, Zhiqiang and Huang, Jeff and Sun, Chuyue and Yu, Cody\_Hao and Cao, Shiyi and Kozyrakis, Christos and Stoica, Ion and Gonzalez, Joseph E and others},
 publisher = {arXiv},
 title = {Efficiently Programming Large Language Models using SGLang.},
 year = {2023}
}

@article{zheng2023judging,
 author = {Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Lin, Zi and Li, Zhuohan and Li, Dacheng and Xing, Eric and others},
 journal = {Advances in Neural Information Processing Systems},
 pages = {46595--46623},
 title = {Judging llm-as-a-judge with mt-bench and chatbot arena},
 volume = {36},
 year = {2023}
}

@article{zheng2025dape,
 author = {Zheng, Chuanyang and Gao, Yihang and Shi, Han and Huang, Minbin and Li, Jingyao and Xiong, Jing and Ren, Xiaozhe and Ng, Michael and Jiang, Xin and Li, Zhenguo and others},
 journal = {Advances in Neural Information Processing Systems},
 pages = {26659--26700},
 title = {DAPE: Data-Adaptive Positional Encoding for Length Extrapolation},
 volume = {37},
 year = {2025}
}

@inproceedings{zhong2021qmsum,
 author = {Zhong, Ming and Yin, Da and Yu, Tao and Zaidi, Ahmad and Mutuma, Mutethia and Jha, Rahul and Hassan, Ahmed and Celikyilmaz, Asli and Liu, Yang and Qiu, Xipeng and others},
 booktitle = {Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
 pages = {5905--5921},
 title = {QMSum: A New Benchmark for Query-based Multi-domain Meeting Summarization},
 year = {2021}
}

@misc{zhong2023memorybankenhancinglargelanguage,
 archiveprefix = {arXiv},
 author = {Wanjun Zhong and Lianghong Guo and Qiqi Gao and He Ye and Yanlin Wang},
 eprint = {2305.10250},
 primaryclass = {cs.CL},
 title = {MemoryBank: Enhancing Large Language Models with Long-Term Memory},
 url = {https://arxiv.org/abs/2305.10250},
 year = {2023}
}

@inproceedings{DBLP:conf/aaai/ZhongGGYW24,
  author       = {Wanjun Zhong and
                  Lianghong Guo and
                  Qiqi Gao and
                  He Ye and
                  Yanlin Wang},
  editor       = {Michael J. Wooldridge and
                  Jennifer G. Dy and
                  Sriraam Natarajan},
  title        = {MemoryBank: Enhancing Large Language Models with Long-Term Memory},
  booktitle    = {Thirty-Eighth {AAAI} Conference on Artificial Intelligence, {AAAI}
                  2024, Thirty-Sixth Conference on Innovative Applications of Artificial
                  Intelligence, {IAAI} 2024, Fourteenth Symposium on Educational Advances
                  in Artificial Intelligence, {EAAI} 2014, February 20-27, 2024, Vancouver,
                  Canada},
  pages        = {19724--19731},
  publisher    = {{AAAI} Press},
  year         = {2024},
  url          = {https://doi.org/10.1609/aaai.v38i17.29946},
  doi          = {10.1609/AAAI.V38I17.29946},
  timestamp    = {Tue, 04 Mar 2025 08:09:48 +0100},
  biburl       = {https://dblp.org/rec/conf/aaai/ZhongGGYW24.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{zhong2024distserve,
 author = {Zhong, Yinmin and Liu, Shengyu and Chen, Junda and Hu, Jianbo and Zhu, Yibo and Liu, Xuanzhe and Jin, Xin and Zhang, Hao},
 booktitle = {18th USENIX Symposium on Operating Systems Design and Implementation (OSDI 24)},
 pages = {193--210},
 title = {$\{$DistServe$\}$: Disaggregating prefill and decoding for goodput-optimized large language model serving},
 year = {2024}
}

@article{zhou-2024-arxiv-mlvu,
 author = {Junjie Zhou and
Yan Shu and
Bo Zhao and
Boya Wu and
Shitao Xiao and
Xi Yang and
Yongping Xiong and
Bo Zhang and
Tiejun Huang and
Zheng Liu},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/journals/corr/abs-2406-04264.bib},
 doi = {10.48550/ARXIV.2406.04264},
 eprint = {2406.04264},
 eprinttype = {arXiv},
 journal = {CoRR},
 timestamp = {Fri, 05 Jul 2024 16:54:14 +0200},
 title = {{MLVU:} {A} Comprehensive Benchmark for Multi-Task Long Video Understanding},
 url = {https://doi.org/10.48550/arXiv.2406.04264},
 volume = {abs/2406.04264},
 year = {2024}
}

@inproceedings{zhou-etal-2024-num_sys,
 address = {Miami, Florida, USA},
 author = {Zhou, Zhejian  and
Wang, JIayu  and
Lin, Dahua  and
Chen, Kai},
 booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2024},
 doi = {10.18653/v1/2024.findings-emnlp.218},
 editor = {Al-Onaizan, Yaser  and
Bansal, Mohit  and
Chen, Yun-Nung},
 month = {November},
 pages = {3806--3820},
 publisher = {Association for Computational Linguistics},
 title = {Scaling Behavior for Large Language Models regarding Numeral Systems: An Example using Pythia},
 url = {https://aclanthology.org/2024.findings-emnlp.218/},
 year = {2024}
}

@misc{zhou2023agents,
 archiveprefix = {arXiv},
 author = {Wangchunshu Zhou and Yuchen Eleanor Jiang and Long Li and Jialong Wu and Tiannan Wang and Shi Qiu and Jintian Zhang and Jing Chen and Ruipu Wu and Shuai Wang and Shiding Zhu and Jiyu Chen and Wentao Zhang and Xiangru Tang and Ningyu Zhang and Huajun Chen and Peng Cui and Mrinmaya Sachan},
 eprint = {2309.07870},
 primaryclass = {cs.CL},
 title = {Agents: An Open-source Framework for Autonomous Language Agents},
 url = {https://arxiv.org/abs/2309.07870},
 year = {2023}
}

@article{DBLP:journals/corr/abs-2309-07870,
  author       = {Wangchunshu Zhou and
                  Yuchen Eleanor Jiang and
                  Long Li and
                  Jialong Wu and
                  Tiannan Wang and
                  Shi Qiu and
                  Jintian Zhang and
                  Jing Chen and
                  Ruipu Wu and
                  Shuai Wang and
                  Shiding Zhu and
                  Jiyu Chen and
                  Wentao Zhang and
                  Ningyu Zhang and
                  Huajun Chen and
                  Peng Cui and
                  Mrinmaya Sachan},
  title        = {Agents: An Open-source Framework for Autonomous Language Agents},
  journal      = {CoRR},
  volume       = {abs/2309.07870},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2309.07870},
  doi          = {10.48550/ARXIV.2309.07870},
  eprinttype    = {arXiv},
  eprint       = {2309.07870},
  timestamp    = {Mon, 14 Oct 2024 08:21:04 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2309-07870.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{zhou2023dynaicl,
 archiveprefix = {arXiv},
 author = {Wangchunshu Zhou and Yuchen Eleanor Jiang and Ryan Cotterell and Mrinmaya Sachan},
 eprint = {2305.11170},
 primaryclass = {cs.CL},
 title = {Efficient Prompting via Dynamic In-Context Learning},
 url = {https://arxiv.org/abs/2305.11170},
 year = {2023}
}

@misc{zhou2023recurrentgpt,
 archiveprefix = {arXiv},
 author = {Wangchunshu Zhou and Yuchen Eleanor Jiang and Peng Cui and Tiannan Wang and Zhenxin Xiao and Yifan Hou and Ryan Cotterell and Mrinmaya Sachan},
 eprint = {2305.13304},
 primaryclass = {cs.CL},
 title = {RecurrentGPT: Interactive Generation of (Arbitrarily) Long Text},
 url = {https://arxiv.org/abs/2305.13304},
 year = {2023}
}

@inproceedings{zhou2023webarena,
  author       = {Shuyan Zhou and
                  Frank F. Xu and
                  Hao Zhu and
                  Xuhui Zhou and
                  Robert Lo and
                  Abishek Sridhar and
                  Xianyi Cheng and
                  Tianyue Ou and
                  Yonatan Bisk and
                  Daniel Fried and
                  Uri Alon and
                  Graham Neubig},
  title        = {WebArena: {A} Realistic Web Environment for Building Autonomous Agents},
  booktitle    = {The Twelfth International Conference on Learning Representations,
                  {ICLR} 2024, Vienna, Austria, May 7-11, 2024},
  publisher    = {OpenReview.net},
  year         = {2024},
  url          = {https://openreview.net/forum?id=oKn9c6ytLx},
  timestamp    = {Wed, 07 Aug 2024 17:11:53 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/ZhouX0ZLSCOBF0N24.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{zhou2024symboliclearningenablesselfevolving,
 archiveprefix = {arXiv},
 author = {Wangchunshu Zhou and Yixin Ou and Shengwei Ding and Long Li and Jialong Wu and Tiannan Wang and Jiamin Chen and Shuai Wang and Xiaohua Xu and Ningyu Zhang and Huajun Chen and Yuchen Eleanor Jiang},
 eprint = {2406.18532},
 primaryclass = {cs.CL},
 title = {Symbolic Learning Enables Self-Evolving Agents},
 url = {https://arxiv.org/abs/2406.18532},
 year = {2024}
}

@article{DBLP:journals/corr/abs-2406-18532,
  author       = {Wangchunshu Zhou and
                  Yixin Ou and
                  Shengwei Ding and
                  Long Li and
                  Jialong Wu and
                  Tiannan Wang and
                  Jiamin Chen and
                  Shuai Wang and
                  Xiaohua Xu and
                  Ningyu Zhang and
                  Huajun Chen and
                  Yuchen Eleanor Jiang},
  title        = {Symbolic Learning Enables Self-Evolving Agents},
  journal      = {CoRR},
  volume       = {abs/2406.18532},
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2406.18532},
  doi          = {10.48550/ARXIV.2406.18532},
  eprinttype    = {arXiv},
  eprint       = {2406.18532},
  timestamp    = {Thu, 07 Nov 2024 08:16:14 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2406-18532.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{zhou2025dynamickvtaskawareadaptivekv,
 archiveprefix = {arXiv},
 author = {Xiabin Zhou and Wenbin Wang and Minyan Zeng and Jiaxian Guo and Xuebo Liu and Li Shen and Min Zhang and Liang Ding},
 eprint = {2412.14838},
 primaryclass = {cs.CL},
 title = {DynamicKV: Task-Aware Adaptive KV Cache Compression for Long Context LLMs},
 url = {https://arxiv.org/abs/2412.14838},
 year = {2025}
}

@article{zhu-2024-arxiv-focusllava,
 author = {Yuke Zhu and
Chi Xie and
Shuang Liang and
Bo Zheng and
Sheng Guo},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/journals/corr/abs-2411-14228.bib},
 doi = {10.48550/ARXIV.2411.14228},
 eprint = {2411.14228},
 eprinttype = {arXiv},
 journal = {CoRR},
 timestamp = {Wed, 01 Jan 2025 13:20:24 +0100},
 title = {FocusLLaVA: {A} Coarse-to-Fine Approach for Efficient and Effective
Visual Token Compression},
 url = {https://doi.org/10.48550/arXiv.2411.14228},
 volume = {abs/2411.14228},
 year = {2024}
}

@inproceedings{zhu-etal-2024-longembed,
 address = {Miami, Florida, USA},
 author = {Zhu, Dawei  and
Wang, Liang  and
Yang, Nan  and
Song, Yifan  and
Wu, Wenhao  and
Wei, Furu  and
Li, Sujian},
 booktitle = {Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
 doi = {10.18653/v1/2024.emnlp-main.47},
 editor = {Al-Onaizan, Yaser  and
Bansal, Mohit  and
Chen, Yun-Nung},
 month = {November},
 pages = {802--816},
 publisher = {Association for Computational Linguistics},
 title = {{L}ong{E}mbed: Extending Embedding Models for Long Context Retrieval},
 url = {https://aclanthology.org/2024.emnlp-main.47},
 year = {2024}
}


@article{zhu2023large,
  author       = {Yutao Zhu and
                  Huaying Yuan and
                  Shuting Wang and
                  Jiongnan Liu and
                  Wenhan Liu and
                  Chenlong Deng and
                  Zhicheng Dou and
                  Ji{-}Rong Wen},
  title        = {Large Language Models for Information Retrieval: {A} Survey},
  journal      = {CoRR},
  volume       = {abs/2308.07107},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2308.07107},
  doi          = {10.48550/ARXIV.2308.07107},
  eprinttype    = {arXiv},
  eprint       = {2308.07107},
  timestamp    = {Sun, 06 Oct 2024 21:24:00 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2308-07107.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{zhu2023pose,
 author = {Zhu, Dawei and Yang, Nan and Wang, Liang and Song, Yifan and Wu, Wenhao and Wei, Furu and Li, Sujian},
 booktitle = {The Twelfth International Conference on Learning Representations},
 title = {PoSE: Efficient Context Window Extension of LLMs via Positional Skip-wise Training},
 year = {2023}
}

@article{zhu2024longembed,
 author = {Zhu, Dawei and Wang, Liang and Yang, Nan and Song, Yifan and Wu, Wenhao and Wei, Furu and Li, Sujian},
 journal = {arXiv preprint arXiv:2404.12096},
 title = {LongEmbed: Extending Embedding Models for Long Context Retrieval},
 year = {2024}
}

@inproceedings{DBLP:conf/emnlp/ZhuW0SWWL24,
  author       = {Dawei Zhu and
                  Liang Wang and
                  Nan Yang and
                  Yifan Song and
                  Wenhao Wu and
                  Furu Wei and
                  Sujian Li},
  editor       = {Yaser Al{-}Onaizan and
                  Mohit Bansal and
                  Yun{-}Nung Chen},
  title        = {LongEmbed: Extending Embedding Models for Long Context Retrieval},
  booktitle    = {Proceedings of the 2024 Conference on Empirical Methods in Natural
                  Language Processing, {EMNLP} 2024, Miami, FL, USA, November 12-16,
                  2024},
  pages        = {802--816},
  publisher    = {Association for Computational Linguistics},
  year         = {2024},
  url          = {https://aclanthology.org/2024.emnlp-main.47},
  timestamp    = {Tue, 11 Feb 2025 13:38:25 +0100},
  biburl       = {https://dblp.org/rec/conf/emnlp/ZhuW0SWWL24.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Zhu2024LongEmbedEE,
 author = {Dawei Zhu and Liang Wang and Nan Yang and Yifan Song and Wenhao Wu and Furu Wei and Sujian Li},
 booktitle = {Conference on Empirical Methods in Natural Language Processing},
 title = {LongEmbed: Extending Embedding Models for Long Context Retrieval},
 url = {https://api.semanticscholar.org/CorpusID:269214659},
 year = {2024}
}

@misc{zhuge2024languageagentsoptimizablegraphs,
 archiveprefix = {arXiv},
 author = {Mingchen Zhuge and Wenyi Wang and Louis Kirsch and Francesco Faccio and Dmitrii Khizbullin and Jürgen Schmidhuber},
 eprint = {2402.16823},
 primaryclass = {cs.AI},
 title = {Language Agents as Optimizable Graphs},
 url = {https://arxiv.org/abs/2402.16823},
 year = {2024}
}

@inproceedings{DBLP:conf/icml/ZhugeWKFKS24,
  author       = {Mingchen Zhuge and
                  Wenyi Wang and
                  Louis Kirsch and
                  Francesco Faccio and
                  Dmitrii Khizbullin and
                  J{\"{u}}rgen Schmidhuber},
  title        = {GPTSwarm: Language Agents as Optimizable Graphs},
  booktitle    = {Forty-first International Conference on Machine Learning, {ICML} 2024,
                  Vienna, Austria, July 21-27, 2024},
  publisher    = {OpenReview.net},
  year         = {2024},
  url          = {https://openreview.net/forum?id=uTC9AFXIhg},
  timestamp    = {Mon, 02 Sep 2024 16:55:25 +0200},
  biburl       = {https://dblp.org/rec/conf/icml/ZhugeWKFKS24.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{zou-2025-arxiv-hlv1k,
 author = {Heqing Zou and
Tianze Luo and
Guiyang Xie and
Fengmao Lv and
Guangcong Wang and
Junyang Chen and
Zhuochen Wang and
Hansheng Zhang and
Huaijian Zhang and
others},
 bibsource = {dblp computer science bibliography, https://dblp.org},
 biburl = {https://dblp.org/rec/journals/corr/abs-2501.01645.bib},
 doi = {10.48550/ARXIV.2501.01645},
 eprint = {2501.01645},
 eprinttype = {arXiv},
 journal = {CoRR},
 timestamp = {Thu, 17 Oct 2024 12:28:14 +0200},
 title = {HLV-1K: A Large-scale Hour-Long Video Benchmark for Time-Specific Long Video Understanding},
 url = {https://doi.org/10.48550/arXiv.2501.01645},
 volume = {abs/2501.01645},
 year = {2025}
}

@article{zou2024retrieval,
 author = {Zou, Kaijian and Khalifa, Muhammad and Wang, Lu},
 journal = {arXiv preprint arXiv:2411.07130},
 title = {Retrieval or Global Context Understanding? On Many-Shot In-Context Learning for Long-Context Evaluation},
 year = {2024}
}

@misc{zuhri2024mlkvmultilayerkeyvalueheads,
 archiveprefix = {arXiv},
 author = {Zayd Muhammad Kawakibi Zuhri and Muhammad Farid Adilazuarda and Ayu Purwarianti and Alham Fikri Aji},
 eprint = {2406.09297},
 primaryclass = {cs.LG},
 title = {MLKV: Multi-Layer Key-Value Heads for Memory Efficient Transformer Decoding},
 url = {https://arxiv.org/abs/2406.09297},
 year = {2024}
}
@article{anthropic_induction_heads,
   title={In-context Learning and Induction Heads},
   author={Olsson, Catherine and Elhage, Nelson and Nanda, Neel and Joseph, Nicholas and DasSarma, Nova and Henighan, Tom and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and Drain, Dawn and Ganguli, Deep and Hatfield-Dodds, Zac and Hernandez, Danny and Johnston, Scott and Jones, Andy and Kernion, Jackson and Lovitt, Liane and Ndousse, Kamal and Amodei, Dario and Brown, Tom and Clark, Jack and Kaplan, Jared and McCandlish, Sam and Olah, Chris},
   year={2022},
   journal={Transformer Circuits Thread},
   note={https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html}
}

@inproceedings{wang_interpretability_wild_2023,
	title = {Interpretability in the {Wild}: a {Circuit} for {Indirect} {Object} {Identification} in {GPT}-2 {Small}},
	url = {https://openreview.net/forum?id=NpsVSN6o4ul},
	booktitle = {The {Eleventh} {International} {Conference} on {Learning} {Representations}},
	author = {Wang, Kevin Ro and Variengien, Alexandre and Conmy, Arthur and Shlegeris, Buck and Steinhardt, Jacob},
	year = {2023},
}

@InProceedings{arith_head_icml,
  title = 	 {Interpreting and Improving Large Language Models in Arithmetic Calculation},
  author =       {Zhang, Wei and Wan, Chaoqun and Zhang, Yonggang and Cheung, Yiu-Ming and Tian, Xinmei and Shen, Xu and Ye, Jieping},
  booktitle = 	 {Proceedings of the 41st International Conference on Machine Learning},
  pages = 	 {59932--59950},
  year = 	 {2024},
  editor = 	 {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},
  volume = 	 {235},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {21--27 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://raw.githubusercontent.com/mlresearch/v235/main/assets/zhang24bk/zhang24bk.pdf},
  url = 	 {https://proceedings.mlr.press/v235/zhang24bk.html},
}

@online{jianlin_rope_beta_base,
        title={Transformer 10},
        author={Su, Jianlin},
        year={2023},
        month={Aug},
        url={https://spaces.ac.cn/archives/9675},
}

@misc{cooney2023circuitsvis,
    title = {CircuitsVis},
    author = {Alan Cooney and Neel Nanda},
    year = {2023},
    howpublished = {\url{https://github.com/TransformerLensOrg/CircuitsVis}},
}

@misc{chen2024circuitcomplexityboundsropebased,
      title={Circuit Complexity Bounds for RoPE-based Transformer Architecture}, 
      author={Bo Chen and Xiaoyu Li and Yingyu Liang and Jiangxuan Long and Zhenmei Shi and Zhao Song},
      year={2024},
      eprint={2411.07602},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2411.07602}, 
}

@inproceedings{nips_fourier_high_freq,
 author = {Tancik, Matthew and Srinivasan, Pratul and Mildenhall, Ben and Fridovich-Keil, Sara and Raghavan, Nithin and Singhal, Utkarsh and Ramamoorthi, Ravi and Barron, Jonathan and Ng, Ren},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {7537--7547},
 publisher = {Curran Associates, Inc.},
 title = {Fourier Features Let Networks Learn High Frequency Functions in Low Dimensional Domains},
 url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/55053683268957697aa39fba6f231c68-Paper.pdf},
 volume = {33},
 year = {2020}
}

@misc{ntk_aware_reddit,
    title = {{NTK-Aware Scaled RoPE allows LLaMA models to have extended (8k+) context size without any fine-tuning and minimal perplexity degradation.}},
    author = {bloc97},
    url = "https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_scaled_rope_allows_llama_models_to_have/",
    year = 2023
}

@misc{alex_library,
    title = {{Library of Alexandria}},
    author = {wiki},
    url = "https://en.wikipedia.org/wiki/Library_of_Alexandria",
    year = 2024
}
@misc{tang_astronomer,
    title = {{Chinese astronomy}},
    author = {wiki},
    url = "https://en.wikipedia.org/wiki/Chinese_astronomy",
    year = 2024
}
@misc{print_press,
    title = {{Printing press}},
    author = {wiki},
    url = "https://en.wikipedia.org/wiki/Printing_press",
    year = 2023
}
@misc{men2024baseropeboundscontext,
      title={Base of RoPE Bounds Context Length}, 
      author={Xin Men and Mingyu Xu and Bingning Wang and Qingyu Zhang and Hongyu Lin and Xianpei Han and Weipeng Chen},
      year={2024},
      eprint={2405.14591},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2405.14591}, 
}

@misc{yang_hybrid_attn_rope_nope,
      title={Rope to Nope and Back Again: A New Hybrid Attention Strategy}, 
      author={Bowen Yang and Bharat Venkitesh and Dwarak Talupuru and Hangyu Lin and David Cairuz and Phil Blunsom and Acyr Locatelli},
      year={2025},
      eprint={2501.18795},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2501.18795}, 
}
@online{kexuefm-9708,
        title={Transformer 12},
        author={Su, Jianlin},
        year={2023},
        month={Aug},
        url={https://kexue.fm/archives/9708},
}
@article{shannon1948mathematical,
  title={A mathematical theory of communication},
  author={Shannon, Claude Elwood},
  journal={The Bell system technical journal},
  volume={27},
  number={3},
  pages={379--423},
  year={1948},
  publisher={Nokia Bell Labs}
}

@article{chen1999empirical,
  title={An empirical study of smoothing techniques for language modeling},
  author={Chen, Stanley F and Goodman, Joshua},
  journal={Computer Speech \& Language},
  volume={13},
  number={4},
  pages={359--394},
  year={1999},
  publisher={Elsevier}
}


@inproceedings{mikolov2010recurrent,
  title={Recurrent neural network based language model.},
  author={Mikolov, Tomas and Karafi{\'a}t, Martin and Burget, Lukas and Cernock{\`y}, Jan and Khudanpur, Sanjeev},
  booktitle={Interspeech},
  volume={2},
  number={3},
  pages={1045--1048},
  year={2010},
  organization={Makuhari}
}

@article{chang2024flux,
  title={FLUX: fast software-based communication overlap on gpus through kernel fusion},
  author={Chang, Li-Wen and Bao, Wenlei and Hou, Qi and Jiang, Chengquan and Zheng, Ningxin and Zhong, Yinmin and Zhang, Xuanrun and Song, Zuquan and Yao, Chengji and Jiang, Ziheng and others},
  journal={arXiv preprint arXiv:2406.06858},
  year={2024}
}

@inproceedings{sourouri2014effective,
  title={Effective multi-GPU communication using multiple CUDA streams and threads},
  author={Sourouri, Mohammed and Gillberg, Tor and Baden, Scott B and Cai, Xing},
  booktitle={2014 20th IEEE International Conference on Parallel and Distributed Systems (ICPADS)},
  pages={981--986},
  year={2014},
  organization={IEEE}
}

@inproceedings{kim2024tccl,
  title={Tccl: Discovering better communication paths for pcie gpu clusters},
  author={Kim, Heehoon and Ryu, Junyeol and Lee, Jaejin},
  booktitle={Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3},
  pages={999--1015},
  year={2024}
}

@misc{flashmla2025,
      title={FlashMLA: Efficient MLA decoding kernels},
      author={Jiashi Li},
      year={2025},
      publisher = {GitHub},
      howpublished = {\url{https://github.com/deepseek-ai/FlashMLA}},
}

@misc{deepseek3fs,
      title={3FS: Fire-Flyer File System},
      author={DeepSeek},
      year={2025},
      publisher = {GitHub},
      howpublished = {\url{https://github.com/deepseek-ai/3FS}},
}

@misc{deepgemm2025,
      title={DeepGEMM: clean and efficient FP8 GEMM kernels with fine-grained scaling}, 
      author={Chenggang Zhao and Liang Zhao and Jiashi Li and Zhean Xu},
      year={2025},
      publisher = {GitHub},
      howpublished = {\url{https://github.com/deepseek-ai/DeepGEMM}},
}

@misc{deepseekai2024deepseekv3technicalreport,
      title={DeepSeek-V3 Technical Report}, 
      author={DeepSeek-AI},
      year={2024},
      eprint={2412.19437},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2412.19437}, 
}
@article{ling2025longreason,
  title={LongReason: A Synthetic Long-Context Reasoning Benchmark via Context Expansion},
  author={Ling, Zhan and Liu, Kang and Yan, Kai and Yang, Yifan and Lin, Weijian and Fan, Ting-Han and Shen, Lingfeng and Du, Zhengyin and Chen, Jiecao},
  journal={arXiv preprint arXiv:2501.15089},
  year={2025}
}
@article{zheng2024dape,
  title={DAPE V2: Process Attention Score as Feature Map for Length Extrapolation},
  author={Zheng, Chuanyang and Gao, Yihang and Shi, Han and Xiong, Jing and Sun, Jiankai and Li, Jingyao and Huang, Minbin and Ren, Xiaozhe and Ng, Michael and Jiang, Xin and others},
  journal={arXiv preprint arXiv:2410.04798},
  year={2024}
}
@inproceedings{weng2024longvlm,
  title={Longvlm: Efficient long video understanding via large language models},
  author={Weng, Yuetian and Han, Mingfei and He, Haoyu and Chang, Xiaojun and Zhuang, Bohan},
  booktitle={European Conference on Computer Vision},
  pages={453--470},
  year={2024},
  organization={Springer}
}

@misc{oai2025computeruse,
 author = {Team, OpenAI},
 howpublished = {\url{https://openai.com/index/computer-using-agent/}},
 title = {Computer-Using Agent},
 year = {2025}
}

@misc{manus,
 author = {Team, Manus},
 howpublished = {\url{https://manus.im/}},
 title = {Leave it to Manus},
 year = {2025}
}

@misc{coze,
 author = {Team, Coze},
 howpublished = {\url{https://www.coze.cn/}},
 title = {Coze},
 year = {2025}
}

@misc{dify,
 author = {Team, Dify},
 howpublished = {\url{https://dify.ai/}},
 title = {The Innovation Engine for GenAI Applications},
 year = {2025}
}