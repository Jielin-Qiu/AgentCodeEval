# AgentCodeEval Test Configuration File
# Optimized for testing and development with smaller numbers

api:
  # API Keys (set via environment variables for security)
  # openai_api_key: "your-openai-key-here"
  # anthropic_api_key: "your-anthropic-key-here" 
  # google_api_key: "your-google-key-here"
  # huggingface_token: "your-hf-token-here"
  
  # Rate limiting settings
  max_requests_per_minute: 60
  max_concurrent_requests: 10
  
  # Default models - üèÜ 3 Elite Models
  default_model_openai: "o3"                                  # ‚úÖ Elite: OpenAI o3 (reasoning model)
  default_model_anthropic: "claude-sonnet-4-20250514"         # ‚úÖ Elite: Claude Sonnet 4 via AWS Bedrock
  default_model_google: "gemini-2.5-pro"                      # ‚úÖ Elite: Gemini 2.5 Pro (latest)

data:
  # Local storage directories
  output_dir: "./data/output"
  generated_dir: "./data/generated"
  templates_dir: "./data/templates"
  
  # Synthetic generation settings
  supported_languages:
    - python
    - javascript
    - typescript
    - java
    - cpp
    - go
  
  # Project generation criteria (TESTING VALUES)
  min_files_per_project: 5
  max_files_per_project: 20  # Smaller for testing
  projects_per_language: 1   # Just 1 project per language for testing
  
  # Complexity levels for synthetic projects
  complexity_distribution:
    easy: 0.50      # 50% easy projects (more for testing)
    medium: 0.30    # 30% medium projects
    hard: 0.15      # 15% hard projects
    expert: 0.05    # 5% expert projects
  
  # Generation quality controls
  min_complexity_score: 0.3
  max_complexity_score: 0.9
  min_documentation_ratio: 0.1

benchmark:
  # Scale parameters (TESTING VALUES)
  total_instances: 16  # Fast testing: 1 project √ó 8 categories √ó 2 = 16 scenarios
  
  # Task category distribution (must sum to total_instances)
  task_distribution:
    architectural_understanding: 2   # 2 scenarios per category for fast testing
    cross_file_refactoring: 2
    feature_implementation: 2
    bug_investigation: 2
    multi_session_development: 2
    code_comprehension: 2
    integration_testing: 2
    security_analysis: 2
  
  # Difficulty distribution (must sum to total_instances)
  difficulty_distribution:
    easy: 8         # 50% easy (8/16)
    medium: 5       # 31% medium (5/16)
    hard: 2         # 13% hard (2/16)
    expert: 1       # 6% expert (1/16)
  
  # Context length ranges (min_tokens, max_tokens)
  context_ranges:
    easy: [5000, 20000]      # Smaller for testing
    medium: [20000, 50000]   # Smaller for testing
    hard: [50000, 100000]    # Smaller for testing
    expert: [100000, 200000] # Smaller for testing
  
  # Information coverage requirements
  min_information_coverage: 0.7
  target_information_coverage:
    easy: 0.75
    medium: 0.80    # Lower for testing
    hard: 0.85      # Lower for testing
    expert: 0.90    # Lower for testing

evaluation:
  # Metric weights for CADS (Composite Agent Development Score)
  # Must sum to 1.0
  metric_weights:
    architectural_coherence: 0.20
    dependency_traversal: 0.20
    multi_session_memory: 0.20
    cross_file_reasoning: 0.15
    incremental_development: 0.15
    information_coverage: 0.10
  
  # Scoring thresholds
  score_thresholds:
    excellent:
      min: 4.0
      max: 5.0
    good:
      min: 3.0
      max: 4.0
    fair:
      min: 2.0
      max: 3.0
    poor:
      min: 0.0
      max: 2.0
  
  # Timeout settings (seconds) - Shorter for testing
  task_timeout: 180      # 3 minutes per task (shorter for testing)
  session_timeout: 900   # 15 minutes per session (shorter for testing)
  
  # Validation settings
  human_validation_ratio: 0.05  # 5% manual validation
  inter_rater_agreement_threshold: 0.8 